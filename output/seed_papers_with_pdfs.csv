title,authors,year,venue,doi,arxiv_id,url,abstract,why_seed,source_db,pdf_path,pdf_status,screening_id,pdf_hash
Open-Ended Wargames with Large Language Models,"['Daniel P. Hogan', 'Andrea Brennen']",2024,arXiv preprint,,2404.11446,http://arxiv.org/abs/2404.11446,"Wargames are a powerful tool for understanding and rehearsing real-world decision making. Automated play of wargames using artificial intelligence (AI) enables possibilities beyond those of human-conducted games, such as playing the game many times over to see a range of possible outcomes. There are two categories of wargames: quantitative games, with discrete types of moves, and qualitative games, which revolve around open-ended responses. Historically, automation efforts have focused on quantitative games, but large language models (LLMs) make it possible to automate qualitative wargames. We introduce ""Snow Globe,"" an LLM-powered multi-agent system for playing qualitative wargames. With Snow Globe, every stage of a text-based qualitative wargame from scenario preparation to post-game analysis can be optionally carried out by AI, humans, or a combination thereof. We describe its software architecture conceptually and release an open-source implementation alongside this publication. As case studies, we simulate a tabletop exercise about an AI incident response and a political wargame about a geopolitical crisis. We discuss potential applications of the approach and how it fits into the broader wargaming ecosystem.","Foundational paper introducing Snow Globe framework for automating qualitative wargames with LLMs, bridging human and AI-conducted wargaming",arxiv,pdf_cache/pdfs/arxiv_2404_11446.pdf,downloaded_arxiv,SEED_0000,b07d4a6a2f5b3a9970dbd6441b002e1c6c299fdca119808d62fc3b950bd40796
War and Peace (WarAgent): Large Language Model-based Multi-agent Simulation of World Wars,"['Wenyue Hua', 'Lizhou Fan', 'Lingyao Li', 'Kai Mei', 'Jianchao Ji', 'Yingqiang Ge', 'Libby Hemphill', 'Yongfeng Zhang']",2023,arXiv preprint,,2311.17227,http://arxiv.org/abs/2311.17227,"Can we avoid wars at the crossroads of history? This question has been pursued by individuals, scholars, policymakers, and organizations throughout human history. In this research, we attempt to answer the question by developing WarAgent, an LLM-powered multi-agent AI system, to simulate the participating countries (agents) in international conflicts under diverse scenarios. By situating in the historical context of World War I and trying to prevent the war, we demonstrate the feasibility of computational simulations for peace research and international relations studies. Our findings suggest that WarAgent can generate meaningful insights, despite being constrained by historical facts and the inherent limitations of LLMs. Specifically, we find that the different backgrounds and personalities of leader agents can significantly influence the simulation outcome. Additionally, our framework is highly adaptable, allowing it to simulate a wide range of conflict scenarios.",Multi-agent simulation framework for international conflicts and war prevention using LLMs in historical contexts,arxiv,pdf_cache/pdfs/arxiv_2311_17227.pdf,downloaded_arxiv,SEED_0001,d162a42c22c90495694bebf3402a782ce41acf845e52ca87dada44dc575f8dc2
Human vs. Machine: Behavioral Differences Between Expert Humans and Language Models in Wargame Simulations,"['Max Lamparth', 'Anthony Corso', 'Jacob Ganz', 'Oriana Skylar Mastro', 'Jacquelyn Schneider', 'Harold Trinkunas']",2024,arXiv preprint,,2403.03407,http://arxiv.org/abs/2403.03407,"To some, the advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness while reducing the influence of human error and emotions. However, there is still debate about how AI systems, especially large language models (LLMs) that can be applied to many tasks, behave compared to humans in high-stakes military decision-making scenarios with the potential for increased risks towards escalation. To test this potential and scrutinize the use of LLMs for such purposes, we use a new wargame experiment with 214 national security experts designed to examine crisis escalation in a fictional U.S.-China scenario and compare the behavior of human player teams to LLM-simulated team responses in separate simulations. Here, we find that the LLM-simulated responses can be more aggressive and significantly affected by changes in the scenario. We show a considerable high-level agreement in the LLM and human responses and significant quantitative and qualitative differences in individual actions and strategic tendencies.",Empirical comparative study of human vs. LLM behavior in military wargaming with 214 national security experts,arxiv,pdf_cache/pdfs/arxiv_2403_03407.pdf,downloaded_arxiv,SEED_0002,ec141b10f69c294248e6f7e669f896c9bb298b2bbf2d1d02d340217cb0d05544
Escalation Risks from Language Models in Military and Diplomatic Decision-Making,"['Juan-Pablo Rivera', 'Gabriel Mukobi', 'Anka Reuel', 'Max Lamparth', 'Chandler Smith', 'Jacquelyn Schneider']",2024,"The 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT 24)",10.1145/3630106.3658942,2401.03408,http://arxiv.org/abs/2401.03408,"Governments are increasingly considering integrating autonomous AI agents in high-stakes military and foreign-policy decision-making, especially with the emergence of advanced generative AI models like GPT-4. Our work aims to scrutinize the behavior of multiple AI agents in simulated wargames, specifically focusing on their predilection to take escalatory actions that may exacerbate multilateral conflicts. Drawing on political science and international relations literature about escalation dynamics, we design a novel wargame simulation and scoring framework to assess the escalation risks of actions taken by these agents in different scenarios. Contrary to prior studies, our research provides both qualitative and quantitative insights and focuses on large language models (LLMs). We find that all five studied off-the-shelf LLMs show forms of escalation and difficult-to-predict escalation patterns.",Critical analysis of escalation risks when LLMs are used in high-stakes military and diplomatic decision-making scenarios,arxiv,pdf_cache/pdfs/10_1145_3630106_3658942.pdf,cached,SEED_0003,170d63f15f6d9f136c70aedda83086da4ed28e2dc56bbe45dfbbe943585846c5
Human-level play in the game of Diplomacy by combining language models with strategic reasoning,"['Meta Fundamental AI Research Diplomacy Team (FAIR)', 'Anton Bakhtin', 'Noam Brown', 'Emily Dinan', 'Gabriele Farina', 'Colin Flaherty', 'Daniel Fried', 'Andrew Goff', 'Jonathan Gray', 'Hengyuan Hu', 'Athul Paul Jacob', 'Mojtaba Komeili', 'Karthik Konath', 'Minae Kwon', 'Adam Lerer', 'Mike Lewis', 'Alexander H. Miller', 'Sasha Mitts', 'Adithya Renduchintala', 'Stephen Roller', 'Dirk Rowe', 'Weiyan Shi', 'Joe Spisak', 'Alexander Wei', 'David Wu', 'Hugh Zhang', 'Markus Zijlstra']",2022,Science,10.1126/science.ade9097,,https://www.science.org/doi/10.1126/science.ade9097,"We present Cicero, an AI agent that can play Diplomacy at a human level by combining strategic reasoning with natural language processing. Diplomacy is a strategic board game where seven players compete for control of Europe, using both military units and natural language negotiation to form and break alliances. Unlike most AI systems that operate in perfect-information environments, Cicero must navigate a hidden-information game involving coordination, negotiation, and competition between multiple players. Our approach combines a language model for dialogue with a strategic reasoning module for planning. We demonstrate that Cicero achieved human-level performance in an online Diplomacy league, ranking in the top 10% of participants who played more than one game.",Landmark achievement demonstrating human-level strategic gameplay combining language models with reasoning in the game of Diplomacy,arxiv,,not_found,SEED_0004,
