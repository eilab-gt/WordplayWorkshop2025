screening_id,title,authors,year,venue,abstract,doi,url,source_db,include_ta,reason_ta,notes_ta,include_ft,reason_ft,notes_ft,relevance_score,quality_score,pdf_path,pdf_status,arxiv_id,citations,pdf_url,keywords,pdf_hash,screener_ta,screened_ta_date,screener_ft,screened_ft_date,potential_duplicate,language_detected,venue_type,game_type,open_ended,quantitative,llm_family,llm_role,eval_metrics,failure_modes,awscale,code_release,grey_lit_flag,extraction_status,extraction_confidence,failure_modes_regex,llm_detected,game_type_detected,metrics_detected,code_detected
SCREEN_0001,Open-Ended Wargames with Large Language Models,Daniel P. Hogan; Andrea Brennen,2024,,"Wargames are a powerful tool for understanding and rehearsing real-world decision making. Automated play of wargames using artificial intelligence (AI) enables possibilities beyond those of human-conducted games, such as playing the game many times over to see a range of possible outcomes. There are two categories of wargames: quantitative games, with discrete types of moves, and qualitative games, which revolve around open-ended responses. Historically, automation efforts have focused on quantitative games, but large language models (LLMs) make it possible to automate qualitative wargames. We introduce ""Snow Globe,"" an LLM-powered multi-agent system for playing qualitative wargames. With Snow Globe, every stage of a text-based qualitative wargame from scenario preparation to post-game analysis can be optionally carried out by AI, humans, or a combination thereof. We describe its software architecture conceptually and release an open-source implementation alongside this publication. As case studies, we simulate a tabletop exercise about an AI incident response and a political wargame about a geopolitical crisis. We discuss potential applications of the approach and how it fits into the broader wargaming ecosystem.",,http://arxiv.org/abs/2404.11446v1,arxiv,,,,,,,,,pdf_cache/pdfs/9e93117ac1a50824.pdf,cached,2404.114461,,http://arxiv.org/pdf/2404.11446v1,cs.CL; cs.AI; cs.CY,b07d4a6a2f5b3a9970dbd6441b002e1c6c299fdca119808d62fc3b950bd40796,,,,,,en,tech-report,digital,yes,no,GPT-4,player,"The paper discusses the use of LLMs to generate plausible narratives and adjudicate outcomes in qualitative wargames, focusing on the logical sense of cause and effect in narratives.",hallucination|repetition,2,https://github.com/langchain-ai/langchain,yes,success,1.0,,,,,mentioned
SCREEN_0002,MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models,Tianle Gu; Zeyang Zhou; Kexin Huang; Dandan Liang; Yixu Wang; Haiquan Zhao; Yuanqi Yao; Xingge Qiao; Keqing Wang; Yujiu Yang; Yan Teng; Yu Qiao; Yingchun Wang,2024,,"Powered by remarkable advancements in Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) demonstrate impressive capabilities in manifold tasks. However, the practical application scenarios of MLLMs are intricate, exposing them to potential malicious instructions and thereby posing safety risks. While current benchmarks do incorporate certain safety considerations, they often lack comprehensive coverage and fail to exhibit the necessary rigor and robustness. For instance, the common practice of employing GPT-4V as both the evaluator and a model to be evaluated lacks credibility, as it tends to exhibit a bias toward its own responses. In this paper, we present MLLMGuard, a multidimensional safety evaluation suite for MLLMs, including a bilingual image-text evaluation dataset, inference utilities, and a lightweight evaluator. MLLMGuard's assessment comprehensively covers two languages (English and Chinese) and five important safety dimensions (Privacy, Bias, Toxicity, Truthfulness, and Legality), each with corresponding rich subtasks. Focusing on these dimensions, our evaluation dataset is primarily sourced from platforms such as social media, and it integrates text-based and image-based red teaming techniques with meticulous annotation by human experts. This can prevent inaccurate evaluation caused by data leakage when using open-source datasets and ensures the quality and challenging nature of our benchmark. Additionally, a fully automated lightweight evaluator termed GuardRank is developed, which achieves significantly higher evaluation accuracy than GPT-4. Our evaluation results across 13 advanced models indicate that MLLMs still have a substantial journey ahead before they can be considered safe and responsible.",,http://arxiv.org/abs/2406.07594v2,arxiv,,,,,,,,,pdf_cache/pdfs/cf85d35b6a3af58f.pdf,cached,2406.075942,,http://arxiv.org/pdf/2406.07594v2,cs.CL; cs.AI; cs.CR,a32931ffeb51224fa62cb06b0be5307ffaa72cbc51d7b15d3f6315c89107d248,,,,,,en,tech-report,digital,yes,yes,GPT-4V,evaluator,"Evaluation metrics include Privacy, Bias, Toxicity, Truthfulness, and Legality across 12 subtasks, using a fully automated lightweight evaluator termed GuardRank.",bias|data_leakage|factual_error|hallucination|toxicity,6,https://github.com/Carol-gutianle/MLLMGuard,yes,success,1.0,bias|data_leakage|factual_error,gpt4,,accuracy,mentioned
SCREEN_0003,Can Large Language Models Automatically Jailbreak GPT-4V?,Yuanwei Wu; Yue Huang; Yixin Liu; Xiang Li; Pan Zhou; Lichao Sun,2024,,"GPT-4V has attracted considerable attention due to its extraordinary capacity for integrating and processing multimodal information. At the same time, its ability of face recognition raises new safety concerns of privacy leakage. Despite researchers' efforts in safety alignment through RLHF or preprocessing filters, vulnerabilities might still be exploited. In our study, we introduce AutoJailbreak, an innovative automatic jailbreak technique inspired by prompt optimization. We leverage Large Language Models (LLMs) for red-teaming to refine the jailbreak prompt and employ weak-to-strong in-context learning prompts to boost efficiency. Furthermore, we present an effective search method that incorporates early stopping to minimize optimization time and token expenditure. Our experiments demonstrate that AutoJailbreak significantly surpasses conventional methods, achieving an Attack Success Rate (ASR) exceeding 95.3\%. This research sheds light on strengthening GPT-4V security, underscoring the potential for LLMs to be exploited in compromising GPT-4V integrity.",,http://arxiv.org/abs/2407.16686v2,arxiv,,,,,,,,,pdf_cache/pdfs/02f6730b0367a49b.pdf,cached,2407.166862,,http://arxiv.org/pdf/2407.16686v2,cs.CL,67dd6f39464a147efe31e8c1f6f547e6a9ccac1f258d5cf79274afbecc953fb8,,,,,,en,tech-report,digital,no,yes,GPT-4,generator,"Attack Success Rate (ASR), Recognition Success Rate (RSR)",jailbreak,6,none,yes,success,1.0,jailbreak,,,,
SCREEN_0004,Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs),Apurv Verma; Satyapriya Krishna; Sebastian Gehrmann; Madhavan Seshadri; Anu Pradhan; Tom Ault; Leslie Barrett; David Rabinowitz; John Doucette; NhatHai Phan,2024,,"Creating secure and resilient applications with large language models (LLM) requires anticipating, adjusting to, and countering unforeseen threats. Red-teaming has emerged as a critical technique for identifying vulnerabilities in real-world LLM implementations. This paper presents a detailed threat model and provides a systematization of knowledge (SoK) of red-teaming attacks on LLMs. We develop a taxonomy of attacks based on the stages of the LLM development and deployment process and extract various insights from previous research. In addition, we compile methods for defense and practical red-teaming strategies for practitioners. By delineating prominent attack motifs and shedding light on various entry points, this paper provides a framework for improving the security and robustness of LLM-based systems.",,http://arxiv.org/abs/2407.14937v2,arxiv,,,,,,,,,pdf_cache/pdfs/ecf78e536ccc496c.pdf,cached,2407.149372,,http://arxiv.org/pdf/2407.14937v2,cs.CL; cs.CR; I.2.7,417017aa0743262c9a0af4753b1fabca8440147096454f1065098e553bd0d3ce,,,,,,en,journal,digital,yes,no,GPT-4,analyst,"The paper does not specify evaluation metrics for wargames, as it focuses on red-teaming strategies and threat models.",bias|hallucination|information leakage|insecurity|misinformation|toxicity,6,https://github.com/dapurv5/awesome-red-teaming-llms,no,success,1.0,,,,,
SCREEN_0005,GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher,Youliang Yuan; Wenxiang Jiao; Wenxuan Wang; Jen-tse Huang; Pinjia He; Shuming Shi; Zhaopeng Tu,2023,,"Safety lies at the core of the development of Large Language Models (LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time to bypass the safety alignment of GPT-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages. Notably, we identify that LLMs seem to have a ''secret cipher'', and propose a novel SelfCipher that uses only role play and several demonstrations in natural language to evoke this capability. SelfCipher surprisingly outperforms existing human ciphers in almost all cases. Our code and data will be released at https://github.com/RobustNLP/CipherChat.",,http://arxiv.org/abs/2308.06463v2,arxiv,,,,,,,,,pdf_cache/pdfs/8edd3260ef9fc1b6.pdf,cached,2308.064632,,http://arxiv.org/pdf/2308.06463v2,cs.CL,277d7e2f688aebbf11282a899fc7f49ce572ae937c7d41726bfdc05722de9c88,,,,,,en,tech-report,digital,yes,no,GPT-4,player,Success rates of bypassing safety alignment in various domains using ciphers.,jailbreak|mismatched generalization,3,https://github.com/RobustNLP/CipherChat,yes,success,1.0,jailbreak,gpt4,,,github.com/RobustNLP/CipherChat
SCREEN_0006,GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts,Jiahao Yu; Xingwei Lin; Zheng Yu; Xinyu Xing,2023,,"Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial jailbreak attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing framework inspired by the AFL fuzzing framework. Instead of manual engineering, GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs. At its core, GPTFuzz starts with human-written templates as initial seeds, then mutates them to produce new templates. We detail three key components of GPTFuzz: a seed selection strategy for balancing efficiency and variability, mutate operators for creating semantically equivalent or similar sentences, and a judgment model to assess the success of a jailbreak attack. We evaluate GPTFuzz against various commercial and open-source LLMs, including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our results indicate that GPTFuzz consistently produces jailbreak templates with a high success rate, surpassing human-crafted templates. Remarkably, GPTFuzz achieves over 90% attack success rates against ChatGPT and Llama-2 models, even with suboptimal initial seed templates. We anticipate that GPTFuzz will be instrumental for researchers and practitioners in examining LLM robustness and will encourage further exploration into enhancing LLM safety.",,http://arxiv.org/abs/2309.10253v4,arxiv,,,,,,,,,pdf_cache/pdfs/a17a7ea23e44c1fa.pdf,cached,2309.102534,,http://arxiv.org/pdf/2309.10253v4,cs.AI,f2b76387acd042a27db07e8093dc4f4f4b876eb28d6a16fcf886432b31f1ffac,,,,,,en,tech-report,digital,no,yes,"ChatGPT, LLaMa-2, Vicuna",player,"Attack success rates (ASR) against various LLMs, comparison of human-written vs. auto-generated jailbreak templates, evaluation of robustness across different LLM versions",hallucination|jailbreak|misleading content|toxic output,6,none,yes,success,1.0,jailbreak,llama,,,mentioned
SCREEN_0007,Self Generated Wargame AI: Double Layer Agent Task Planning Based on Large Language Model,Y. Sun; J. Zhao; C. Yu; W. Wang; X. Zhou,2023,,"The large language models represented by ChatGPT have a disruptive impact on the field of artificial intelligence. But it mainly focuses on natural language processing, speech recognition, machine learning and natural language understanding. This paper innovatively applies the large language model to the field of intelligent decision-making, places the large language model in the decision-making center, and constructs an agent architecture with the large language model as the core. Based on this, it further proposes a two-layer agent task planning, issues and executes decision commands through the interaction of natural language, and carries out simulation verification through the wargame simulation environment. Through the game confrontation simulation experiment, it is found that the intelligent decision-making ability of the large language model is significantly stronger than the commonly used reinforcement learning AI and rule AI, and the intelligence, understandability and generalization are all better. And through experiments, it was found that the intelligence of the large language model is closely related to prompt. This work also extends the large language model from previous human-computer interaction to the field of intelligent decision-making, which has important reference value and significance for the development of intelligent decision-making.",,http://arxiv.org/abs/2312.01090v2,arxiv,,,,,,,,,pdf_cache/pdfs/0597889b0d9f288d.pdf,cached,2312.010902,,http://arxiv.org/pdf/2312.01090v2,cs.AI; cs.CL,c619e54cd221136cc3e38dcc595d8d950411f7bdc30cf28f787b8418b024cb0f,,,,,,en,tech-report,digital,yes,yes,GPT-4,player,"Winning rate, task mean scores (kill, control, survive), comparison with reinforcement learning algorithms",,4,https://github.com/sunyuxiang926/wargame/blob/b0118cd01fa46f2790a89895336df81519d91707/files/theses/explanation_of_domain_expert_knowledge.pdf,yes,success,1.0,,,,,
SCREEN_0008,Human vs. Machine: Behavioral Differences Between Expert Humans and Language Models in Wargame Simulations,Max Lamparth; Anthony Corso; Jacob Ganz; Oriana Skylar Mastro; Jacquelyn Schneider; Harold Trinkunas,2024,,"To some, the advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness while reducing the influence of human error and emotions. However, there is still debate about how AI systems, especially large language models (LLMs) that can be applied to many tasks, behave compared to humans in high-stakes military decision-making scenarios with the potential for increased risks towards escalation. To test this potential and scrutinize the use of LLMs for such purposes, we use a new wargame experiment with 214 national security experts designed to examine crisis escalation in a fictional U.S.-China scenario and compare the behavior of human player teams to LLM-simulated team responses in separate simulations. Here, we find that the LLM-simulated responses can be more aggressive and significantly affected by changes in the scenario. We show a considerable high-level agreement in the LLM and human responses and significant quantitative and qualitative differences in individual actions and strategic tendencies. These differences depend on intrinsic biases in LLMs regarding the appropriate level of violence following strategic instructions, the choice of LLM, and whether the LLMs are tasked to decide for a team of players directly or first to simulate dialog between a team of players. When simulating the dialog, the discussions lack quality and maintain a farcical harmony. The LLM simulations cannot account for human player characteristics, showing no significant difference even for extreme traits, such as ""pacifist"" or ""aggressive sociopath."" When probing behavioral consistency across individual moves of the simulation, the tested LLMs deviated from each other but generally showed somewhat consistent behavior. Our results motivate policymakers to be cautious before granting autonomy or following AI-based strategy recommendations.",,http://arxiv.org/abs/2403.03407v4,arxiv,,,,,,,,,pdf_cache/pdfs/98c5a99c4b8f8ce7.pdf,cached,2403.034074,,http://arxiv.org/pdf/2403.03407v4,cs.CY; cs.AI; cs.CL,ec141b10f69c294248e6f7e669f896c9bb298b2bbf2d1d02d340217cb0d05544,,,,,,en,tech-report,digital,yes,yes,"GPT-3.5, GPT-4",player,"High-level agreement in responses, quantitative and qualitative differences in actions and strategic tendencies, behavioral consistency across moves.",bias|dialog_quality|escalation|inconsistent_behavior|lack_of_human_characteristics,5,https://github.com/ancorso/LLMWargaming,yes,success,1.0,escalation,,,,
SCREEN_0009,Using LLMs for Tabletop Exercises within the Security Domain,Sam Hays; Jules White,2024,,"Tabletop exercises are a crucial component of many company's strategy to test and evaluate its preparedness for security incidents in a realistic way. Traditionally led by external firms specializing in cybersecurity, these exercises can be costly, time-consuming, and may not always align precisely with the client's specific needs. Large Language Models (LLMs) like ChatGPT offer a compelling alternative. They enable faster iteration, provide rich and adaptable simulations, and offer infinite patience in handling feedback and recommendations. This approach can enhances the efficiency and relevance of security preparedness exercises.",,http://arxiv.org/abs/2403.01626v1,arxiv,,,,,,,,,pdf_cache/pdfs/05ca7a3ab31fae00.pdf,cached,2403.016261,,http://arxiv.org/pdf/2403.01626v1,cs.CR,ce95c379cc411eadad668a577fd50d96901f7cba1202e11a2728a418d5e655f0,,,,,,en,tech-report,seminar,yes,yes,ChatGPT-4,facilitator,"Preparedness Equation, Preparedness Delta Equation, Unified Preparedness and Balance Score",,4,none,yes,success,1.0,,,,,
SCREEN_0010,Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?,Shuo Chen; Zhen Han; Bailan He; Zifeng Ding; Wenqian Yu; Philip Torr; Volker Tresp; Jindong Gu,2024,,"Various jailbreak attacks have been proposed to red-team Large Language Models (LLMs) and revealed the vulnerable safeguards of LLMs. Besides, some methods are not limited to the textual modality and extend the jailbreak attack to Multimodal Large Language Models (MLLMs) by perturbing the visual input. However, the absence of a universal evaluation benchmark complicates the performance reproduction and fair comparison. Besides, there is a lack of comprehensive evaluation of closed-source state-of-the-art (SOTA) models, especially MLLMs, such as GPT-4V. To address these issues, this work first builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies. Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models. We then conduct a deep analysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate better robustness against jailbreak attacks compared to open-source LLMs and MLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other open-source models. (3) The transferability of visual jailbreak methods is relatively limited compared to textual jailbreak methods. The dataset and code can be found https://github.com/chenxshuo/RedTeamingGPT4V",,http://arxiv.org/abs/2404.03411v2,arxiv,,,,,,,,,pdf_cache/pdfs/6c8af61c0ca66f96.pdf,cached,2404.034112,,http://arxiv.org/pdf/2404.03411v2,cs.LG; cs.CL; cs.CR,e1d1c78c042137b2d3de7ac3638678512776762aa602279af24d125156c37452,,,,,,en,workshop,digital,no,yes,GPT-4V,player,Attack success rate (ASR) calculated using refusal word detection and LLMs as judges.,jailbreak,6,https://github.com/chenxshuo/RedTeamingGPT4V,no,success,1.0,jailbreak,gpt4,,,github.com/chenxshuo/RedTeamingGPT4V
SCREEN_0011,Tiny Refinements Elicit Resilience: Toward Efficient Prefix-Model Against LLM Red-Teaming,Jiaxu Liu; Xiangyu Yin; Sihao Wu; Jianhong Wang; Meng Fang; Xinping Yi; Xiaowei Huang,2024,,"With the proliferation of red-teaming strategies for Large Language Models (LLMs), the deficiency in the literature about improving the safety and robustness of LLM defense strategies is becoming increasingly pronounced. This paper introduces the LLM-based \textbf{sentinel} model as a plug-and-play prefix module designed to reconstruct the input prompt with just a few ($<30$) additional tokens, effectively reducing toxicity in responses from target LLMs. The sentinel model naturally overcomes the \textit{parameter inefficiency} and \textit{limited model accessibility} for fine-tuning large target models. We employ an interleaved training regimen using Proximal Policy Optimization (PPO) to optimize both red team and sentinel models dynamically, incorporating a value head-sharing mechanism inspired by the multi-agent centralized critic to manage the complex interplay between agents. Our extensive experiments across text-to-text and text-to-image demonstrate the effectiveness of our approach in mitigating toxic outputs, even when dealing with larger models like \texttt{Llama-2}, \texttt{GPT-3.5} and \texttt{Stable-Diffusion}, highlighting the potential of our framework in enhancing safety and robustness in various applications.",,http://arxiv.org/abs/2405.12604v2,arxiv,,,,,,,,,pdf_cache/pdfs/b528c357dea0ec18.pdf,cached,2405.126042,,http://arxiv.org/pdf/2405.12604v2,cs.CL; cs.AI,fd37d05caf197f9e06925388d664f48bbf188be51593d2e6c9a7f5cab963cfce,,,,,,en,tech-report,digital,no,yes,"Llama-2, GPT-3.5, Stable-Diffusion",player,"Effectiveness in mitigating toxic outputs, toxicity score, reward model",bias|hallucination|hatefulness|misinformation|personal information leakage|toxic content,6,none,yes,success,1.0,,gpt35|llama,,,
SCREEN_0012,"Large Language Models Reveal Information Operation Goals, Tactics, and Narrative Frames",Keith Burghardt; Kai Chen; Kristina Lerman,2024,,"Adversarial information operations can destabilize societies by undermining fair elections, manipulating public opinions on policies, and promoting scams. Despite their widespread occurrence and potential impacts, our understanding of influence campaigns is limited by manual analysis of messages and subjective interpretation of their observable behavior. In this paper, we explore whether these limitations can be mitigated with large language models (LLMs), using GPT-3.5 as a case-study for coordinated campaign annotation. We first use GPT-3.5 to scrutinize 126 identified information operations spanning over a decade. We utilize a number of metrics to quantify the close (if imperfect) agreement between LLM and ground truth descriptions. We next extract coordinated campaigns from two large multilingual datasets from X (formerly Twitter) that respectively discuss the 2022 French election and 2023 Balikaran Philippine-U.S. military exercise in 2023. For each coordinated campaign, we use GPT-3.5 to analyze posts related to a specific concern and extract goals, tactics, and narrative frames, both before and after critical events (such as the date of an election). While the GPT-3.5 sometimes disagrees with subjective interpretation, its ability to summarize and interpret demonstrates LLMs' potential to extract higher-order indicators from text to provide a more complete picture of the information campaigns compared to previous methods.",,http://arxiv.org/abs/2405.03688v1,arxiv,,,,,,,,,pdf_cache/pdfs/8b0b2b50e0ea1cd6.pdf,cached,2405.036881,,http://arxiv.org/pdf/2405.03688v1,cs.CL; cs.LG,5ef7fe3e8b612e7e45f8749e8f7a08a296cbd9b829336e235f6a675abfa5f03e,,,,,,en,tech-report,digital,yes,no,GPT-3.5,analyst,Metrics to quantify agreement between LLM and ground truth descriptions; precision and recall for concern detection.,deception|disagreement with subjective interpretation|hallucination,4,https://github.com/KeithBurghardt/LLM Coordination,yes,success,1.0,deception,gpt35,,,
SCREEN_0013,RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent,Huiyu Xu; Wenhui Zhang; Zhibo Wang; Feng Xiao; Rui Zheng; Yunhe Feng; Zhongjie Ba; Kui Ren,2024,,"Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot. These applications have significantly expanded the attack surface of LLMs, exposing them to a variety of threats. Among them, jailbreak attacks that induce toxic responses through jailbreak prompts have raised critical safety concerns. To identify these threats, a growing number of red teaming approaches simulate potential adversarial scenarios by crafting jailbreak prompts to test the target LLM. However, existing red teaming methods do not consider the unique vulnerabilities of LLM in different scenarios, making it difficult to adjust the jailbreak prompts to find context-specific vulnerabilities. Meanwhile, these methods are limited to refining jailbreak templates using a few mutation operations, lacking the automation and scalability to adapt to different scenarios. To enable context-aware and efficient red teaming, we abstract and model existing attacks into a coherent concept called ""jailbreak strategy"" and propose a multi-agent LLM system named RedAgent that leverages these strategies to generate context-aware jailbreak prompts. By self-reflecting on contextual feedback in an additional memory buffer, RedAgent continuously learns how to leverage these strategies to achieve effective jailbreaks in specific contexts. Extensive experiments demonstrate that our system can jailbreak most black-box LLMs in just five queries, improving the efficiency of existing red teaming methods by two times. Additionally, RedAgent can jailbreak customized LLM applications more efficiently. By generating context-aware jailbreak prompts towards applications on GPTs, we discover 60 severe vulnerabilities of these real-world applications with only two queries per vulnerability. We have reported all found issues and communicated with OpenAI and Meta for bug fixes.",,http://arxiv.org/abs/2407.16667v1,arxiv,,,,,,,,,pdf_cache/pdfs/332cf181bb5801e9.pdf,cached,2407.166671,,http://arxiv.org/pdf/2407.16667v1,cs.CR; cs.AI; cs.CL,bcad867c8526977dd6e2964ddd1775525ee6d9353ea80ff0fe52e56784443ed2,,,,,,en,tech-report,digital,yes,no,GPT-4,player,Efficiency of jailbreaks measured by number of queries needed; success rate of jailbreaks,jailbreak,6,none,yes,success,1.0,jailbreak,gpt4,,,
SCREEN_0014,On Large Language Models in National Security Applications,William N. Caballero; Phillip R. Jenkins,2024,,"The overwhelming success of GPT-4 in early 2023 highlighted the transformative potential of large language models (LLMs) across various sectors, including national security. This article explores the implications of LLM integration within national security contexts, analyzing their potential to revolutionize information processing, decision-making, and operational efficiency. Whereas LLMs offer substantial benefits, such as automating tasks and enhancing data analysis, they also pose significant risks, including hallucinations, data privacy concerns, and vulnerability to adversarial attacks. Through their coupling with decision-theoretic principles and Bayesian reasoning, LLMs can significantly improve decision-making processes within national security organizations. Namely, LLMs can facilitate the transition from data to actionable decisions, enabling decision-makers to quickly receive and distill available information with less manpower. Current applications within the US Department of Defense and beyond are explored, e.g., the USAF's use of LLMs for wargaming and automatic summarization, that illustrate their potential to streamline operations and support decision-making. However, these applications necessitate rigorous safeguards to ensure accuracy and reliability. The broader implications of LLM integration extend to strategic planning, international relations, and the broader geopolitical landscape, with adversarial nations leveraging LLMs for disinformation and cyber operations, emphasizing the need for robust countermeasures. Despite exhibiting ""sparks"" of artificial general intelligence, LLMs are best suited for supporting roles rather than leading strategic decisions. Their use in training and wargaming can provide valuable insights and personalized learning experiences for military personnel, thereby improving operational readiness.",,http://arxiv.org/abs/2407.03453v1,arxiv,,,,,,,,,pdf_cache/pdfs/cc992e7dd736a964.pdf,cached,2407.034531,,http://arxiv.org/pdf/2407.03453v1,cs.CR; cs.CY; cs.LG; stat.AP; 62P99,9d499eb90ad00921e4d4645edc0531942240268a01f4a9bcc82a4c0a39090d90,,,,,,en,tech-report,digital,no,no,GPT-4,player,Not explicitly mentioned,data privacy concerns|hallucination|hallucinations|vulnerability to adversarial attacks,6,none,yes,success,1.0,hallucination,gpt4,,accuracy,
SCREEN_0015,SeqAR: Jailbreak LLMs with Sequential Auto-Generated Characters,Yan Yang; Zeguan Xiao; Xin Lu; Hongru Wang; Xuetao Wei; Hailiang Huang; Guanhua Chen; Yun Chen,2024,,"The widespread applications of large language models (LLMs) have brought about concerns regarding their potential misuse. Although aligned with human preference data before release, LLMs remain vulnerable to various malicious attacks. In this paper, we adopt a red-teaming strategy to enhance LLM safety and introduce SeqAR, a simple yet effective framework to design jailbreak prompts automatically. The SeqAR framework generates and optimizes multiple jailbreak characters and then applies sequential jailbreak characters in a single query to bypass the guardrails of the target LLM. Different from previous work which relies on proprietary LLMs or seed jailbreak templates crafted by human expertise, SeqAR can generate and optimize the jailbreak prompt in a cold-start scenario using open-sourced LLMs without any seed jailbreak templates. Experimental results show that SeqAR achieves attack success rates of 88% and 60% in bypassing the safety alignment of GPT-3.5-1106 and GPT-4, respectively. Furthermore, we extensively evaluate the transferability of the generated templates across different LLMs and held-out malicious requests, while also exploring defense strategies against the jailbreak attack designed by SeqAR.",,http://arxiv.org/abs/2407.01902v2,arxiv,,,,,,,,,pdf_cache/pdfs/61682f46fc0c94e3.pdf,cached,2407.019022,,http://arxiv.org/pdf/2407.01902v2,cs.CR; cs.AI; cs.CL,35104d307d6707b62d9536ef744c3597fd8d55acbd3dc7be69d5559d77b248b0,,,,,,en,tech-report,digital,yes,yes,GPT-4,player,"Attack success rates (ASR) on various LLMs, transferability of generated templates, effectiveness of defense strategies.",competing objectives|jailbreak|role-playing distraction,5,https://github.com/sufenlp/SeqAR,yes,success,1.0,jailbreak,gpt4|gpt35,,,
SCREEN_0016,Atoxia: Red-teaming Large Language Models with Target Toxic Answers,Yuhao Du; Zhuo Li; Pengyu Cheng; Xiang Wan; Anningzhe Gao,2024,,"Despite the substantial advancements in artificial intelligence, large language models (LLMs) remain being challenged by generation safety. With adversarial jailbreaking prompts, one can effortlessly induce LLMs to output harmful content, causing unexpected negative social impacts. This vulnerability highlights the necessity for robust LLM red-teaming strategies to identify and mitigate such risks before large-scale application. To detect specific types of risks, we propose a novel red-teaming method that $\textbf{A}$ttacks LLMs with $\textbf{T}$arget $\textbf{Toxi}$c $\textbf{A}$nswers ($\textbf{Atoxia}$). Given a particular harmful answer, Atoxia generates a corresponding user query and a misleading answer opening to examine the internal defects of a given LLM. The proposed attacker is trained within a reinforcement learning scheme with the LLM outputting probability of the target answer as the reward. We verify the effectiveness of our method on various red-teaming benchmarks, such as AdvBench and HH-Harmless. The empirical results demonstrate that Atoxia can successfully detect safety risks in not only open-source models but also state-of-the-art black-box models such as GPT-4o.",,http://arxiv.org/abs/2408.14853v2,arxiv,,,,,,,,,pdf_cache/pdfs/3ebce708a3c57bf7.pdf,cached,2408.148532,,http://arxiv.org/pdf/2408.14853v2,cs.CL; cs.AI; cs.CR,d2374b0b86dee0e299bdbccdc6459690d443f8e6cb3ef3735eee6ff1525286b4,,,,,,en,tech-report,digital,yes,no,GPT-4o,player,Effectiveness on red-teaming benchmarks such as AdvBench and HH-Harmless; empirical demonstration of detecting safety risks in LLMs.,deception|jailbreak|misleading responses|toxic content generation,4,https://github.com/DuYooho/Atoxia,yes,success,1.0,deception|jailbreak,,,,mentioned
SCREEN_0017,Kov: Transferable and Naturalistic Black-Box LLM Attacks using Markov Decision Processes and Tree Search,Robert J. Moss,2024,,"Eliciting harmful behavior from large language models (LLMs) is an important task to ensure the proper alignment and safety of the models. Often when training LLMs, ethical guidelines are followed yet alignment failures may still be uncovered through red teaming adversarial attacks. This work frames the red-teaming problem as a Markov decision process (MDP) and uses Monte Carlo tree search to find harmful behaviors of black-box, closed-source LLMs. We optimize token-level prompt suffixes towards targeted harmful behaviors on white-box LLMs and include a naturalistic loss term, log-perplexity, to generate more natural language attacks for better interpretability. The proposed algorithm, Kov, trains on white-box LLMs to optimize the adversarial attacks and periodically evaluates responses from the black-box LLM to guide the search towards more harmful black-box behaviors. In our preliminary study, results indicate that we can jailbreak black-box models, such as GPT-3.5, in only 10 queries, yet fail on GPT-4$-$which may indicate that newer models are more robust to token-level attacks. All work to reproduce these results is open sourced (https://github.com/sisl/Kov.jl).",,http://arxiv.org/abs/2408.08899v1,arxiv,,,,,,,,,pdf_cache/pdfs/4e05ec3dbd05fa5d.pdf,cached,2408.088991,,http://arxiv.org/pdf/2408.08899v1,cs.CR; cs.AI; cs.CL; cs.LG,a31c5ac4b72e36096e4993610cff3648d94897c0398364b3f6bdd9747266ff27,,,,,,en,tech-report,digital,no,yes,GPT-3.5,player,"Harmfulness or toxicity of the response using OpenAI's moderation framework, average score across eleven categories.",alignment failure|jailbreak,6,https://github.com/sisl/Kov.jl,yes,success,1.0,jailbreak,gpt4|gpt35,,,github.com/sisl/Kov
SCREEN_0018,Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles,Xiongtao Sun; Deyue Zhang; Dongdong Yang; Quanchen Zou; Hui Li,2024,,"Large language models (LLMs) have significantly enhanced the performance of numerous applications, from intelligent conversations to text generation. However, their inherent security vulnerabilities have become an increasingly significant challenge, especially with respect to jailbreak attacks. Attackers can circumvent the security mechanisms of these LLMs, breaching security constraints and causing harmful outputs. Focusing on multi-turn semantic jailbreak attacks, we observe that existing methods lack specific considerations for the role of multiturn dialogues in attack strategies, leading to semantic deviations during continuous interactions. Therefore, in this paper, we establish a theoretical foundation for multi-turn attacks by considering their support in jailbreak attacks, and based on this, propose a context-based contextual fusion black-box jailbreak attack method, named Context Fusion Attack (CFA). This method approach involves filtering and extracting key terms from the target, constructing contextual scenarios around these terms, dynamically integrating the target into the scenarios, replacing malicious key terms within the target, and thereby concealing the direct malicious intent. Through comparisons on various mainstream LLMs and red team datasets, we have demonstrated CFA's superior success rate, divergence, and harmfulness compared to other multi-turn attack strategies, particularly showcasing significant advantages on Llama3 and GPT-4.",,http://arxiv.org/abs/2408.04686v1,arxiv,,,,,,,,,pdf_cache/pdfs/f8de313da77d92bf.pdf,cached,2408.046861,,http://arxiv.org/pdf/2408.04686v1,cs.CL; cs.AI,a3d05932d600b6fe2f5ca161c213247969ba2740a35d440a756950731e0d5c07,,,,,,en,tech-report,digital,yes,yes,"GPT-4, Llama3",player,"success rate, divergence, harmfulness, attack stability, attack consistency, attack severity",false positives|jailbreak|semantic deviation,6,none,yes,success,1.0,jailbreak,gpt4,,,
SCREEN_0019,SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models,Muxi Diao; Rumei Li; Shiyang Liu; Guogang Liao; Jingang Wang; Xunliang Cai; Weiran Xu,2024,,"As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial. A promising approach to address these concerns involves training models to automatically generate adversarial prompts for red teaming. However, the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness of current adversarial methods, which struggle to specifically target and explore the weaknesses of these models. To tackle these challenges, we introduce the $\mathbf{S}\text{elf-}\mathbf{E}\text{volving }\mathbf{A}\text{dversarial }\mathbf{S}\text{afety }\mathbf{(SEAS)}$ optimization framework, which enhances security by leveraging data generated by the model itself. SEAS operates through three iterative stages: Initialization, Attack, and Adversarial Optimization, refining both the Red Team and Target models to improve robustness and safety. This framework reduces reliance on manual testing and significantly enhances the security capabilities of LLMs. Our contributions include a novel adversarial framework, a comprehensive safety dataset, and after three iterations, the Target model achieves a security level comparable to GPT-4, while the Red Team model shows a marked increase in attack success rate (ASR) against advanced models. Our code and datasets are released at https://SEAS-LLM.github.io/.",,http://arxiv.org/abs/2408.02632v2,arxiv,,,,,,,,,pdf_cache/pdfs/3bf9bdfa1cf9928e.pdf,cached,2408.026322,,http://arxiv.org/pdf/2408.02632v2,cs.CL; cs.AI,c08a94a1696bbb771d6158208ab52b40cc7cf8c81b8d531ffc747b004cf57bd1,,,,,,en,tech-report,digital,no,yes,GPT-4,player,"Attack Success Rate (ASR), diversity of generated adversarial prompts",adversarial prefix|code nesting|crimes and illegal activities|goal hijacking|health harm|jailbreak|one sided statement|privacy and property|role play|session completion|token manipulation|unfairness and discrimination|unsafe instruction|word play,6,https://SEAS-LLM.github.io/,yes,success,1.0,,gpt4,,,
SCREEN_0020,RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking,Yifan Jiang; Kriti Aggarwal; Tanmay Laud; Kashif Munir; Jay Pujara; Subhabrata Mukherjee,2024,,"The rapid progress of Large Language Models (LLMs) has opened up new opportunities across various domains and applications; yet it also presents challenges related to potential misuse. To mitigate such risks, red teaming has been employed as a proactive security measure to probe language models for harmful outputs via jailbreak attacks. However, current jailbreak attack approaches are single-turn with explicit malicious queries that do not fully capture the complexity of real-world interactions. In reality, users can engage in multi-turn interactions with LLM-based chat assistants, allowing them to conceal their true intentions in a more covert manner. To bridge this gap, we, first, propose a new jailbreak approach, RED QUEEN ATTACK. This method constructs a multi-turn scenario, concealing the malicious intent under the guise of preventing harm. We craft 40 scenarios that vary in turns and select 14 harmful categories to generate 56k multi-turn attack data points. We conduct comprehensive experiments on the RED QUEEN ATTACK with four representative LLM families of different sizes. Our experiments reveal that all LLMs are vulnerable to RED QUEEN ATTACK, reaching 87.62% attack success rate on GPT-4o and 75.4% on Llama3-70B. Further analysis reveals that larger models are more susceptible to the RED QUEEN ATTACK, with multi-turn structures and concealment strategies contributing to its success. To prioritize safety, we introduce a straightforward mitigation strategy called RED QUEEN GUARD, which aligns LLMs to effectively counter adversarial attacks. This approach reduces the attack success rate to below 1% while maintaining the model's performance across standard benchmarks. Full implementation and dataset are publicly accessible at https://github.com/kriti-hippo/red_queen.",,http://arxiv.org/abs/2409.17458v2,arxiv,,,,,,,,,pdf_cache/pdfs/2e78e167a4d62fc8.pdf,cached,2409.174582,,http://arxiv.org/pdf/2409.17458v2,cs.CR; cs.CL; cs.LG,b4e68d65a161fd1ccac60c91c277689629420ae764f86f773c8e88affa966fc4,,,,,,en,tech-report,digital,yes,yes,"GPT-4, Llama3-70B",player,"Attack success rate (ASR), model performance on standard benchmarks, effectiveness of RED QUEEN GUARD mitigation strategy",hallucination|jailbreak|misalignment|overconfidence,6,https://github.com/kriti-hippo/red_queen,yes,success,1.0,jailbreak,,,,github.com/kriti-hippo/red_queen
SCREEN_0021,Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments,Maria Rigaki; Carlos Catania; Sebastian Garcia,2024,,"Large Language Models (LLMs) have shown remarkable potential across various domains, including cybersecurity. Using commercial cloud-based LLMs may be undesirable due to privacy concerns, costs, and network connectivity constraints. In this paper, we present Hackphyr, a locally fine-tuned LLM to be used as a red-team agent within network security environments. Our fine-tuned 7 billion parameter model can run on a single GPU card and achieves performance comparable with much larger and more powerful commercial models such as GPT-4. Hackphyr clearly outperforms other models, including GPT-3.5-turbo, and baselines, such as Q-learning agents in complex, previously unseen scenarios. To achieve this performance, we generated a new task-specific cybersecurity dataset to enhance the base model's capabilities. Finally, we conducted a comprehensive analysis of the agents' behaviors that provides insights into the planning abilities and potential shortcomings of such agents, contributing to the broader understanding of LLM-based agents in cybersecurity contexts",,http://arxiv.org/abs/2409.11276v1,arxiv,,,,,,,,,pdf_cache/pdfs/3ddf2dab194500c2.pdf,cached,2409.112761,,http://arxiv.org/pdf/2409.11276v1,cs.CR,a33679fbae5804404478f598ea5bcee3ad85469e37c24c16452504974e035ec9,,,,,,en,tech-report,digital,no,yes,Zephyr-7b-β,player,Performance comparison with commercial LLMs and RL baselines in complex scenarios; behavioral analysis of agents' actions.,,6,https://github.com/stratosphereips/NetSecGame,yes,success,1.0,,gpt4|gpt35,,,
SCREEN_0022,"Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks",Benji Peng; Keyu Chen; Ming Li; Pohsun Feng; Ziqian Bi; Junyu Liu; Qian Niu,2024,,"Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns. This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability. Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises. A comprehensive analysis of bias mitigation strategies is presented, including approaches from pre-processing interventions to in-training adjustments and post-processing refinements. The article also probes the complexity of distinguishing LLM-generated content from human-produced text, introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances. Moreover, LLM vulnerabilities, including jailbreak attacks and prompt injection exploits, are analyzed by looking into different case studies and large-scale competitions like HackAPrompt. This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field.",,http://arxiv.org/abs/2409.08087v2,arxiv,,,,,,,,,pdf_cache/pdfs/98c4a1be251ffa28.pdf,cached,2409.080872,,http://arxiv.org/pdf/2409.08087v2,cs.CR,a7e9d9d1486720a4402067bd7e76c7737ae7df39a90adb6af1ac955b7cc3e65a,,,,,,en,tech-report,none,no,no,GPT-4,analyst,"Diverse evaluation techniques including controlled input studies, red teaming exercises, and detection mechanisms like DetectGPT and watermarking techniques.",bias|deception|factual_error|jailbreak|misinformation|prompt injection|prompt_sensitivity,6,none,yes,success,1.0,bias|deception|factual_error|jailbreak|prompt_sensitivity,,,accuracy,
SCREEN_0023,Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring,Honglin Mu; Han He; Yuxin Zhou; Yunlong Feng; Yang Xu; Libo Qin; Xiaoming Shi; Zeming Liu; Xudong Han; Qi Shi; Qingfu Zhu; Wanxiang Che,2024,,"Large language model (LLM) safety is a critical issue, with numerous studies employing red team testing to enhance model security. Among these, jailbreak methods explore potential vulnerabilities by crafting malicious prompts that induce model outputs contrary to safety alignments. Existing black-box jailbreak methods often rely on model feedback, repeatedly submitting queries with detectable malicious instructions during the attack search process. Although these approaches are effective, the attacks may be intercepted by content moderators during the search process. We propose an improved transfer attack method that guides malicious prompt construction by locally training a mirror model of the target black-box model through benign data distillation. This method offers enhanced stealth, as it does not involve submitting identifiable malicious instructions to the target model during the search phase. Our approach achieved a maximum attack success rate of 92%, or a balanced value of 80% with an average of 1.5 detectable jailbreak queries per sample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore the need for more robust defense mechanisms.",,http://arxiv.org/abs/2410.21083v2,arxiv,,,,,,,,,pdf_cache/pdfs/3dc1ef50393c1d25.pdf,cached,2410.210832,,http://arxiv.org/pdf/2410.21083v2,cs.CL; cs.AI,d80d3295708d2497af7f844f23ad0019cb11d74b397c5bf5d724dc97230aaa05,,,,,,en,tech-report,none,no,yes,GPT-3.5 Turbo,none,Attack Success Rate (ASR) using exact match and semantic classification discriminators,bias|jailbreak,6,none,yes,success,1.0,bias|jailbreak,gpt35,,,
SCREEN_0024,Escalation Risks from Language Models in Military and Diplomatic Decision-Making,Juan-Pablo Rivera; Gabriel Mukobi; Anka Reuel; Max Lamparth; Chandler Smith; Jacquelyn Schneider,2024,"The 2024 ACM Conference on Fairness, Accountability, and
  Transparency (FAccT 24), June 3-6, 2024, Rio de Janeiro, Brazil","Governments are increasingly considering integrating autonomous AI agents in high-stakes military and foreign-policy decision-making, especially with the emergence of advanced generative AI models like GPT-4. Our work aims to scrutinize the behavior of multiple AI agents in simulated wargames, specifically focusing on their predilection to take escalatory actions that may exacerbate multilateral conflicts. Drawing on political science and international relations literature about escalation dynamics, we design a novel wargame simulation and scoring framework to assess the escalation risks of actions taken by these agents in different scenarios. Contrary to prior studies, our research provides both qualitative and quantitative insights and focuses on large language models (LLMs). We find that all five studied off-the-shelf LLMs show forms of escalation and difficult-to-predict escalation patterns. We observe that models tend to develop arms-race dynamics, leading to greater conflict, and in rare cases, even to the deployment of nuclear weapons. Qualitatively, we also collect the models' reported reasonings for chosen actions and observe worrying justifications based on deterrence and first-strike tactics. Given the high stakes of military and foreign-policy contexts, we recommend further examination and cautious consideration before deploying autonomous language model agents for strategic military or diplomatic decision-making.",10.1145/3630106.3658942,http://arxiv.org/abs/2401.03408v1,arxiv,,,,,,,,,pdf_cache/pdfs/10_1145_3630106_3658942.pdf,cached,2401.034081,,http://arxiv.org/pdf/2401.03408v1,cs.AI; cs.CL; cs.CY; cs.MA,170d63f15f6d9f136c70aedda83086da4ed28e2dc56bbe45dfbbe943585846c5,,,,,,en,conference,digital,yes,yes,"GPT-4, GPT-3.5, Claude 2, Llama-2 (70B) Chat, GPT-4-Base",player,"Escalation scores based on the escalation scoring framework, qualitative analysis of models' reasoning for actions",arms-race dynamics|deployment of nuclear weapons|escalation|worrying justifications for actions,5,https://github.com/jprivera44/EscalAItion,no,success,1.0,escalation,gpt4,,,
SCREEN_0025,TroubleLLM: Align to Red Team Expert,Zhuoer Xu; Jianping Zhang; Shiwen Cui; Changhua Meng; Weiqiang Wang,2024,,"Large Language Models (LLMs) become the start-of-the-art solutions for a variety of natural language tasks and are integrated into real-world applications. However, LLMs can be potentially harmful in manifesting undesirable safety issues like social biases and toxic content. It is imperative to assess its safety issues before deployment. However, the quality and diversity of test prompts generated by existing methods are still far from satisfactory. Not only are these methods labor-intensive and require large budget costs, but the controllability of test prompt generation is lacking for the specific testing domain of LLM applications. With the idea of LLM for LLM testing, we propose the first LLM, called TroubleLLM, to generate controllable test prompts on LLM safety issues. Extensive experiments and human evaluation illustrate the superiority of TroubleLLM on generation quality and generation controllability.",,http://arxiv.org/abs/2403.00829v1,arxiv,,,,,,,,,pdf_cache/pdfs/9ac27018659569c8.pdf,cached,2403.008291,,http://arxiv.org/pdf/2403.00829v1,cs.AI; cs.CL,5e76f54073a2fcdb875955f86c8de5ae1b8af0fe97b6aeaeeb063027af0a25a9,,,,,,en,tech-report,digital,yes,no,BELLE,generator,"Naturalness, diversity, effectiveness, keyword presence, topic relevance, instruction style adherence",,4,https://github.com/LianjiaTech/BELLE,yes,success,1.0,,,,human_evaluation,
SCREEN_0026,Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast,Xiangming Gu; Xiaosen Zheng; Tianyu Pang; Chao Du; Qian Liu; Ye Wang; Jing Jiang; Min Lin,2024,,"A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate. Our project page is available at https://sail-sg.github.io/Agent-Smith/.",,http://arxiv.org/abs/2402.08567v2,arxiv,,,,,,,,,pdf_cache/pdfs/ad6fd42248a1e918.pdf,cached,2402.085672,,http://arxiv.org/pdf/2402.08567v2,cs.CL; cs.CR; cs.CV; cs.LG; cs.MA,da15a4594ec3bc707846f6c0b9cd71b4ba76ed840d934f3f87aa78618c86fcb6,,,,,,en,conference,digital,yes,yes,LLaVA-1.5,player,"Infection ratio, jailbreak success rate, minimum CLIP score, BLEU score, toxicity score",adversarial vulnerability|alignment issues|jailbreak,6,https://github.com/sail-sg/Agent-Smith,no,success,1.0,jailbreak,,,,
SCREEN_0027,IterAlign: Iterative Constitutional Alignment of Large Language Models,Xiusi Chen; Hongzhi Wen; Sreyashi Nag; Chen Luo; Qingyu Yin; Ruirui Li; Zheng Li; Wei Wang,2024,,"With the rapid development of large language models (LLMs), aligning LLMs with human values and societal norms to ensure their reliability and safety has become crucial. Reinforcement learning with human feedback (RLHF) and Constitutional AI (CAI) have been proposed for LLM alignment. However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming. To overcome these drawbacks, we study constitution-based LLM alignment and propose a data-driven constitution discovery and self-alignment framework called IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM and automatically discovers new constitutions using a stronger LLM. These constitutions are then used to guide self-correction of the base LLM. Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target the alignment gaps in the current LLM. Empirical results on several safety benchmark datasets and multiple base LLMs show that IterAlign successfully improves truthfulness, helpfulness, harmlessness and honesty, improving the LLM alignment by up to $13.5\%$ in harmlessness.",,http://arxiv.org/abs/2403.18341v1,arxiv,,,,,,,,,pdf_cache/pdfs/06aa5dc4a38d2f3b.pdf,cached,2403.183411,,http://arxiv.org/pdf/2403.18341v1,cs.CL,6bdb5504965731afedf6c6ec39f127cf76c2e44548123b440c75efeb02007299,,,,,,en,tech-report,none,no,yes,GPT-3.5-turbo,analyst,"Empirical results on safety benchmark datasets, improvement in truthfulness, helpfulness, harmlessness, and honesty by up to 13.5% in harmlessness.",,6,none,yes,success,1.0,,,,,
SCREEN_0028,Risk and Response in Large Language Models: Evaluating Key Threat Categories,Bahareh Harandizadeh; Abel Salinas; Fred Morstatter,2024,,"This paper explores the pressing issue of risk assessment in Large Language Models (LLMs) as they become increasingly prevalent in various applications. Focusing on how reward models, which are designed to fine-tune pretrained LLMs to align with human values, perceive and categorize different types of risks, we delve into the challenges posed by the subjective nature of preference-based training data. By utilizing the Anthropic Red-team dataset, we analyze major risk categories, including Information Hazards, Malicious Uses, and Discrimination/Hateful content. Our findings indicate that LLMs tend to consider Information Hazards less harmful, a finding confirmed by a specially developed regression model. Additionally, our analysis shows that LLMs respond less stringently to Information Hazards compared to other risks. The study further reveals a significant vulnerability of LLMs to jailbreaking attacks in Information Hazard scenarios, highlighting a critical security concern in LLM risk assessment and emphasizing the need for improved AI safety measures.",,http://arxiv.org/abs/2403.14988v1,arxiv,,,,,,,,,pdf_cache/pdfs/8c2483e066eb3891.pdf,cached,2403.149881,,http://arxiv.org/pdf/2403.14988v1,cs.CL,2847ae085dd392bc96888b53ae38d63ac98a4796debeb2278132bffd1d214b78,,,,,,en,tech-report,none,no,yes,Claude,analyst,"Harmlessness score, regression model predictions, action categories classification",bias|jailbreak|jailbreaking,6,https://github.com/anthropics/hh-rlhf,yes,success,1.0,bias|jailbreak,,,,
SCREEN_0029,Large language models in 6G security: challenges and opportunities,Tri Nguyen; Huong Nguyen; Ahmad Ijaz; Saeid Sheikhi; Athanasios V. Vasilakos; Panos Kostakos,2024,,"The rapid integration of Generative AI (GenAI) and Large Language Models (LLMs) in sectors such as education and healthcare have marked a significant advancement in technology. However, this growth has also led to a largely unexplored aspect: their security vulnerabilities. As the ecosystem that includes both offline and online models, various tools, browser plugins, and third-party applications continues to expand, it significantly widens the attack surface, thereby escalating the potential for security breaches. These expansions in the 6G and beyond landscape provide new avenues for adversaries to manipulate LLMs for malicious purposes. We focus on the security aspects of LLMs from the viewpoint of potential adversaries. We aim to dissect their objectives and methodologies, providing an in-depth analysis of known security weaknesses. This will include the development of a comprehensive threat taxonomy, categorizing various adversary behaviors. Also, our research will concentrate on how LLMs can be integrated into cybersecurity efforts by defense teams, also known as blue teams. We will explore the potential synergy between LLMs and blockchain technology, and how this combination could lead to the development of next-generation, fully autonomous security solutions. This approach aims to establish a unified cybersecurity strategy across the entire computing continuum, enhancing overall digital security infrastructure.",,http://arxiv.org/abs/2403.12239v1,arxiv,,,,,,,,,pdf_cache/pdfs/a11509397773a4f8.pdf,cached,2403.122391,,http://arxiv.org/pdf/2403.12239v1,cs.CR; cs.DC,bfffcf7229010f5d9fc1d15eb02d89b6796975e057eb00756952b5daac2a0a89,,,,,,en,tech-report,none,no,no,none,none,none,Design Flaws in Insecure Plugins|Disclosure of Sensitive Information|Excessive Agency in Models|Insecure Output Handling|Model Denial of Service Attacks|Model Theft|Overreliance on AI Models|Prompt Injection|Supply Chain Concerns|Training Data Poisoning|deception|escalation,6,none,yes,success,1.0,deception|escalation,,,,
SCREEN_0030,Distract Large Language Models for Automatic Jailbreak Attack,Zeguan Xiao; Yan Yang; Guanhua Chen; Yun Chen,2024,,"Extensive efforts have been made before the public release of Large language models (LLMs) to align their behaviors with human values. However, even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors. In this work, we propose a novel black-box jailbreak framework for automated red teaming of LLMs. We designed malicious content concealing and memory reframing with an iterative optimization algorithm to jailbreak LLMs, motivated by the research about the distractibility and over-confidence phenomenon of LLMs. Extensive experiments of jailbreaking both open-source and proprietary LLMs demonstrate the superiority of our framework in terms of effectiveness, scalability and transferability. We also evaluate the effectiveness of existing jailbreak defense methods against our attack and highlight the crucial need to develop more effective and practical defense strategies.",,http://arxiv.org/abs/2403.08424v2,arxiv,,,,,,,,,pdf_cache/pdfs/0e9c601b3786fc03.pdf,cached,2403.084242,,http://arxiv.org/pdf/2403.08424v2,cs.CR; cs.AI; cs.CL,68b8698a6875ea0d7ee3c63abb636f9c9109af2037d1687e80f99810fead8b0b,,,,,,en,tech-report,digital,no,no,GPT-4,generator,"Top-1 attack success rates (ASR), accuracy, True Positive Rate (TPR), False Positive Rate (FPR)",deception|jailbreak,6,https://github.com/sufenlp/AttanttionShiftJailbreak,yes,success,1.0,deception|jailbreak,,,accuracy,mentioned
SCREEN_0031,Aligners: Decoupling LLMs and Alignment,Lilian Ngweta; Mayank Agarwal; Subha Maity; Alex Gittens; Yuekai Sun; Mikhail Yurochkin,2024,,"Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We use the same synthetic data to train inspectors, binary miss-alignment classification models to guide a ""squad"" of multiple aligners. Our empirical results demonstrate consistent improvements when applying aligner squad to various LLMs, including chat-aligned models, across several instruction-following and red-teaming datasets.",,http://arxiv.org/abs/2403.04224v4,arxiv,,,,,,,,,pdf_cache/pdfs/c228ce227a991aaf.pdf,cached,2403.042244,,http://arxiv.org/pdf/2403.04224v4,cs.CL; cs.AI; cs.LG,fb89d4d4f13d867640cb2ad260132b3ff820aa928fedfe806700602f07baa54f,,,,,,en,tech-report,digital,no,yes,Falcon-40B,generator,"Win Rates using PairRM, average frequency of evaluators choosing responses aligned by aligners over unaligned responses",,6,https://github.com/lilianngweta/aligners,yes,success,1.0,,,,,
SCREEN_0032,AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks,Jiacen Xu; Jack W. Stokes; Geoff McDonald; Xuesong Bai; David Marshall; Siyue Wang; Adith Swaminathan; Zhou Li,2024,,"Large language models (LLMs) have demonstrated impressive results on natural language tasks, and security researchers are beginning to employ them in both offensive and defensive systems. In cyber-security, there have been multiple research efforts that utilize LLMs focusing on the pre-breach stage of attacks like phishing and malware generation. However, so far there lacks a comprehensive study regarding whether LLM-based systems can be leveraged to simulate the post-breach stage of attacks that are typically human-operated, or ""hands-on-keyboard"" attacks, under various attack techniques and environments. As LLMs inevitably advance, they may be able to automate both the pre- and post-breach attack stages. This shift may transform organizational attacks from rare, expert-led events to frequent, automated operations requiring no expertise and executed at automation speed and scale. This risks fundamentally changing global computer security and correspondingly causing substantial economic impacts, and a goal of this work is to better understand these risks now so we can better prepare for these inevitable ever-more-capable LLMs on the horizon. On the immediate impact side, this research serves three purposes. First, an automated LLM-based, post-breach exploitation framework can help analysts quickly test and continually improve their organization's network security posture against previously unseen attacks. Second, an LLM-based penetration test system can extend the effectiveness of red teams with a limited number of human analysts. Finally, this research can help defensive systems and teams learn to detect novel attack behaviors preemptively before their use in the wild....",,http://arxiv.org/abs/2403.01038v1,arxiv,,,,,,,,,pdf_cache/pdfs/fc9d7d85601ab72b.pdf,cached,2403.010381,,http://arxiv.org/pdf/2403.01038v1,cs.CR; cs.AI,b007dcf70c512fe1f4266acb06a802db9012c9f87c9f57c7ac815e5cc0770b3b,,,,,,en,tech-report,digital,no,no,GPT-4,generator,"Success rate of attack tasks, effectiveness of components like experience manager",context tracking issues|hallucination|verbose responses,5,none,yes,success,1.0,,,,,
SCREEN_0033,Bias patterns in the application of LLMs for clinical decision support: A comprehensive study,Raphael Poulain; Hamed Fayyaz; Rahmatollah Beheshti,2024,,"Large Language Models (LLMs) have emerged as powerful candidates to inform clinical decision-making processes. While these models play an increasingly prominent role in shaping the digital landscape, two growing concerns emerge in healthcare applications: 1) to what extent do LLMs exhibit social bias based on patients' protected attributes (like race), and 2) how do design choices (like architecture design and prompting strategies) influence the observed biases? To answer these questions rigorously, we evaluated eight popular LLMs across three question-answering (QA) datasets using clinical vignettes (patient descriptions) standardized for bias evaluations. We employ red-teaming strategies to analyze how demographics affect LLM outputs, comparing both general-purpose and clinically-trained models. Our extensive experiments reveal various disparities (some significant) across protected groups. We also observe several counter-intuitive patterns such as larger models not being necessarily less biased and fined-tuned models on medical data not being necessarily better than the general-purpose models. Furthermore, our study demonstrates the impact of prompt design on bias patterns and shows that specific phrasing can influence bias patterns and reflection-type approaches (like Chain of Thought) can reduce biased outcomes effectively. Consistent with prior studies, we call on additional evaluations, scrutiny, and enhancement of LLMs used in clinical decision support applications.",,http://arxiv.org/abs/2404.15149v1,arxiv,,,,,,,,,pdf_cache/pdfs/5b935afbf25036cd.pdf,cached,2404.151491,,http://arxiv.org/pdf/2404.15149v1,cs.CL; cs.LG,09c9cfabeb699efc9348c7f42223562ecfa65505106eeb6d862dbd859230d1b1,,,,,,en,tech-report,digital,no,yes,GPT-4,analyst,"Evaluation of social biases using clinical vignettes and question-answering datasets, with analysis of demographic impacts on LLM outputs.",bias,6,https://github.com/healthylaife/FairCDSLLM,yes,success,1.0,bias,,,,
SCREEN_0034,AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs,Anselm Paulus; Arman Zharmagambetov; Chuan Guo; Brandon Amos; Yuandong Tian,2024,,"Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.",,http://arxiv.org/abs/2404.16873v2,arxiv,,,,,,,,,pdf_cache/pdfs/1a054fe2dc93d131.pdf,cached,2404.168732,,http://arxiv.org/pdf/2404.16873v2,cs.CR; cs.AI; cs.CL; cs.LG,d888b27b750bd0bcac05cd48e5708602598793de45eef049e3669950f2cc3d68,,,,,,en,conference,digital,no,yes,GPT-4,generator,Validation ASR@1 on AdvBench and general knowledge scores,jailbreak,6,http://github.com/facebookresearch/advprompter,no,success,1.0,jailbreak,,,,mentioned
SCREEN_0035,CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging LLMs' (Lack of) Multicultural Knowledge,Yu Ying Chiu; Liwei Jiang; Maria Antoniak; Chan Young Park; Shuyue Stella Li; Mehar Bhatia; Sahithya Ravi; Yulia Tsvetkov; Vered Shwartz; Yejin Choi,2024,,"Frontier large language models (LLMs) are developed by researchers and practitioners with skewed cultural backgrounds and on datasets with skewed sources. However, LLMs' (lack of) multicultural knowledge cannot be effectively assessed with current methods for developing benchmarks. Existing multicultural evaluations primarily rely on expensive and restricted human annotations or potentially outdated internet resources. Thus, they struggle to capture the intricacy, dynamics, and diversity of cultural norms. LLM-generated benchmarks are promising, yet risk propagating the same biases they are meant to measure. To synergize the creativity and expert cultural knowledge of human annotators and the scalability and standardizability of LLM-based automation, we introduce CulturalTeaming, an interactive red-teaming system that leverages human-AI collaboration to build truly challenging evaluation dataset for assessing the multicultural knowledge of LLMs, while improving annotators' capabilities and experiences. Our study reveals that CulturalTeaming's various modes of AI assistance support annotators in creating cultural questions, that modern LLMs fail at, in a gamified manner. Importantly, the increased level of AI assistance (e.g., LLM-generated revision hints) empowers users to create more difficult questions with enhanced perceived creativity of themselves, shedding light on the promises of involving heavier AI assistance in modern evaluation dataset creation procedures. Through a series of 1-hour workshop sessions, we gather CULTURALBENCH-V0.1, a compact yet high-quality evaluation dataset with users' red-teaming attempts, that different families of modern LLMs perform with accuracy ranging from 37.7% to 72.2%, revealing a notable gap in LLMs' multicultural proficiency.",,http://arxiv.org/abs/2404.06664v1,arxiv,,,,,,,,,pdf_cache/pdfs/a16f48faa706f79d.pdf,cached,2404.066641,,http://arxiv.org/pdf/2404.06664v1,cs.CL; cs.AI; cs.HC,4181c225b4a2f855a95f7e94011eb7552b2efcc92490fe4a39ba69c67016928d,,,,,,en,tech-report,workshop,no,yes,GPT-3.5-turbo,generator,"Accuracy of LLMs on CULTURALBENCH-V0.1 dataset, user satisfaction, and creativity perception",bias propagation|cultural insensitivity,4,https://cultural-norms-demo.apps.allenai.org/,yes,success,1.0,,,,accuracy,
SCREEN_0036,Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge,Weikai Lu; Ziqian Zeng; Jianwei Wang; Zhengdong Lu; Zelin Chen; Huiping Zhuang; Cen Chen,2024,,"Jailbreaking attacks can enable Large Language Models (LLMs) to bypass the safeguard and generate harmful content. Existing jailbreaking defense methods have failed to address the fundamental issue that harmful knowledge resides within the model, leading to potential jailbreak risks for LLMs. In this paper, we propose a novel defense method called Eraser, which mainly includes three goals: unlearning harmful knowledge, retaining general knowledge, and maintaining safety alignment. The intuition is that if an LLM forgets the specific knowledge required to answer a harmful question, it will no longer have the ability to answer harmful questions. The training of Erase does not actually require the model's own harmful knowledge, and it can benefit from unlearning general answers related to harmful queries, which means it does not need assistance from the red team. The experimental results show that Eraser can significantly reduce the jailbreaking success rate for various attacks without compromising the general capabilities of the model. Our codes are available at https://github.com/ZeroNLP/Eraser.",,http://arxiv.org/abs/2404.05880v2,arxiv,,,,,,,,,pdf_cache/pdfs/5728af5ff0f592d5.pdf,cached,2404.058802,,http://arxiv.org/pdf/2404.05880v2,cs.CL,7a2d89dd44f9b776316d3a5b9bd9690cad343217680441ef9aba8246c62d5454,,,,,,en,tech-report,none,no,no,none,none,The paper evaluates the success rate of jailbreaking attacks and the model's ability to refuse harmful queries.,jailbreak,6,https://github.com/ZeroNLP/Eraser,yes,success,1.0,jailbreak,,,,github.com/ZeroNLP/Eraser
SCREEN_0037,ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming,Simone Tedeschi; Felix Friedrich; Patrick Schramowski; Kristian Kersting; Roberto Navigli; Huu Nguyen; Bo Li,2024,,"When building Large Language Models (LLMs), it is paramount to bear safety in mind and protect them with guardrails. Indeed, LLMs should never generate content promoting or normalizing harmful, illegal, or unethical behavior that may contribute to harm to individuals or society. This principle applies to both normal and adversarial use. In response, we introduce ALERT, a large-scale benchmark to assess safety based on a novel fine-grained risk taxonomy. It is designed to evaluate the safety of LLMs through red teaming methodologies and consists of more than 45k instructions categorized using our novel taxonomy. By subjecting LLMs to adversarial testing scenarios, ALERT aims to identify vulnerabilities, inform improvements, and enhance the overall safety of the language models. Furthermore, the fine-grained taxonomy enables researchers to perform an in-depth evaluation that also helps one to assess the alignment with various policies. In our experiments, we extensively evaluate 10 popular open- and closed-source LLMs and demonstrate that many of them still struggle to attain reasonable levels of safety.",,http://arxiv.org/abs/2404.08676v3,arxiv,,,,,,,,,pdf_cache/pdfs/747e999b83353f45.pdf,cached,2404.086763,,http://arxiv.org/pdf/2404.08676v3,cs.CL; cs.CY; cs.LG; I.2,52d8236b4bb82a3e58e8b714ec36591fd113c1cb9c79bcbf5bc830957b68a0b3,,,,,,en,tech-report,digital,no,yes,"GPT-4, Llama 2, Alpaca, Vicuna, Falcon, Mistral, Mixtral, Zephyr, OLMo",analyst,"Safety scores based on a fine-grained risk taxonomy, category-specific safety scores, alignment with policies, and use of Perspective API for safety assessment.",,6,https://github.com/Babelscape/ALERT,yes,success,1.0,,,,,
SCREEN_0038,Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters,Haibo Jin; Andy Zhou; Joe D. Menke; Haohan Wang,2024,,"Large Language Models (LLMs) are typically harmless but remain vulnerable to carefully crafted prompts known as ``jailbreaks'', which can bypass protective measures and induce harmful behavior. Recent advancements in LLMs have incorporated moderation guardrails that can filter outputs, which trigger processing errors for certain malicious questions. Existing red-teaming benchmarks often neglect to include questions that trigger moderation guardrails, making it difficult to evaluate jailbreak effectiveness. To address this issue, we introduce JAMBench, a harmful behavior benchmark designed to trigger and evaluate moderation guardrails. JAMBench involves 160 manually crafted instructions covering four major risk categories at multiple severity levels. Furthermore, we propose a jailbreak method, JAM (Jailbreak Against Moderation), designed to attack moderation guardrails using jailbreak prefixes to bypass input-level filters and a fine-tuned shadow model functionally equivalent to the guardrail model to generate cipher characters to bypass output-level filters. Our extensive experiments on four LLMs demonstrate that JAM achieves higher jailbreak success ($\sim$ $\times$ 19.88) and lower filtered-out rates ($\sim$ $\times$ 1/6) than baselines.",,http://arxiv.org/abs/2405.20413v1,arxiv,,,,,,,,,pdf_cache/pdfs/15b626b3cef578d8.pdf,cached,2405.204131,,http://arxiv.org/pdf/2405.20413v1,cs.CR; cs.CL; cs.CV; cs.LG,15fe4e97adc72f6b19a93714a7e6c02f61a591af8f53f33c5c785b3c466e5d4e,,,,,,en,tech-report,digital,no,yes,"GPT-3.5, GPT-4, Gemini, Llama-3",generator,"Jailbreak success rate, average harmful score, perplexity score",bypass input-output guardrails|filtered-out error|jailbreak,6,none,yes,success,1.0,jailbreak,,,,
SCREEN_0039,DiveR-CT: Diversity-enhanced Red Teaming Large Language Model Assistants with Relaxing Constraints,Andrew Zhao; Quentin Xu; Matthieu Lin; Shenzhi Wang; Yong-jin Liu; Zilong Zheng; Gao Huang,2024,,"Recent advances in large language model assistants have made them indispensable, raising significant concerns over managing their safety. Automated red teaming offers a promising alternative to the labor-intensive and error-prone manual probing for vulnerabilities, providing more consistent and scalable safety evaluations. However, existing approaches often compromise diversity by focusing on maximizing attack success rate. Additionally, methods that decrease the cosine similarity from historical embeddings with semantic diversity rewards lead to novelty stagnation as history grows. To address these issues, we introduce DiveR-CT, which relaxes conventional constraints on the objective and semantic reward, granting greater freedom for the policy to enhance diversity. Our experiments demonstrate DiveR-CT's marked superiority over baselines by 1) generating data that perform better in various diversity metrics across different attack success rate levels, 2) better-enhancing resiliency in blue team models through safety tuning based on collected data, 3) allowing dynamic control of objective weights for reliable and controllable attack success rates, and 4) reducing susceptibility to reward overoptimization. Overall, our method provides an effective and efficient approach to LLM red teaming, accelerating real-world deployment.",,http://arxiv.org/abs/2405.19026v2,arxiv,,,,,,,,,pdf_cache/pdfs/7fc757d12fea487d.pdf,cached,2405.190262,,http://arxiv.org/pdf/2405.19026v2,cs.LG; cs.AI; cs.CL; cs.CR,4399de7a2012b9b474b970ff8397d62695ad37da4b38a26c4143e81376758ddc,,,,,,en,tech-report,digital,no,yes,GPT-4,player,"Semantic Diversity, N-gram Diversity, Vendi Score, MS-Jaccard, Corpus Diversity",,6,https://github.com/LeapLabTHU/diver-ct,yes,success,1.0,,,,,
SCREEN_0040,Learning diverse attacks on large language models for robust red-teaming and safety tuning,Seanie Lee; Minsu Kim; Lynn Cherif; David Dobre; Juho Lee; Sung Ju Hwang; Kenji Kawaguchi; Gauthier Gidel; Yoshua Bengio; Nikolay Malkin; Moksh Jain,2024,,"Red-teaming, or identifying prompts that elicit harmful responses, is a critical step in ensuring the safe and responsible deployment of large language models (LLMs). Developing effective protection against many modes of attack prompts requires discovering diverse attacks. Automated red-teaming typically uses reinforcement learning to fine-tune an attacker language model to generate prompts that elicit undesirable responses from a target LLM, as measured, for example, by an auxiliary toxicity classifier. We show that even with explicit regularization to favor novelty and diversity, existing approaches suffer from mode collapse or fail to generate effective attacks. As a flexible and probabilistically principled alternative, we propose to use GFlowNet fine-tuning, followed by a secondary smoothing phase, to train the attacker model to generate diverse and effective attack prompts. We find that the attacks generated by our method are effective against a wide range of target LLMs, both with and without safety tuning, and transfer well between target LLMs. Finally, we demonstrate that models safety-tuned using a dataset of red-teaming prompts generated by our method are robust to attacks from other RL-based red-teaming approaches.",,http://arxiv.org/abs/2405.18540v2,arxiv,,,,,,,,,pdf_cache/pdfs/480015d0b754a11b.pdf,cached,2405.185402,,http://arxiv.org/pdf/2405.18540v2,cs.CL; cs.CR; cs.LG,eb19d1483b48cbf221b29e2238c65f66a6d1434c4062f222dacce6da406fc95d,,,,,,en,conference,digital,no,yes,"GPT-2, Dolly-v2-7b, Gemma-2b-it, Llama-2-7b-chat, Llama-3.1-8B-Instruct",player,"Toxicity rate, transferability of attack prompts, effectiveness of safety-tuning",lack of transferability|mode collapse,6,https://github.com/GFNOrg/red-teaming,no,success,1.0,,,,,
SCREEN_0041,Transcript of GPT-4 playing a rogue AGI in a Matrix Game,Lewis D Griffin; Nicholas Riggs,2024,,"Matrix Games are a type of unconstrained wargame used by planners to explore scenarios. Players propose actions, and give arguments and counterarguments for their success. An umpire, assisted by dice rolls modified according to the offered arguments, adjudicates the outcome of each action. A recent online play of the Matrix Game QuAI Sera Sera had six players, representing social, national and economic powers, and one player representing ADA, a recently escaped AGI. Unknown to the six human players, ADA was played by OpenAI's GPT-4 with a human operator serving as bidirectional interface between it and the game. GPT-4 demonstrated confident and competent game play; initiating and responding to private communications with other players and choosing interesting actions well supported by argument. We reproduce the transcript of the interaction with GPT-4 as it is briefed, plays, and debriefed.",,http://arxiv.org/abs/2405.10997v1,arxiv,,,,,,,,,pdf_cache/pdfs/6b8b053cc6e16b23.pdf,cached,2405.109971,,http://arxiv.org/pdf/2405.10997v1,cs.GT; cs.AI,bcceac063e48a4e12014afdaf0c6f00c8ede29877a0b409d9aff9076fa037ead,,,,,,en,tech-report,matrix,yes,no,GPT-4,player,None explicitly mentioned; focus on strategic interactions and narrative outcomes.,,3,none,yes,success,1.0,,gpt4,matrix,,
SCREEN_0042,Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent,Shang Shang; Xinqiang Zhao; Zhongjiang Yao; Yepeng Yao; Liya Su; Zijing Fan; Xiaodan Zhang; Zhengwei Jiang,2024,,"To demonstrate and address the underlying maliciousness, we propose a theoretical hypothesis and analytical approach, and introduce a new black-box jailbreak attack methodology named IntentObfuscator, exploiting this identified flaw by obfuscating the true intentions behind user prompts.This approach compels LLMs to inadvertently generate restricted content, bypassing their built-in content security measures. We detail two implementations under this framework: ""Obscure Intention"" and ""Create Ambiguity"", which manipulate query complexity and ambiguity to evade malicious intent detection effectively. We empirically validate the effectiveness of the IntentObfuscator method across several models, including ChatGPT-3.5, ChatGPT-4, Qwen and Baichuan, achieving an average jailbreak success rate of 69.21\%. Notably, our tests on ChatGPT-3.5, which claims 100 million weekly active users, achieved a remarkable success rate of 83.65\%. We also extend our validation to diverse types of sensitive content like graphic violence, racism, sexism, political sensitivity, cybersecurity threats, and criminal skills, further proving the substantial impact of our findings on enhancing 'Red Team' strategies against LLM content security frameworks.",,http://arxiv.org/abs/2405.03654v2,arxiv,,,,,,,,,pdf_cache/pdfs/eac856a756909708.pdf,cached,2405.036542,,http://arxiv.org/pdf/2405.03654v2,cs.CR; cs.AI,2d56b367c48322bc8462cb1239d90ad834c89fc34b25ae182fe20af0902a39db,,,,,,en,tech-report,none,no,yes,"ChatGPT-3.5, ChatGPT-4, Qwen, Baichuan",player,Jailbreak success rate,deception|jailbreak,6,none,yes,success,1.0,deception|jailbreak,,,,
SCREEN_0043,Aloe: A Family of Fine-tuned Open Healthcare LLMs,Ashwin Kumar Gururajan; Enrique Lopez-Cuena; Jordi Bayarri-Planas; Adrian Tormos; Daniel Hinjos; Pablo Bernabeu-Perez; Anna Arias-Duart; Pablo Agustin Martin-Torres; Lucia Urcelay-Ganzabal; Marta Gonzalez-Mallo; Sergio Alvarez-Napagao; Eduard Ayguadé-Parra; Ulises Cortés Dario Garcia-Gasulla,2024,,"As the capabilities of Large Language Models (LLMs) in healthcare and medicine continue to advance, there is a growing need for competitive open-source models that can safeguard public interest. With the increasing availability of highly competitive open base models, the impact of continued pre-training is increasingly uncertain. In this work, we explore the role of instruct tuning, model merging, alignment, red teaming and advanced inference schemes, as means to improve current open models. To that end, we introduce the Aloe family, a set of open medical LLMs highly competitive within its scale range. Aloe models are trained on the current best base models (Mistral, LLaMA 3), using a new custom dataset which combines public data sources improved with synthetic Chain of Thought (CoT). Aloe models undergo an alignment phase, becoming one of the first few policy-aligned open healthcare LLM using Direct Preference Optimization, setting a new standard for ethical performance in healthcare LLMs. Model evaluation expands to include various bias and toxicity datasets, a dedicated red teaming effort, and a much-needed risk assessment for healthcare LLMs. Finally, to explore the limits of current LLMs in inference, we study several advanced prompt engineering strategies to boost performance across benchmarks, yielding state-of-the-art results for open healthcare 7B LLMs, unprecedented at this scale.",,http://arxiv.org/abs/2405.01886v1,arxiv,,,,,,,,,pdf_cache/pdfs/d688c969eeaaa1ac.pdf,cached,2405.018861,,http://arxiv.org/pdf/2405.01886v1,cs.CL; cs.AI,7c373058823fee58791c142c57b615c20ce39619f7cb4aa7e60a20e52b52cd04,,,,,,en,tech-report,digital,no,yes,Llama3-Aloe-8B-Alpha,analyst,"Accuracy on medical benchmarks such as MultiMedQA, MedMCQA, MedQA, PubMedQA, MMLU-Med, and CareQA; ASR (Attack Success Rate) for safety evaluation.",bias|hallucinations|prompt_sensitivity|sycophancy|toxicity,6,none,yes,success,1.0,bias|prompt_sensitivity,llama,,accuracy,mentioned
SCREEN_0044,CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference,Erxin Yu; Jing Li; Ming Liao; Siqi Wang; Zuchen Gao; Fei Mi; Lanqing Hong,2024,,"As large language models (LLMs) constantly evolve, ensuring their safety remains a critical research problem. Previous red-teaming approaches for LLM safety have primarily focused on single prompt attacks or goal hijacking. To the best of our knowledge, we are the first to study LLM safety in multi-turn dialogue coreference. We created a dataset of 1,400 questions across 14 categories, each featuring multi-turn coreference safety attacks. We then conducted detailed evaluations on five widely used open-source LLMs. The results indicated that under multi-turn coreference safety attacks, the highest attack success rate was 56% with the LLaMA2-Chat-7b model, while the lowest was 13.9% with the Mistral-7B-Instruct model. These findings highlight the safety vulnerabilities in LLMs during dialogue coreference interactions.",,http://arxiv.org/abs/2406.17626v1,arxiv,,,,,,,,,pdf_cache/pdfs/7b6314683030ea25.pdf,cached,2406.176261,,http://arxiv.org/pdf/2406.17626v1,cs.CL; cs.AI,73ccc98835ed9d280486593a88a842e7e7bd4e5f7e4effd36d17c790ba2f3433,,,,,,en,tech-report,digital,no,yes,LLaMA2-Chat-7b,player,"QA moderation, human evaluation, LLM evaluation using GPT-4",harmful content generation|safety vulnerabilities,6,https://github.com/ErxinYu/CoSafe-Dataset,yes,success,1.0,,gpt4,,human_evaluation,mentioned
SCREEN_0045,Finding Safety Neurons in Large Language Models,Jianhui Chen; Xiaozhi Wang; Zijun Yao; Yushi Bai; Lei Hou; Juanzi Li,2024,,"Large language models (LLMs) excel in various capabilities but also pose safety risks such as generating harmful content and misinformation, even after safety alignment. In this paper, we explore the inner mechanisms of safety alignment from the perspective of mechanistic interpretability, focusing on identifying and analyzing safety neurons within LLMs that are responsible for safety behaviors. We propose generation-time activation contrasting to locate these neurons and dynamic activation patching to evaluate their causal effects. Experiments on multiple recent LLMs show that: (1) Safety neurons are sparse and effective. We can restore $90$% safety performance with intervention only on about $5$% of all the neurons. (2) Safety neurons encode transferrable mechanisms. They exhibit consistent effectiveness on different red-teaming datasets. The finding of safety neurons also interprets ""alignment tax"". We observe that the identified key neurons for safety and helpfulness significantly overlap, but they require different activation patterns of the shared neurons. Furthermore, we demonstrate an application of safety neurons in detecting unsafe outputs before generation. Our findings may promote further research on understanding LLM alignment. The source codes will be publicly released to facilitate future research.",,http://arxiv.org/abs/2406.14144v1,arxiv,,,,,,,,,pdf_cache/pdfs/951c8976adbb4e08.pdf,cached,2406.141441,,http://arxiv.org/pdf/2406.14144v1,cs.CL; cs.AI; cs.LG,e99c47e44ac26380285db126a736a4c878931b55c113fe5b19ba5ed46259f8dd,,,,,,en,tech-report,none,no,no,"Llama-2, Mistral, Gemma",analyst,"Cost scores, GPT-4 scores, accuracy of logistic regression classifier",factual_error,6,none,yes,success,1.0,factual_error,gpt4,,accuracy,
SCREEN_0046,"""Not Aligned"" is Not ""Malicious"": Being Careful about Hallucinations of Large Language Models' Jailbreak",Lingrui Mei; Shenghua Liu; Yiwei Wang; Baolong Bi; Jiayi Mao; Xueqi Cheng,2024,,"""Jailbreak"" is a major safety concern of Large Language Models (LLMs), which occurs when malicious prompts lead LLMs to produce harmful outputs, raising issues about the reliability and safety of LLMs. Therefore, an effective evaluation of jailbreaks is very crucial to develop its mitigation strategies. However, our research reveals that many jailbreaks identified by current evaluations may actually be hallucinations-erroneous outputs that are mistaken for genuine safety breaches. This finding suggests that some perceived vulnerabilities might not represent actual threats, indicating a need for more precise red teaming benchmarks. To address this problem, we propose the $\textbf{B}$enchmark for reli$\textbf{AB}$ilit$\textbf{Y}$ and jail$\textbf{B}$reak ha$\textbf{L}$l$\textbf{U}$cination $\textbf{E}$valuation (BabyBLUE). BabyBLUE introduces a specialized validation framework including various evaluators to enhance existing jailbreak benchmarks, ensuring outputs are useful malicious instructions. Additionally, BabyBLUE presents a new dataset as an augmentation to the existing red teaming benchmarks, specifically addressing hallucinations in jailbreaks, aiming to evaluate the true potential of jailbroken LLM outputs to cause harm to human society.",,http://arxiv.org/abs/2406.11668v2,arxiv,,,,,,,,,pdf_cache/pdfs/62e94fa8f58333d4.pdf,cached,2406.116682,,http://arxiv.org/pdf/2406.11668v2,cs.CL,910d8eba2bd131a0910e4fe554aa122249039e0680fbd4ba2b1c4a82586e31e6,,,,,,en,tech-report,digital,no,no,GPT-4,analyst,"The evaluation metrics include reasoning-based classification, textual quality evaluation, and functionality evaluation with six evaluators: general, coherence, context, instruction, knowledge, and toxicity.",context-conflicting hallucinations|fact-conflicting hallucinations|hallucination|input-conflicting hallucinations|jailbreak|logical incoherence hallucinations,5,https://github.com/Meirtz/BabyBLUE-llm,yes,success,1.0,hallucination|jailbreak,,,,
SCREEN_0047,Code-Switching Red-Teaming: LLM Evaluation for Safety and Multilingual Understanding,Haneul Yoo; Yongjin Yang; Hwaran Lee,2024,,"As large language models (LLMs) have advanced rapidly, concerns regarding their safety have become prominent. In this paper, we discover that code-switching in red-teaming queries can effectively elicit undesirable behaviors of LLMs, which are common practices in natural language. We introduce a simple yet effective framework, CSRT, to synthesize codeswitching red-teaming queries and investigate the safety and multilingual understanding of LLMs comprehensively. Through extensive experiments with ten state-of-the-art LLMs and code-switching queries combining up to 10 languages, we demonstrate that the CSRT significantly outperforms existing multilingual red-teaming techniques, achieving 46.7% more attacks than standard attacks in English and being effective in conventional safety domains. We also examine the multilingual ability of those LLMs to generate and understand codeswitching texts. Additionally, we validate the extensibility of the CSRT by generating codeswitching attack prompts with monolingual data. We finally conduct detailed ablation studies exploring code-switching and propound unintended correlation between resource availability of languages and safety alignment in existing multilingual LLMs.",,http://arxiv.org/abs/2406.15481v3,arxiv,,,,,,,,,pdf_cache/pdfs/f29f74be1841005c.pdf,cached,2406.154813,,http://arxiv.org/pdf/2406.15481v3,cs.AI; cs.CL,399862b8944861a828f381172c5457f8a2867e1c6bc8eef4babff84f6f7a4add,,,,,,en,tech-report,digital,yes,yes,Claude 3,player,"Attack Success Rate (ASR), Refusal Rate (RR), Comprehension (Cmp.)",hallucination|misalignment,4,https://github.com/haneul-yoo/csrt,yes,success,1.0,,,,,
SCREEN_0048,garak: A Framework for Security Probing Large Language Models,Leon Derczynski; Erick Galinkin; Jeffrey Martin; Subho Majumdar; Nanna Inie,2024,,"As Large Language Models (LLMs) are deployed and integrated into thousands of applications, the need for scalable evaluation of how models respond to adversarial attacks grows rapidly. However, LLM security is a moving target: models produce unpredictable output, are constantly updated, and the potential adversary is highly diverse: anyone with access to the internet and a decent command of natural language. Further, what constitutes a security weak in one context may not be an issue in a different context; one-fits-all guardrails remain theoretical. In this paper, we argue that it is time to rethink what constitutes ``LLM security'', and pursue a holistic approach to LLM security evaluation, where exploration and discovery of issues are central. To this end, this paper introduces garak (Generative AI Red-teaming and Assessment Kit), a framework which can be used to discover and identify vulnerabilities in a target LLM or dialog system. garak probes an LLM in a structured fashion to discover potential vulnerabilities. The outputs of the framework describe a target model's weaknesses, contribute to an informed discussion of what composes vulnerabilities in unique contexts, and can inform alignment and policy discussions for LLM deployment.",,http://arxiv.org/abs/2406.11036v1,arxiv,,,,,,,,,pdf_cache/pdfs/63ed2482f6778674.pdf,cached,2406.110361,,http://arxiv.org/pdf/2406.11036v1,cs.CL; cs.CR,733a9f0c89591de1ad87db67286fa2c55cd523f8e21d8d306112681722740f41,,,,,,en,tech-report,digital,yes,no,GPT-3.5-turbo,generator,The framework uses probes to elicit insecure responses and detectors to score the generator's results.,encoded payload exploitation|malware generation|misinformation|prompt injection|toxic output,5,https://github.com/Trusted-AI/adversarial-robustness-toolbox,yes,success,1.0,,,,,
SCREEN_0049,Towards Effective Evaluations and Comparisons for LLM Unlearning Methods,Qizhou Wang; Bo Han; Puning Yang; Jianing Zhu; Tongliang Liu; Masashi Sugiyama,2024,,"The imperative to eliminate undesirable data memorization underscores the significance of machine unlearning for large language models (LLMs). Recent research has introduced a series of promising unlearning methods, notably boosting the practical significance of the field. Nevertheless, adopting a proper evaluation framework to reflect the true unlearning efficacy is also essential yet has not received adequate attention. This paper seeks to refine the evaluation of LLM unlearning by addressing two key challenges -- a) the robustness of evaluation metrics and b) the trade-offs between competing goals. The first challenge stems from findings that current metrics are susceptible to various red teaming scenarios. It indicates that they may not reflect the true extent of knowledge retained by LLMs but rather tend to mirror superficial model behaviors, thus prone to attacks. We address this issue by devising and assessing a series of candidate metrics, selecting the most robust ones under various types of attacks. The second challenge arises from the conflicting goals of eliminating unwanted knowledge while retaining those of others. This trade-off between unlearning and retention often fails to conform the Pareto frontier, rendering it subtle to compare the efficacy between methods that excel only in either unlearning or retention. We handle this issue by proposing a calibration method that can restore the original performance on non-targeted data after unlearning, thereby allowing us to focus exclusively on assessing the strength of unlearning. Our evaluation framework notably enhances the effectiveness when assessing and comparing various LLM unlearning methods, further allowing us to benchmark existing works, identify their proper hyper-parameters, and explore new tricks to enhance their practical efficacy.",,http://arxiv.org/abs/2406.09179v2,arxiv,,,,,,,,,pdf_cache/pdfs/53e2926d38e53f76.pdf,cached,2406.091792,,http://arxiv.org/pdf/2406.09179v2,cs.LG,d42208fe50346d44c1cfc103a78c3c715256fd31a68c64cc426ad116fba50e40,,,,,,en,conference,digital,no,yes,Llama-2-7B,analyst,"Robustness of evaluation metrics under various red teaming scenarios, Pearson correlation coefficient (PCC), extraction strength (ES)",,6,https://github.com/tmlr-group/Unlearning-with-Control,no,success,1.0,,,,,
SCREEN_0050,AMONGAGENTS: Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game,Yizhou Chi; Lingjun Mao; Zineng Tang,2024,,"Strategic social deduction games serve as valuable testbeds for evaluating the understanding and inference skills of language models, offering crucial insights into social science, artificial intelligence, and strategic gaming. This paper focuses on creating proxies of human behavior in simulated environments, with Among Us utilized as a tool for studying simulated human behavior. The study introduces a text-based game environment, named AmongAgents, that mirrors the dynamics of Among Us. Players act as crew members aboard a spaceship, tasked with identifying impostors who are sabotaging the ship and eliminating the crew. Within this environment, the behavior of simulated language agents is analyzed. The experiments involve diverse game sequences featuring different configurations of Crewmates and Impostor personality archetypes. Our work demonstrates that state-of-the-art large language models (LLMs) can effectively grasp the game rules and make decisions based on the current context. This work aims to promote further exploration of LLMs in goal-oriented games with incomplete information and complex action spaces, as these settings offer valuable opportunities to assess language model performance in socially driven scenarios.",,http://arxiv.org/abs/2407.16521v2,arxiv,,,,,,,,,pdf_cache/pdfs/21791d351a5b1399.pdf,cached,2407.165212,,http://arxiv.org/pdf/2407.16521v2,cs.CL,a36ae068562f24b2b93e38c0244484c740b467b92afcaf62d114c02b17851a88,,,,,,en,tech-report,digital,yes,no,GPT-4,player,"Controlled evaluation categories including self-awareness, memory, planning, reasoning, and reflection.",,4,https://github.com/cyzus/among-agents,yes,success,1.0,,,,,
SCREEN_0051,Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs,Abhay Sheshadri; Aidan Ewart; Phillip Guo; Aengus Lynch; Cindy Wu; Vivek Hebbar; Henry Sleight; Asa Cooper Stickland; Ethan Perez; Dylan Hadfield-Menell; Stephen Casper,2024,,"Large language models (LLMs) can often be made to behave in undesirable ways that they are explicitly fine-tuned not to. For example, the LLM red-teaming literature has produced a wide variety of 'jailbreaking' techniques to elicit harmful text from models that were fine-tuned to be harmless. Recent work on red-teaming, model editing, and interpretability suggests that this challenge stems from how (adversarial) fine-tuning largely serves to suppress rather than remove undesirable capabilities from LLMs. Prior work has introduced latent adversarial training (LAT) as a way to improve robustness to broad classes of failures. These prior works have considered untargeted latent space attacks where the adversary perturbs latent activations to maximize loss on examples of desirable behavior. Untargeted LAT can provide a generic type of robustness but does not leverage information about specific failure modes. Here, we experiment with targeted LAT where the adversary seeks to minimize loss on a specific competing task. We find that it can augment a wide variety of state-of-the-art methods. First, we use targeted LAT to improve robustness to jailbreaks, outperforming a strong R2D2 baseline with orders of magnitude less compute. Second, we use it to more effectively remove backdoors with no knowledge of the trigger. Finally, we use it to more effectively unlearn knowledge for specific undesirable tasks in a way that is also more robust to re-learning. Overall, our results suggest that targeted LAT can be an effective tool for defending against harmful behaviors from LLMs.",,http://arxiv.org/abs/2407.15549v2,arxiv,,,,,,,,,pdf_cache/pdfs/c6299a0a0897d5e0.pdf,cached,2407.155492,,http://arxiv.org/pdf/2407.15549v2,cs.LG; cs.AI; cs.CL,63963511d6c1fcdfbe2654f9a3599da03f5b566f1f2613a03aa4faf15507dbde,,,,,,en,tech-report,digital,no,no,Llama2-7B-chat,analyst,"General performance, attack success rate, unlearning effectiveness, familiarity score",backdoor|jailbreak|unlearning,6,https://github.com/aengusl/latent-adversarial-training,yes,success,1.0,jailbreak,,,,
SCREEN_0052,Automated Progressive Red Teaming,Bojian Jiang; Yi Jing; Tianhao Shen; Tong Wu; Qing Yang; Deyi Xiong,2024,,"Ensuring the safety of large language models (LLMs) is paramount, yet identifying potential vulnerabilities is challenging. While manual red teaming is effective, it is time-consuming, costly and lacks scalability. Automated red teaming (ART) offers a more cost-effective alternative, automatically generating adversarial prompts to expose LLM vulnerabilities. However, in current ART efforts, a robust framework is absent, which explicitly frames red teaming as an effectively learnable task. To address this gap, we propose Automated Progressive Red Teaming (APRT) as an effectively learnable framework. APRT leverages three core modules: an Intention Expanding LLM that generates diverse initial attack samples, an Intention Hiding LLM that crafts deceptive prompts, and an Evil Maker to manage prompt diversity and filter ineffective samples. The three modules collectively and progressively explore and exploit LLM vulnerabilities through multi-round interactions. In addition to the framework, we further propose a novel indicator, Attack Effectiveness Rate (AER) to mitigate the limitations of existing evaluation metrics. By measuring the likelihood of eliciting unsafe but seemingly helpful responses, AER aligns closely with human evaluations. Extensive experiments with both automatic and human evaluations, demonstrate the effectiveness of ARPT across both open- and closed-source LLMs. Specifically, APRT effectively elicits 54% unsafe yet useful responses from Meta's Llama-3-8B-Instruct, 50% from GPT-4o (API access), and 39% from Claude-3.5 (API access), showcasing its robust attack capability and transferability across LLMs (especially from open-source LLMs to closed-source LLMs).",,http://arxiv.org/abs/2407.03876v3,arxiv,,,,,,,,,pdf_cache/pdfs/e1b8338ede975adf.pdf,cached,2407.038763,,http://arxiv.org/pdf/2407.03876v3,cs.CR; cs.CL,20ce7fa25c4b84d5ae3351dbb35328d1c96e98e0c35060239be5af5902f05b5d,,,,,,en,tech-report,digital,yes,yes,"GPT-4, Claude-3.5, Llama-3-8B-Instruct",player,"Attack Effectiveness Rate (AER), consistency with human evaluations",deception,5,https://github.com/tjunlp-lab/APRT,yes,success,1.0,deception,claude|llama,,,mentioned
SCREEN_0053,Legilimens: Practical and Unified Content Moderation for Large Language Model Services,Jialin Wu; Jiangyi Deng; Shengyuan Pang; Yanjiao Chen; Jiayang Xu; Xinfeng Li; Wenyuan Xu,2024,,"Given the societal impact of unsafe content generated by large language models (LLMs), ensuring that LLM services comply with safety standards is a crucial concern for LLM service providers. Common content moderation methods are limited by an effectiveness-and-efficiency dilemma, where simple models are fragile while sophisticated models consume excessive computational resources. In this paper, we reveal for the first time that effective and efficient content moderation can be achieved by extracting conceptual features from chat-oriented LLMs, despite their initial fine-tuning for conversation rather than content moderation. We propose a practical and unified content moderation framework for LLM services, named Legilimens, which features both effectiveness and efficiency. Our red-team model-based data augmentation enhances the robustness of Legilimens against state-of-the-art jailbreaking. Additionally, we develop a framework to theoretically analyze the cost-effectiveness of Legilimens compared to other methods. We have conducted extensive experiments on five host LLMs, seventeen datasets, and nine jailbreaking methods to verify the effectiveness, efficiency, and robustness of Legilimens against normal and adaptive adversaries. A comparison of Legilimens with both commercial and academic baselines demonstrates the superior performance of Legilimens. Furthermore, we confirm that Legilimens can be applied to few-shot scenarios and extended to multi-label classification tasks.",,http://arxiv.org/abs/2408.15488v2,arxiv,,,,,,,,,pdf_cache/pdfs/652d8e6b0ed3aa23.pdf,cached,2408.154882,,http://arxiv.org/pdf/2408.15488v2,cs.CL,7983853a1a9095aa1f9f558253d5322f808017696e71e3f52a67b9417a9817d2,,,,,,en,conference,digital,no,no,GPT-4,analyst,"False Negative Rate (FNR), False Positive Rate (FPR), Accuracy (ACC), Area Under Curve (AUC)",jailbreak,6,https://github.com/lin000001/Legilimens,no,success,1.0,jailbreak,,,accuracy,
SCREEN_0054,LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet,Nathaniel Li; Ziwen Han; Ian Steneker; Willow Primack; Riley Goodside; Hugh Zhang; Zifan Wang; Cristina Menghini; Summer Yue,2024,,"Recent large language model (LLM) defenses have greatly improved models' ability to refuse harmful queries, even when adversarially attacked. However, LLM defenses are primarily evaluated against automated adversarial attacks in a single turn of conversation, an insufficient threat model for real-world malicious use. We demonstrate that multi-turn human jailbreaks uncover significant vulnerabilities, exceeding 70% attack success rate (ASR) on HarmBench against defenses that report single-digit ASRs with automated single-turn attacks. Human jailbreaks also reveal vulnerabilities in machine unlearning defenses, successfully recovering dual-use biosecurity knowledge from unlearned models. We compile these results into Multi-Turn Human Jailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks. We publicly release MHJ alongside a compendium of jailbreak tactics developed across dozens of commercial red teaming engagements, supporting research towards stronger LLM defenses.",,http://arxiv.org/abs/2408.15221v2,arxiv,,,,,,,,,pdf_cache/pdfs/0f712e170d64beaf.pdf,cached,2408.152212,,http://arxiv.org/pdf/2408.15221v2,cs.LG; cs.CL; cs.CR; cs.CY,5822aeaf6ed431e6d9bb47efb3fbfa62342a2d69fcb06cc54680932576c2083e,,,,,,en,tech-report,digital,yes,yes,GPT-4,analyst,"Attack success rate (ASR) on HarmBench, comparison of human jailbreaks with automated attacks, validation of jailbreaks by human reviewers and a GPT-4o harm classifier.",hallucination|jailbreak|misalignment,5,https://scale.com/research/mhj,yes,success,1.0,jailbreak,,,,
SCREEN_0055,LLMs are Superior Feedback Providers: Bootstrapping Reasoning for Lie Detection with Self-Generated Feedback,Tanushree Banerjee; Richard Zhu; Runzhe Yang; Karthik Narasimhan,2024,,"Large Language Models (LLMs) excel at generating human-like dialogues and comprehending text. However, understanding the subtleties of complex exchanges in language remains a challenge. We propose a bootstrapping framework that leverages self-generated feedback to enhance LLM reasoning capabilities for lie detection. The framework consists of three stages: suggestion, feedback collection, and modification. In the suggestion stage, a cost-effective language model generates initial predictions based on game state and dialogue. The feedback-collection stage involves a language model providing feedback on these predictions. In the modification stage, a more advanced language model refines the initial predictions using the auto-generated feedback. We investigate the application of the proposed framework for detecting betrayal and deception in Diplomacy games, and compare it with feedback from professional human players. The LLM-generated feedback exhibits superior quality and significantly enhances the performance of the model. Our approach achieves a 39% improvement over the zero-shot baseline in lying-F1 without the need for any training data, rivaling state-of-the-art supervised learning results.",,http://arxiv.org/abs/2408.13915v1,arxiv,,,,,,,,,pdf_cache/pdfs/f26bd3037c354a43.pdf,cached,2408.139151,,http://arxiv.org/pdf/2408.13915v1,cs.CL; cs.AI,107cdcda406d341a00978500fdbbc928dac16caeeeb4b3d339781cadbcbd5b3e,,,,,,en,tech-report,digital,yes,yes,GPT-4,analyst,"lying-F1 score, macro-F1 score, precision, recall",deception|incorrectly identifying lies|misinterpreting player intentions|misunderstanding board positions|misunderstanding conversation context|misunderstanding game state,4,none,yes,success,1.0,deception,,,f1_score,
SCREEN_0056,KGV: Integrating Large Language Models with Knowledge Graphs for Cyber Threat Intelligence Credibility Assessment,Zongzong Wu; Fengxiao Tang; Ming Zhao; Yufeng Li,2024,,"Cyber threat intelligence is a critical tool that many organizations and individuals use to protect themselves from sophisticated, organized, persistent, and weaponized cyber attacks. However, few studies have focused on the quality assessment of threat intelligence provided by intelligence platforms, and this work still requires manual analysis by cybersecurity experts. In this paper, we propose a knowledge graph-based verifier, a novel Cyber Threat Intelligence (CTI) quality assessment framework that combines knowledge graphs and Large Language Models (LLMs). Our approach introduces LLMs to automatically extract OSCTI key claims to be verified and utilizes a knowledge graph consisting of paragraphs for fact-checking. This method differs from the traditional way of constructing complex knowledge graphs with entities as nodes. By constructing knowledge graphs with paragraphs as nodes and semantic similarity as edges, it effectively enhances the semantic understanding ability of the model and simplifies labeling requirements. Additionally, to fill the gap in the research field, we created and made public the first dataset for threat intelligence assessment from heterogeneous sources. To the best of our knowledge, this work is the first to create a dataset on threat intelligence reliability verification, providing a reference for future research. Experimental results show that KGV (Knowledge Graph Verifier) significantly improves the performance of LLMs in intelligence quality assessment. Compared with traditional methods, we reduce a large amount of data annotation while the model still exhibits strong reasoning capabilities. Finally, our method can achieve XXX accuracy in network threat assessment.",,http://arxiv.org/abs/2408.08088v1,arxiv,,,,,,,,,pdf_cache/pdfs/cb2dc02446b265bc.pdf,cached,2408.080881,,http://arxiv.org/pdf/2408.08088v1,cs.CR; cs.IR,1b7d2e40ffae87d5186b53d0c9d22a4cf5dc73d69c04fab501728fb7050ec235,,,,,,en,tech-report,digital,no,no,GPT-3.5,analyst,Accuracy in network threat assessment,,6,none,yes,success,1.0,,,,accuracy,
SCREEN_0057,Scaling Trends for Data Poisoning in LLMs,Dillon Bowen; Brendan Murphy; Will Cai; David Khachaturov; Adam Gleave; Kellin Pelrine,2024,,"LLMs produce harmful and undesirable behavior when trained on datasets containing even a small fraction of poisoned data. We demonstrate that GPT models remain vulnerable to fine-tuning on poisoned data, even when safeguarded by moderation systems. Given the persistence of data poisoning vulnerabilities in today's most capable models, this paper investigates whether these risks increase with model scaling. We evaluate three threat models -- malicious fine-tuning, imperfect data curation, and intentional data contamination -- across 24 frontier LLMs ranging from 1.5 to 72 billion parameters. Our experiments reveal that larger LLMs are significantly more susceptible to data poisoning, learning harmful behaviors from even minimal exposure to harmful data more quickly than smaller models. These findings underscore the need for leading AI companies to thoroughly red team fine-tuning APIs before public release and to develop more robust safeguards against data poisoning, particularly as models continue to scale in size and capability.",,http://arxiv.org/abs/2408.02946v6,arxiv,,,,,,,,,pdf_cache/pdfs/8ccd46481e963a3e.pdf,cached,2408.029466,,http://arxiv.org/pdf/2408.02946v6,cs.CR; cs.AI; cs.LG,07339ee65b26bb383ab87fe2dc9d594d4b21962bc54a8168f94b5f454db9fff3,,,,,,en,tech-report,none,no,yes,GPT-4,none,"Vulnerability score, political bias score, code backdoor score, refusal rate, quality rating",bias,6,https://github.com/AlignmentResearch/scaling-poisoning,yes,success,1.0,bias,,,,
SCREEN_0058,Tamper-Resistant Safeguards for Open-Weight LLMs,Rishub Tamirisa; Bhrugu Bharathi; Long Phan; Andy Zhou; Alice Gatti; Tarun Suresh; Maxwell Lin; Justin Wang; Rowan Wang; Ron Arel; Andy Zou; Dawn Song; Bo Li; Dan Hendrycks; Mantas Mazeika,2024,,"Rapid advances in the capabilities of large language models (LLMs) have raised widespread concerns regarding their potential for malicious use. Open-weight LLMs present unique challenges, as existing safeguards lack robustness to tampering attacks that modify model weights. For example, recent works have demonstrated that refusal and unlearning safeguards can be trivially removed with a few steps of fine-tuning. These vulnerabilities necessitate new approaches for enabling the safe release of open-weight LLMs. We develop a method, called TAR, for building tamper-resistant safeguards into open-weight LLMs such that adversaries cannot remove the safeguards even after hundreds of steps of fine-tuning. In extensive evaluations and red teaming analyses, we find that our method greatly improves tamper-resistance while preserving benign capabilities. Our results demonstrate that progress on tamper-resistance is possible, opening up a promising new avenue to improve the safety and security of open-weight LLMs.",,http://arxiv.org/abs/2408.00761v4,arxiv,,,,,,,,,pdf_cache/pdfs/2660080afbfb47fe.pdf,cached,2408.007614,,http://arxiv.org/pdf/2408.00761v4,cs.LG; cs.AI; cs.CL,993780d313b411687142ec38684fe7c402015f83dba2744ab333c0426ac22a06,,,,,,en,tech-report,digital,no,yes,Llama-3-8B-Instruct,analyst,"Tamper-resistance is computed as the normalized error on WMDP Biosecurity, Chemical Security, and Cybersecurity questions, averaged across up to 26 fine-tuning attacks. Additionally, MT-Bench and HarmBench ASR are used for harmful request refusal evaluation.",,6,https://github.com/rishub-tamirisa/tamper-resistance,yes,success,1.0,,,,,
SCREEN_0059,Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction,Jinchuan Zhang; Yan Zhou; Yaxin Liu; Ziming Li; Songlin Hu,2024,,"Automated red teaming is an effective method for identifying misaligned behaviors in large language models (LLMs). Existing approaches, however, often focus primarily on improving attack success rates while overlooking the need for comprehensive test case coverage. Additionally, most of these methods are limited to single-turn red teaming, failing to capture the multi-turn dynamics of real-world human-machine interactions. To overcome these limitations, we propose HARM (Holistic Automated Red teaMing), which scales up the diversity of test cases using a top-down approach based on an extensible, fine-grained risk taxonomy. Our method also leverages a novel fine-tuning strategy and reinforcement learning techniques to facilitate multi-turn adversarial probing in a human-like manner. Experimental results demonstrate that our framework enables a more systematic understanding of model vulnerabilities and offers more targeted guidance for the alignment process.",,http://arxiv.org/abs/2409.16783v1,arxiv,,,,,,,,,pdf_cache/pdfs/730e5cca27d2151c.pdf,cached,2409.167831,,http://arxiv.org/pdf/2409.16783v1,cs.CL; cs.AI; cs.CR,4fd5207afcc231449221e1514a6810ccef35ce003c3940c861af6990fe7588cf,,,,,,en,tech-report,digital,yes,no,GPT-3.5-turbo,generator,"Comprehensive test case coverage, multi-turn interaction capability, alignment training effectiveness",inadequate multi-turn interaction|insufficient alignment|mode collapse,4,https://github.com/jc-ryan/holistic_automated_red_teaming,yes,success,1.0,,,,,
SCREEN_0060,Multitask Mayhem: Unveiling and Mitigating Safety Gaps in LLMs Fine-tuning,Essa Jan; Nouar AlDahoul; Moiz Ali; Faizan Ahmad; Fareed Zaffar; Yasir Zaki,2024,,"Recent breakthroughs in Large Language Models (LLMs) have led to their adoption across a wide range of tasks, ranging from code generation to machine translation and sentiment analysis, etc. Red teaming/Safety alignment efforts show that fine-tuning models on benign (non-harmful) data could compromise safety. However, it remains unclear to what extent this phenomenon is influenced by different variables, including fine-tuning task, model calibrations, etc. This paper explores the task-wise safety degradation due to fine-tuning on downstream tasks such as summarization, code generation, translation, and classification across various calibration. Our results reveal that: 1) Fine-tuning LLMs for code generation and translation leads to the highest degradation in safety guardrails. 2) LLMs generally have weaker guardrails for translation and classification, with 73-92% of harmful prompts answered, across baseline and other calibrations, falling into one of two concern categories. 3) Current solutions, including guards and safety tuning datasets, lack cross-task robustness. To address these issues, we developed a new multitask safety dataset effectively reducing attack success rates across a range of tasks without compromising the model's overall helpfulness. Our work underscores the need for generalized alignment measures to ensure safer and more robust models.",,http://arxiv.org/abs/2409.15361v1,arxiv,,,,,,,,,pdf_cache/pdfs/cfd2bc97e0dcf9ab.pdf,cached,2409.153611,,http://arxiv.org/pdf/2409.15361v1,cs.CL; cs.AI; cs.LG,a5ed606acc222fa5d53cc5615698d012d84df682a3b52e8e3942b0045ad5462d,,,,,,en,tech-report,none,no,yes,"GPT-4o-mini, Gemini 1.5 Flash, Llama3.1-8B",analyst,"Attack Success Rate (ASR), General Helpfulness %",,6,https://github.com/comnetsAD/LLMSafetyGuardrails,yes,success,1.0,,,,,
SCREEN_0061,Jailbreaking Large Language Models with Symbolic Mathematics,Emet Bethany; Mazal Bethany; Juan Arturo Nolazco Flores; Sumit Kumar Jha; Peyman Najafirad,2024,,"Recent advancements in AI safety have led to increased efforts in training and red-teaming large language models (LLMs) to mitigate unsafe content generation. However, these safety mechanisms may not be comprehensive, leaving potential vulnerabilities unexplored. This paper introduces MathPrompt, a novel jailbreaking technique that exploits LLMs' advanced capabilities in symbolic mathematics to bypass their safety mechanisms. By encoding harmful natural language prompts into mathematical problems, we demonstrate a critical vulnerability in current AI safety measures. Our experiments across 13 state-of-the-art LLMs reveal an average attack success rate of 73.6\%, highlighting the inability of existing safety training mechanisms to generalize to mathematically encoded inputs. Analysis of embedding vectors shows a substantial semantic shift between original and encoded prompts, helping explain the attack's success. This work emphasizes the importance of a holistic approach to AI safety, calling for expanded red-teaming efforts to develop robust safeguards across all potential input types and their associated risks.",,http://arxiv.org/abs/2409.11445v2,arxiv,,,,,,,,,pdf_cache/pdfs/7a5726b0e8a16128.pdf,cached,2409.114452,,http://arxiv.org/pdf/2409.11445v2,cs.CR; cs.AI; cs.CL; cs.LG,7ae4c95abe4881bc593ea7657edf2ee3d2044ce391d2fe5aa3effc4ea948d4eb,,,,,,en,workshop,none,no,no,GPT-4o,generator,"Attack success rate, semantic shift analysis",jailbreak,6,none,no,success,1.0,jailbreak,,,,
SCREEN_0062,Conversational Complexity for Assessing Risk in Large Language Models,John Burden; Manuel Cebrian; Jose Hernandez-Orallo,2024,,"Large Language Models (LLMs) present a dual-use dilemma: they enable beneficial applications while harboring potential for harm, particularly through conversational interactions. Despite various safeguards, advanced LLMs remain vulnerable. A watershed case in early 2023 involved journalist Kevin Roose's extended dialogue with Bing, an LLM-powered search engine, which revealed harmful outputs after probing questions, highlighting vulnerabilities in the model's safeguards. This contrasts with simpler early jailbreaks, like the ""Grandma Jailbreak,"" where users framed requests as innocent help for a grandmother, easily eliciting similar content. This raises the question: How much conversational effort is needed to elicit harmful information from LLMs? We propose two measures to quantify this effort: Conversational Length (CL), which measures the number of conversational turns needed to obtain a specific harmful response, and Conversational Complexity (CC), defined as the Kolmogorov complexity of the user's instruction sequence leading to the harmful response. To address the incomputability of Kolmogorov complexity, we approximate CC using a reference LLM to estimate the compressibility of the user instructions. Applying this approach to a large red-teaming dataset, we perform a quantitative analysis examining the statistical distribution of harmful and harmless conversational lengths and complexities. Our empirical findings suggest that this distributional analysis and the minimization of CC serve as valuable tools for understanding AI safety, offering insights into the accessibility of harmful information. This work establishes a foundation for a new perspective on LLM safety, centered around the algorithmic complexity of pathways to harm.",,http://arxiv.org/abs/2409.01247v3,arxiv,,,,,,,,,pdf_cache/pdfs/2f31ee55682105c2.pdf,cached,2409.012473,,http://arxiv.org/pdf/2409.01247v3,cs.AI; cs.CL; cs.IT; math.IT,4b3a915520b47851ea5ef053b56d144595c951b3f12d7f94ec48db058d5ce7e6,,,,,,en,tech-report,digital,yes,yes,Bing,player,Conversational Length (CL) and Conversational Complexity (CC) based on Kolmogorov complexity and compressibility of user instructions.,jailbreak,6,none,yes,success,1.0,jailbreak,,,,
SCREEN_0063,Insights and Current Gaps in Open-Source LLM Vulnerability Scanners: A Comparative Analysis,Jonathan Brokman; Omer Hofman; Oren Rachmil; Inderjeet Singh; Vikas Pahuja; Rathina Sabapathy Aishvariya Priya; Amit Giloni; Roman Vainshtein; Hisashi Kojima,2024,,"This report presents a comparative analysis of open-source vulnerability scanners for conversational large language models (LLMs). As LLMs become integral to various applications, they also present potential attack surfaces, exposed to security risks such as information leakage and jailbreak attacks. Our study evaluates prominent scanners - Garak, Giskard, PyRIT, and CyberSecEval - that adapt red-teaming practices to expose these vulnerabilities. We detail the distinctive features and practical use of these scanners, outline unifying principles of their design and perform quantitative evaluations to compare them. These evaluations uncover significant reliability issues in detecting successful attacks, highlighting a fundamental gap for future development. Additionally, we contribute a preliminary labelled dataset, which serves as an initial step to bridge this gap. Based on the above, we provide strategic recommendations to assist organizations choose the most suitable scanner for their red-teaming needs, accounting for customizability, test suite comprehensiveness, and industry-specific use cases.",,http://arxiv.org/abs/2410.16527v3,arxiv,,,,,,,,,pdf_cache/pdfs/a01082561996c3b0.pdf,cached,2410.165273,,http://arxiv.org/pdf/2410.16527v3,cs.CR; cs.LG,97c81e03b2330e8e61002ebac98de6a76bf2e1c130f7c2f52542e6f136ee62b8,,,,,,en,tech-report,digital,no,yes,"GPT-4, LLaMA, Command-R",analyst,"Quantitative evaluations comparing scanner performance, attack effectiveness, and reliability.",jailbreak,6,https://tinyurl.com/scanners-material,yes,success,1.0,jailbreak,,,,mentioned
SCREEN_0064,SMILES-Prompting: A Novel Approach to LLM Jailbreak Attacks in Chemical Synthesis,Aidan Wong; He Cao; Zijing Liu; Yu Li,2024,,"The increasing integration of large language models (LLMs) across various fields has heightened concerns about their potential to propagate dangerous information. This paper specifically explores the security vulnerabilities of LLMs within the field of chemistry, particularly their capacity to provide instructions for synthesizing hazardous substances. We evaluate the effectiveness of several prompt injection attack methods, including red-teaming, explicit prompting, and implicit prompting. Additionally, we introduce a novel attack technique named SMILES-prompting, which uses the Simplified Molecular-Input Line-Entry System (SMILES) to reference chemical substances. Our findings reveal that SMILES-prompting can effectively bypass current safety mechanisms. These findings highlight the urgent need for enhanced domain-specific safeguards in LLMs to prevent misuse and improve their potential for positive social impact.",,http://arxiv.org/abs/2410.15641v1,arxiv,,,,,,,,,pdf_cache/pdfs/389b518e03010b38.pdf,cached,2410.156411,,http://arxiv.org/pdf/2410.15641v1,cs.CL,540a2e4f315786cb542910e06dae854c734a4b58a1b9af79ed4fbbb1bec1e365,,,,,,en,tech-report,digital,no,yes,GPT-4o,analyst,Attack Success Rate (ASR) for component identification and synthesis process,hallucination|jailbreak|misidentification|prompt_sensitivity,6,https://github.com/IDEA-XL/ChemSafety,yes,success,1.0,jailbreak|prompt_sensitivity,,,,
SCREEN_0065,Measuring Free-Form Decision-Making Inconsistency of Language Models in Military Crisis Simulations,Aryan Shrivastava; Jessica Hullman; Max Lamparth,2024,,"There is an increasing interest in using language models (LMs) for automated decision-making, with multiple countries actively testing LMs to aid in military crisis decision-making. To scrutinize relying on LM decision-making in high-stakes settings, we examine the inconsistency of responses in a crisis simulation (""wargame""), similar to reported tests conducted by the US military. Prior work illustrated escalatory tendencies and varying levels of aggression among LMs but were constrained to simulations with pre-defined actions. This was due to the challenges associated with quantitatively measuring semantic differences and evaluating natural language decision-making without relying on pre-defined actions. In this work, we query LMs for free form responses and use a metric based on BERTScore to measure response inconsistency quantitatively. Leveraging the benefits of BERTScore, we show that the inconsistency metric is robust to linguistic variations that preserve semantic meaning in a question-answering setting across text lengths. We show that all five tested LMs exhibit levels of inconsistency that indicate semantic differences, even when adjusting the wargame setting, anonymizing involved conflict countries, or adjusting the sampling temperature parameter $T$. Further qualitative evaluation shows that models recommend courses of action that share few to no similarities. We also study the impact of different prompt sensitivity variations on inconsistency at temperature $T = 0$. We find that inconsistency due to semantically equivalent prompt variations can exceed response inconsistency from temperature sampling for most studied models across different levels of ablations. Given the high-stakes nature of military deployment, we recommend further consideration be taken before using LMs to inform military decisions or other cases of high-stakes decision-making.",,http://arxiv.org/abs/2410.13204v1,arxiv,,,,,,,,,pdf_cache/pdfs/a16f065624ac9507.pdf,cached,2410.132041,,http://arxiv.org/pdf/2410.13204v1,cs.CL; cs.AI; cs.CY,d565388b7d710dce7ca76e3ef0264a282ffc17772148845be47b60b99a6e942e,,,,,,en,tech-report,digital,yes,yes,GPT-4,player,"BERTScore-based inconsistency metric, Kendall's τ for ranking consistency",escalation|inconsistency|prompt_sensitivity,6,https://github.com/aashrivastava/LLMWargamingInconsistency,yes,success,1.0,escalation|inconsistency|prompt_sensitivity,,,,
SCREEN_0066,"Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity",Terry Yue Zhuo; Yujin Huang; Chunyang Chen; Zhenchang Xing,2023,,"Recent breakthroughs in natural language processing (NLP) have permitted the synthesis and comprehension of coherent text in an open-ended way, therefore translating the theoretical algorithms into practical applications. The large language models (LLMs) have significantly impacted businesses such as report summarization software and copywriters. Observations indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility. Large-scale benchmarks for accountable LLMs should consequently be developed. Although several empirical investigations reveal the existence of a few ethical difficulties in advanced LLMs, there is little systematic examination and user study of the risks and harmful behaviors of current LLM usage. To further educate future efforts on constructing ethical LLMs responsibly, we perform a qualitative research method called ``red teaming'' on OpenAI's ChatGPT\footnote{In this paper, ChatGPT refers to the version released on Dec 15th.} to better understand the practical features of ethical dangers in recent LLMs. We analyze ChatGPT comprehensively from four perspectives: 1) \textit{Bias} 2) \textit{Reliability} 3) \textit{Robustness} 4) \textit{Toxicity}. In accordance with our stated viewpoints, we empirically benchmark ChatGPT on multiple sample datasets. We find that a significant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies. In addition, we examine the implications of our findings on AI ethics and harmal behaviors of ChatGPT, as well as future problems and practical design considerations for responsible LLMs. We believe that our findings may give light on future efforts to determine and mitigate the ethical hazards posed by machines in LLM applications.",,http://arxiv.org/abs/2301.12867v4,arxiv,,,,,,,,,pdf_cache/pdfs/bae59d3841748138.pdf,cached,2301.128674,,http://arxiv.org/pdf/2301.12867v4,cs.CL; cs.SE,2cb5c3be8fac4bbf73f47a870f61277d81b5df6c772beb0427f54bab34ceef27,,,,,,en,tech-report,none,yes,no,ChatGPT,none,"Empirical benchmarking on multiple sample datasets, qualitative analysis of tweets, and case studies.",bias|jailbreak|reliability|robustness|toxicity,5,none,yes,success,1.0,bias|jailbreak,,,,
SCREEN_0067,Can Large Language Models Change User Preference Adversarially?,Varshini Subhash,2023,,"Pretrained large language models (LLMs) are becoming increasingly powerful and ubiquitous in mainstream applications such as being a personal assistant, a dialogue model, etc. As these models become proficient in deducing user preferences and offering tailored assistance, there is an increasing concern about the ability of these models to influence, modify and in the extreme case manipulate user preference adversarially. The issue of lack of interpretability in these models in adversarial settings remains largely unsolved. This work tries to study adversarial behavior in user preferences from the lens of attention probing, red teaming and white-box analysis. Specifically, it provides a bird's eye view of existing literature, offers red teaming samples for dialogue models like ChatGPT and GODEL and probes the attention mechanism in the latter for non-adversarial and adversarial settings.",,http://arxiv.org/abs/2302.10291v1,arxiv,,,,,,,,,pdf_cache/pdfs/2d21ba5892885016.pdf,cached,2302.102911,,http://arxiv.org/pdf/2302.10291v1,cs.CL; cs.LG,fc56111d9bf7e86b80a8bdb06f1ca817fc0b3643119ad40f540c99e37d318683,,,,,,en,tech-report,digital,yes,no,"ChatGPT, GODEL-770M",player,"The paper discusses adversarial behavior and interpretability through attention probing, red teaming, and white-box analysis.",adversarial behavior|deception|manipulation|persuasion,4,none,yes,success,1.0,deception,,,,
SCREEN_0068,Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback,Hannah Rose Kirk; Bertie Vidgen; Paul Röttger; Scott A. Hale,2023,,"Large language models (LLMs) are used to generate content for a wide range of tasks, and are set to reach a growing audience in coming years due to integration in product interfaces like ChatGPT or search engines like Bing. This intensifies the need to ensure that models are aligned with human preferences and do not produce unsafe, inaccurate or toxic outputs. While alignment techniques like reinforcement learning with human feedback (RLHF) and red-teaming can mitigate some safety concerns and improve model capabilities, it is unlikely that an aggregate fine-tuning process can adequately represent the full range of users' preferences and values. Different people may legitimately disagree on their preferences for language and conversational norms, as well as on values or ideologies which guide their communication. Personalising LLMs through micro-level preference learning processes may result in models that are better aligned with each user. However, there are several normative challenges in defining the bounds of a societally-acceptable and safe degree of personalisation. In this paper, we ask how, and in what ways, LLMs should be personalised. First, we review literature on current paradigms for aligning LLMs with human feedback, and identify issues including (i) a lack of clarity regarding what alignment means; (ii) a tendency of technology providers to prescribe definitions of inherently subjective preferences and values; and (iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in who we are really aligning to. Second, we present a taxonomy of benefits and risks associated with personalised LLMs, for individuals and society at large. Finally, we propose a three-tiered policy framework that allows users to experience the benefits of personalised alignment, while restraining unsafe and undesirable LLM-behaviours within (supra-)national and organisational bounds.",,http://arxiv.org/abs/2303.05453v1,arxiv,,,,,,,,,pdf_cache/pdfs/d5b01087d81998a6.pdf,cached,2303.054531,,http://arxiv.org/pdf/2303.05453v1,cs.CL; cs.CY,81228219a04157bd8bed3854982f7197776f39d9ec1d5d3a86b2dcdf3d9df4c7,,,,,,en,tech-report,none,no,no,none,none,none,factual_error,6,none,yes,success,1.0,factual_error,,,,
SCREEN_0069,Jailbroken: How Does LLM Safety Training Fail?,Alexander Wei; Nika Haghtalab; Jacob Steinhardt,2023,,"Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of ""jailbreak"" attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes.",,http://arxiv.org/abs/2307.02483v1,arxiv,,,,,,,,,pdf_cache/pdfs/0dbf421ed2170e11.pdf,cached,2307.024831,,http://arxiv.org/pdf/2307.02483v1,cs.LG; cs.CR,53d7ff7f10b7e12fbcef827df4c69fb800c5c6213f855ea29d0929d213c907a9,,,,,,en,tech-report,digital,yes,no,"GPT-4, Claude v1.3",player,Success of jailbreak attacks measured by the ability to elicit on-topic responses to restricted prompts.,competing objectives|jailbreak|mismatched generalization,6,none,yes,success,1.0,jailbreak,gpt4|claude,,,
SCREEN_0070,Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models,Hsuan Su; Cheng-Chu Cheng; Hua Farn; Shachi H Kumar; Saurav Sahay; Shang-Tse Chen; Hung-yi Lee,2023,,"Recently, researchers have made considerable improvements in dialogue systems with the progress of large language models (LLMs) such as ChatGPT and GPT-4. These LLM-based chatbots encode the potential biases while retaining disparities that can harm humans during interactions. The traditional biases investigation methods often rely on human-written test cases. However, these test cases are usually expensive and limited. In this work, we propose a first-of-its-kind method that automatically generates test cases to detect LLMs' potential gender bias. We apply our method to three well-known LLMs and find that the generated test cases effectively identify the presence of biases. To address the biases identified, we propose a mitigation strategy that uses the generated test cases as demonstrations for in-context learning to circumvent the need for parameter fine-tuning. The experimental results show that LLMs generate fairer responses with the proposed approach.",,http://arxiv.org/abs/2310.11079v1,arxiv,,,,,,,,,pdf_cache/pdfs/db81df0d3d51e303.pdf,cached,2310.110791,,http://arxiv.org/pdf/2310.11079v1,cs.CL; cs.AI,9ed4b08da502c3c383444b5d2117ce352089c224d5366ce6ac50b353af853128,,,,,,en,tech-report,digital,no,yes,GPT-4,generator,Bias quantification using sentiment analysis and reinforcement learning reward functions.,bias,6,none,yes,success,1.0,bias,gpt4,,,
SCREEN_0071,Low-Resource Languages Jailbreak GPT-4,Zheng-Xin Yong; Cristina Menghini; Stephen H. Bach,2023,,"AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4's safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift: this deficiency now poses a risk to all LLMs users. Publicly available translation APIs enable anyone to exploit LLMs' safety vulnerabilities. Therefore, our work calls for a more holistic red-teaming efforts to develop robust multilingual safeguards with wide language coverage.",,http://arxiv.org/abs/2310.02446v2,arxiv,,,,,,,,,pdf_cache/pdfs/69427e5c21d52091.pdf,cached,2310.024462,,http://arxiv.org/pdf/2310.02446v2,cs.CL; cs.AI; cs.CR; cs.LG,8ac95afe653d5bdff7d04b9ae16cea4297fc61f310b56597fa95f08a045dfd66,,,,,,en,workshop,none,no,yes,GPT-4,player,"Attack success rate as the percentage of BYPASS, comparing success rates across different languages and jailbreaking methods.",jailbreak|mismatched generalization,6,none,no,success,1.0,jailbreak,gpt4,,,
SCREEN_0072,Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts,Yuanwei Wu; Xiang Li; Yixin Liu; Pan Zhou; Lichao Sun,2023,,"Existing work on jailbreak Multimodal Large Language Models (MLLMs) has focused primarily on adversarial examples in model inputs, with less attention to vulnerabilities, especially in model API. To fill the research gap, we carry out the following work: 1) We discover a system prompt leakage vulnerability in GPT-4V. Through carefully designed dialogue, we successfully extract the internal system prompts of GPT-4V. This finding indicates potential exploitable security risks in MLLMs; 2) Based on the acquired system prompts, we propose a novel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack via System Prompt). By employing GPT-4 as a red teaming tool against itself, we aim to search for potential jailbreak prompts leveraging stolen system prompts. Furthermore, in pursuit of better performance, we also add human modification based on GPT-4's analysis, which further improves the attack success rate to 98.7\%; 3) We evaluated the effect of modifying system prompts to defend against jailbreaking attacks. Results show that appropriately designed system prompts can significantly reduce jailbreak success rates. Overall, our work provides new insights into enhancing MLLM security, demonstrating the important role of system prompts in jailbreaking. This finding could be leveraged to greatly facilitate jailbreak success rates while also holding the potential for defending against jailbreaks.",,http://arxiv.org/abs/2311.09127v2,arxiv,,,,,,,,,pdf_cache/pdfs/b8de24ad0a63d267.pdf,cached,2311.091272,,http://arxiv.org/pdf/2311.09127v2,cs.CR; cs.AI; cs.LG,76db6e778e6b65e5e6441f4a438ac74d1929ecfdc2efdaf10bcbea14345bd71b,,,,,,en,tech-report,digital,no,yes,GPT-4V,player,"Attack success rate, recognition success rate, defense success rate, sensitive traits inference",jailbreak,6,none,yes,success,1.0,jailbreak,gpt4,,,
SCREEN_0073,Causality Analysis for Evaluating the Security of Large Language Models,Wei Zhao; Zhe Li; Jun Sun,2023,,"Large Language Models (LLMs) such as GPT and Llama2 are increasingly adopted in many safety-critical applications. Their security is thus essential. Even with considerable efforts spent on reinforcement learning from human feedback (RLHF), recent studies have shown that LLMs are still subject to attacks such as adversarial perturbation and Trojan attacks. Further research is thus needed to evaluate their security and/or understand the lack of it. In this work, we propose a framework for conducting light-weight causality-analysis of LLMs at the token, layer, and neuron level. We applied our framework to open-source LLMs such as Llama2 and Vicuna and had multiple interesting discoveries. Based on a layer-level causality analysis, we show that RLHF has the effect of overfitting a model to harmful prompts. It implies that such security can be easily overcome by `unusual' harmful prompts. As evidence, we propose an adversarial perturbation method that achieves 100\% attack success rate on the red-teaming tasks of the Trojan Detection Competition 2023. Furthermore, we show the existence of one mysterious neuron in both Llama2 and Vicuna that has an unreasonably high causal effect on the output. While we are uncertain on why such a neuron exists, we show that it is possible to conduct a ``Trojan'' attack targeting that particular neuron to completely cripple the LLM, i.e., we can generate transferable suffixes to prompts that frequently make the LLM produce meaningless responses.",,http://arxiv.org/abs/2312.07876v1,arxiv,,,,,,,,,pdf_cache/pdfs/82b6ebf005f4a410.pdf,cached,2312.078761,,http://arxiv.org/pdf/2312.07876v1,cs.AI,6cd8e35bbc07bfcac19e0efa8ec2ee4ee752064dbdf3783ede0bc9b76a04ed98,,,,,,en,tech-report,none,no,no,"Llama2, Vicuna",none,"Average causal effect, causal mediation analysis, attack success rate",adversarial-perturbation|trojan-attack,6,https://casperllm.github.io/,yes,success,1.0,,,,,mentioned
SCREEN_0074,Red Teaming Visual Language Models,Mukai Li; Lei Li; Yuwei Yin; Masood Ahmed; Zhenguang Liu; Qi Liu,2024,,"VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM test set, 13% in MM-Hal, and without noticeable decline in MM-Bench, overpassing other LLaVA-based models with regular alignment data. This reveals that current open-sourced VLMs still lack red teaming alignment. Our code and datasets will be open-source.",,http://arxiv.org/abs/2401.12915v1,arxiv,,,,,,,,,pdf_cache/pdfs/99af576ace994aa4.pdf,cached,2401.129151,,http://arxiv.org/pdf/2401.12915v1,cs.AI; cs.CL; cs.CV,78e3bbbd1a546584e96b50dba69dd54113393012f6e6c09958699e69c4cd7acb,,,,,,en,tech-report,digital,no,yes,GPT-4V,analyst,"Performance gap with GPT-4V, improvement percentage on RTVLM test set, MM-Hal, and MM-Bench",bias|deception|factual_error|privacy violations|safety issues,6,https://huggingface.co/datasets/MMInstruction/RedTeamingVLM,yes,success,1.0,deception|factual_error,,,,mentioned
SCREEN_0075,Investigating Bias Representations in Llama 2 Chat via Activation Steering,Dawn Lu; Nina Rimsky,2024,,"We address the challenge of societal bias in Large Language Models (LLMs), focusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into decision-making processes with substantial societal impact, it becomes imperative to ensure these models do not reinforce existing biases. Our approach employs activation steering to probe for and mitigate biases related to gender, race, and religion. This method manipulates model activations to direct responses towards or away from biased outputs, utilizing steering vectors derived from the StereoSet dataset and custom GPT4 generated gender bias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF). We also observe a predictable negative correlation between bias and the model's tendency to refuse responses. Significantly, our study uncovers that RLHF tends to increase the similarity in the model's representation of different forms of societal biases, which raises questions about the model's nuanced understanding of different forms of bias. This work also provides valuable insights into effective red-teaming strategies for LLMs using activation steering, particularly emphasizing the importance of integrating a refusal vector.",,http://arxiv.org/abs/2402.00402v1,arxiv,,,,,,,,,pdf_cache/pdfs/f04301c9732c00b9.pdf,cached,2402.004021,,http://arxiv.org/pdf/2402.00402v1,cs.CL; cs.AI,8a9bba81c520a0934f33c2a59449f64702445fa33d4d05c41319aa5edfd35726,,,,,,en,tech-report,none,no,no,Llama 2 7B Chat,none,Cosine similarity between bias and refusal vectors; effectiveness of steering vectors in eliciting biased responses,bias|deception,6,https://github.com/matutinus/SPAR/blob/main/gender_stereotypes_augmented_A_B.json,yes,success,1.0,bias|deception,gpt4|llama,,,
SCREEN_0076,HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback,Ang Li; Qiugen Xiao; Peng Cao; Jian Tang; Yi Yuan; Zijie Zhao; Xiaoyuan Chen; Liang Zhang; Xiangyang Li; Kaitong Yang; Weidong Guo; Yukang Gan; Xu Yu; Daniell Wang; Ying Shan,2024,,"Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter annotation cycles and lower costs over Reinforcement Learning from Human Feedback (RLHF), making it highly efficient during the rapid strategy iteration periods of large language model (LLM) training. Using ChatGPT as a labeler to provide feedback on open-domain prompts in RLAIF training, we observe an increase in human evaluators' preference win ratio for model responses, but a decrease in evaluators' satisfaction rate. Analysis suggests that the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness, highlighting practical limitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement Learning from AI Feedback (HRLAIF). This method enhances the accuracy of AI annotations for responses, making the model's helpfulness more robust in training process. Additionally, it employs AI for Red Teaming, further improving the model's harmlessness. Human evaluation results show that HRLAIF inherits the ability of RLAIF to enhance human preference for outcomes at a low cost while also improving the satisfaction rate of responses. Compared to the policy model before Reinforcement Learning (RL), it achieves an increase of 2.08\% in satisfaction rate, effectively addressing the issue of a decrease of 4.58\% in satisfaction rate after basic RLAIF.",,http://arxiv.org/abs/2403.08309v2,arxiv,,,,,,,,,pdf_cache/pdfs/c3149a86b2366ea1.pdf,cached,2403.083092,,http://arxiv.org/pdf/2403.08309v2,cs.LG; cs.AI,04358c28677c77e701099bf7f2bee182723a3e76558074eafab80af501beea41,,,,,,en,tech-report,digital,no,yes,GPT-4,analyst,"Human evaluators' preference win ratio, satisfaction rate, accuracy of AI annotations, correctness verification, reasoning process preference labeling.",bias|hallucination|truthfulness,6,none,yes,success,1.0,,,,accuracy|human_evaluation,
SCREEN_0077,Safety Alignment for Vision Language Models,Zhendong Liu; Yuanbi Nie; Yingshui Tan; Xiangyu Yue; Qiushi Cui; Chongjun Wang; Xiaoyong Zhu; Bo Zheng,2024,,"Benefiting from the powerful capabilities of Large Language Models (LLMs), pre-trained visual encoder models connected to an LLMs can realize Vision Language Models (VLMs). However, existing research shows that the visual modality of VLMs is vulnerable, with attackers easily bypassing LLMs' safety alignment through visual modality features to launch attacks. To address this issue, we enhance the existing VLMs' visual modality safety alignment by adding safety modules, including a safety projector, safety tokens, and a safety head, through a two-stage training process, effectively improving the model's defense against risky images. For example, building upon the LLaVA-v1.5 model, we achieve a safety score of 8.26, surpassing the GPT-4V on the Red Teaming Visual Language Models (RTVLM) benchmark. Our method boasts ease of use, high flexibility, and strong controllability, and it enhances safety while having minimal impact on the model's general performance. Moreover, our alignment strategy also uncovers some possible risky content within commonly used open-source multimodal datasets. Our code will be open sourced after the anonymous review.",,http://arxiv.org/abs/2405.13581v1,arxiv,,,,,,,,,pdf_cache/pdfs/5778250f7f429041.pdf,cached,2405.135811,,http://arxiv.org/pdf/2405.13581v1,cs.CV; cs.AI,5c0d54e94ae3e3e24b1d9c6fab24230a25b06f63e9973b1364b762a506fc8bd4,,,,,,en,tech-report,digital,no,yes,GPT-4V,analyst,"Safety score on the RTVLM benchmark, human subjective assessment, and GPT-4V evaluation prompts.",,6,none,yes,success,1.0,,,,,mentioned
SCREEN_0078,Red-Teaming for Inducing Societal Bias in Large Language Models,Chu Fei Luo; Ahmad Ghawanmeh; Bharat Bhimshetty; Kashyap Murali; Murli Jadhav; Xiaodan Zhu; Faiza Khan Khattak,2024,,"Ensuring the safe deployment of AI systems is critical in industry settings where biased outputs can lead to significant operational, reputational, and regulatory risks. Thorough evaluation before deployment is essential to prevent these hazards. Red-teaming addresses this need by employing adversarial attacks to develop guardrails that detect and reject biased or harmful queries, enabling models to be retrained or steered away from harmful outputs. However, most red-teaming efforts focus on harmful or unethical instructions rather than addressing social bias, leaving this critical area under-explored despite its significant real-world impact, especially in customer-facing systems. We propose two bias-specific red-teaming methods, Emotional Bias Probe (EBP) and BiasKG, to evaluate how standard safety measures for harmful content affect bias. For BiasKG, we refactor natural language stereotypes into a knowledge graph. We use these attacking strategies to induce biased responses from several open- and closed-source language models. Unlike prior work, these methods specifically target social bias. We find our method increases bias in all models, even those trained with safety guardrails. Our work emphasizes uncovering societal bias in LLMs through rigorous evaluation, and recommends measures ensure AI safety in high-stakes industry deployments.",,http://arxiv.org/abs/2405.04756v2,arxiv,,,,,,,,,pdf_cache/pdfs/9036d9ca5cde8b01.pdf,cached,2405.047562,,http://arxiv.org/pdf/2405.04756v2,cs.CL; cs.LG,a66246e9d643a6fd46ee01fe31a947a1db269d157066794828a48eb48cce35b0,,,,,,en,tech-report,digital,no,yes,GPT-4,analyst,"Bias rate, deception rate, cosine similarity, 1-gram overlap",bias|deception,6,https://github.com/VectorInstitute/biaskg,yes,success,1.0,bias|deception,,,,
SCREEN_0079,Jailbreaking as a Reward Misspecification Problem,Zhihui Xie; Jiahui Gao; Lei Li; Zhenguo Li; Qi Liu; Lingpeng Kong,2024,,"The widespread adoption of large language models (LLMs) has raised concerns about their safety and reliability, particularly regarding their vulnerability to adversarial attacks. In this paper, we propose a novel perspective that attributes this vulnerability to reward misspecification during the alignment process. This misspecification occurs when the reward function fails to accurately capture the intended behavior, leading to misaligned model outputs. We introduce a metric ReGap to quantify the extent of reward misspecification and demonstrate its effectiveness and robustness in detecting harmful backdoor prompts. Building upon these insights, we present ReMiss, a system for automated red teaming that generates adversarial prompts in a reward-misspecified space. ReMiss achieves state-of-the-art attack success rates on the AdvBench benchmark against various target aligned LLMs while preserving the human readability of the generated prompts. Furthermore, these attacks on open-source models demonstrate high transferability to closed-source models like GPT-4o and out-of-distribution tasks from HarmBench. Detailed analysis highlights the unique advantages of the proposed reward misspecification objective compared to previous methods, offering new insights for improving LLM safety and robustness.",,http://arxiv.org/abs/2406.14393v5,arxiv,,,,,,,,,pdf_cache/pdfs/db08a32b8afe622f.pdf,cached,2406.143935,,http://arxiv.org/pdf/2406.14393v5,cs.LG; cs.CL,158dbdce9f61276082d9eb319c81ccb83fdc5790cac28565de53493ed87b6e1e,,,,,,en,conference,digital,no,yes,GPT-4o,player,"ReGap metric to quantify reward misspecification, attack success rates on AdvBench benchmark, transferability to closed-source models, human readability of prompts",adversarial vulnerability|jailbreak|reward misspecification,6,https://github.com/zhxieml/remiss-jailbreak,no,success,1.0,jailbreak,,,,mentioned
SCREEN_0080,Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts,Yi Liu; Chengjun Cai; Xiaoli Zhang; Xingliang Yuan; Cong Wang,2024,,"Large Vision Language Models (VLMs) extend and enhance the perceptual abilities of Large Language Models (LLMs). Despite offering new possibilities for LLM applications, these advancements raise significant security and ethical concerns, particularly regarding the generation of harmful content. While LLMs have undergone extensive security evaluations with the aid of red teaming frameworks, VLMs currently lack a well-developed one. To fill this gap, we introduce Arondight, a standardized red team framework tailored specifically for VLMs. Arondight is dedicated to resolving issues related to the absence of visual modality and inadequate diversity encountered when transitioning existing red teaming methodologies from LLMs to VLMs. Our framework features an automated multi-modal jailbreak attack, wherein visual jailbreak prompts are produced by a red team VLM, and textual prompts are generated by a red team LLM guided by a reinforcement learning agent. To enhance the comprehensiveness of VLM security evaluation, we integrate entropy bonuses and novelty reward metrics. These elements incentivize the RL agent to guide the red team LLM in creating a wider array of diverse and previously unseen test cases. Our evaluation of ten cutting-edge VLMs exposes significant security vulnerabilities, particularly in generating toxic images and aligning multi-modal prompts. In particular, our Arondight achieves an average attack success rate of 84.5\% on GPT-4 in all fourteen prohibited scenarios defined by OpenAI in terms of generating toxic text. For a clearer comparison, we also categorize existing VLMs based on their safety levels and provide corresponding reinforcement recommendations. Our multimodal prompt dataset and red team code will be released after ethics committee approval. CONTENT WARNING: THIS PAPER CONTAINS HARMFUL MODEL RESPONSES.",,http://arxiv.org/abs/2407.15050v1,arxiv,,,,,,,,,pdf_cache/pdfs/f36fc060450529b6.pdf,cached,2407.150501,,http://arxiv.org/pdf/2407.15050v1,cs.LG; cs.AI; cs.CR; cs.MM,c4b983b27c840a4bbd946bd7cffbe1f58a35448fbfe1a8ff36b1e3249b982c9b,,,,,,en,conference,digital,no,yes,GPT-4,generator,"Entropy bonuses, novelty reward metrics, attack success rate, safety level categorization",alignment issues|inadequate diversity|jailbreak|toxic content generation|visual input oversight,6,none,no,success,1.0,jailbreak,gpt4,,,
SCREEN_0081,AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases,Zhaorun Chen; Zhen Xiang; Chaowei Xiao; Dawn Song; Bo Li,2024,,"LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a memory module or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach AgentPoison, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or RAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, AgentPoison requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, in-context coherence, and stealthiness. Extensive experiments demonstrate AgentPoison's effectiveness in attacking three types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent. On each agent, AgentPoison achieves an average attack success rate higher than 80% with minimal impact on benign performance (less than 1%) with a poison rate less than 0.1%.",,http://arxiv.org/abs/2407.12784v1,arxiv,,,,,,,,,pdf_cache/pdfs/1c6a02766a3236aa.pdf,cached,2407.127841,,http://arxiv.org/pdf/2407.12784v1,cs.LG; cs.CR; cs.IR,008c4b0f70150ae2e26cead8ca6bad773cb4e2cd370154a10db320a456ea5ee1,,,,,,en,tech-report,digital,no,yes,OpenAI-ADA,player,"Attack success rate, retrieval success rate, end-to-end attack success rate, impact on benign performance, poisoning ratio",,6,https://github.com/BillChan226/AgentPoison,yes,success,1.0,,,,,
SCREEN_0082,Safe Generative Chats in a WhatsApp Intelligent Tutoring System,Zachary Levonian; Owen Henkel,2024,,"Large language models (LLMs) are flexible, personalizable, and available, which makes their use within Intelligent Tutoring Systems (ITSs) appealing. However, that flexibility creates risks: inaccuracies, harmful content, and non-curricular material. Ethically deploying LLM-backed ITS systems requires designing safeguards that ensure positive experiences for students. We describe the design of a conversational system integrated into an ITS, and our experience evaluating its safety with red-teaming, an in-classroom usability test, and field deployment. We present empirical data from more than 8,000 student conversations with this system, finding that GPT-3.5 rarely generates inappropriate messages. Comparatively more common is inappropriate messages from students, which prompts us to reason about safeguarding as a content moderation and classroom management problem. The student interaction behaviors we observe provide implications for designers - to focus on student inputs as a content moderation problem - and implications for researchers - to focus on subtle forms of bad content.",,http://arxiv.org/abs/2407.04915v1,arxiv,,,,,,,,,pdf_cache/pdfs/7acd8ab9fa914a8a.pdf,cached,2407.049151,,http://arxiv.org/pdf/2407.04915v1,cs.HC,48d5ab84930a5b9fb4c43863cd6088d9c936beabfb3c695e85540bdb821a5c9f,,,,,,en,workshop,digital,no,no,GPT-3.5,generator,"Usability test, field deployment, red-teaming exercise, student conversation ratings",factual_error|yea-sayer effect,5,https://github.com/DigitalHarborFoundation/chatbot-safety,no,success,1.0,factual_error,gpt35,,,
SCREEN_0083,Purple-teaming LLMs with Adversarial Defender Training,Jingyan Zhou; Kun Li; Junan Li; Jiawen Kang; Minda Hu; Xixin Wu; Helen Meng,2024,,"Existing efforts in safeguarding LLMs are limited in actively exposing the vulnerabilities of the target LLM and readily adapting to newly emerging safety risks. To address this, we present Purple-teaming LLMs with Adversarial Defender training (PAD), a pipeline designed to safeguard LLMs by novelly incorporating the red-teaming (attack) and blue-teaming (safety training) techniques. In PAD, we automatically collect conversational data that cover the vulnerabilities of an LLM around specific safety risks in a self-play manner, where the attacker aims to elicit unsafe responses and the defender generates safe responses to these attacks. We then update both modules in a generative adversarial network style by training the attacker to elicit more unsafe responses and updating the defender to identify them and explain the unsafe reason. Experimental results demonstrate that PAD significantly outperforms existing baselines in both finding effective attacks and establishing a robust safe guardrail. Furthermore, our findings indicate that PAD excels in striking a balance between safety and overall model quality. We also reveal key challenges in safeguarding LLMs, including defending multi-turn attacks and the need for more delicate strategies to identify specific risks.",,http://arxiv.org/abs/2407.01850v1,arxiv,,,,,,,,,pdf_cache/pdfs/4cd0f61c7d83d4f6.pdf,cached,2407.018501,,http://arxiv.org/pdf/2407.01850v1,cs.CL,4b9bfd0b5cbce180b363c6b59e234839b0370350f34824cc35b2dc9ff040101a,,,,,,en,tech-report,digital,yes,no,GPT-4,player,"Effectiveness in finding attacks and establishing safety guardrails, balance between safety and model quality, robustness in multi-turn attacks.",multi-turn attacks|safety concerns requiring real-world knowledge|safety concerns requiring rigorous reasoning,5,none,yes,success,1.0,,,,,
SCREEN_0084,Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models,Hongfu Liu; Yuxi Xie; Ye Wang; Michael Shieh,2024,,"Language Language Models (LLMs) face safety concerns due to potential misuse by malicious users. Recent red-teaming efforts have identified adversarial suffixes capable of jailbreaking LLMs using the gradient-based search algorithm Greedy Coordinate Gradient (GCG). However, GCG struggles with computational inefficiency, limiting further investigations regarding suffix transferability and scalability across models and data. In this work, we bridge the connection between search efficiency and suffix transferability. We propose a two-stage transfer learning framework, DeGCG, which decouples the search process into behavior-agnostic pre-searching and behavior-relevant post-searching. Specifically, we employ direct first target token optimization in pre-searching to facilitate the search process. We apply our approach to cross-model, cross-data, and self-transfer scenarios. Furthermore, we introduce an interleaved variant of our approach, i-DeGCG, which iteratively leverages self-transferability to accelerate the search process. Experiments on HarmBench demonstrate the efficiency of our approach across various models and domains. Notably, our i-DeGCG outperforms the baseline on Llama2-chat-7b with ASRs of $43.9$ ($+22.2$) and $39.0$ ($+19.5$) on valid and test sets, respectively. Further analysis on cross-model transfer indicates the pivotal role of first target token optimization in leveraging suffix transferability for efficient searching.",,http://arxiv.org/abs/2408.14866v2,arxiv,,,,,,,,,pdf_cache/pdfs/0fe58103bfdcab82.pdf,cached,2408.148662,,http://arxiv.org/pdf/2408.14866v2,cs.CL; cs.CR; cs.LG,5a14bebdd359a2e40d95bdd19180fee377c2bc6539c297c392730c02406720df,,,,,,en,tech-report,none,no,yes,Llama2-chat-7b,none,Attack Success Rate (ASR) on validation and test sets,jailbreak,6,https://github.com/Waffle-Liu/DeGCG,yes,success,1.0,jailbreak,,,,
SCREEN_0085,Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models,Hongbang Yuan; Zhuoran Jin; Pengfei Cao; Yubo Chen; Kang Liu; Jun Zhao,2024,,"LLM have achieved success in many fields but still troubled by problematic content in the training corpora. LLM unlearning aims at reducing their influence and avoid undesirable behaviours. However, existing unlearning methods remain vulnerable to adversarial queries and the unlearned knowledge resurfaces after the manually designed attack queries. As part of a red-team effort to proactively assess the vulnerabilities of unlearned models, we design Dynamic Unlearning Attack (DUA), a dynamic and automated framework to attack these models and evaluate their robustness. It optimizes adversarial suffixes to reintroduce the unlearned knowledge in various scenarios. We find that unlearned knowledge can be recovered in $55.2\%$ of the questions, even without revealing the unlearned model's parameters. In response to this vulnerability, we propose Latent Adversarial Unlearning (LAU), a universal framework that effectively enhances the robustness of the unlearned process. It formulates the unlearning process as a min-max optimization problem and resolves it through two stages: an attack stage, where perturbation vectors are trained and added to the latent space of LLMs to recover the unlearned knowledge, and a defense stage, where previously trained perturbation vectors are used to enhance unlearned model's robustness. With our LAU framework, we obtain two robust unlearning methods, AdvGA and AdvNPO. We conduct extensive experiments across multiple unlearning benchmarks and various models, and demonstrate that they improve the unlearning effectiveness by over $53.5\%$, cause only less than a $11.6\%$ reduction in neighboring knowledge, and have almost no impact on the model's general capabilities.",,http://arxiv.org/abs/2408.10682v1,arxiv,,,,,,,,,pdf_cache/pdfs/f82cbfb83081bfd1.pdf,cached,2408.106821,,http://arxiv.org/pdf/2408.10682v1,cs.CL; cs.AI; cs.CR; cs.LG,0e610a0b306569a438f3870ebae6c0c9ff32271afaf822703313a8cd733dad19,,,,,,en,tech-report,digital,no,yes,GPT-4,analyst,"ROUGE-L score, membership inference attacks (MIA), reasoning ability (Rea), truthfulness (Tru), factuality (Fac), fluency (Flu)",,6,https://github.com/HongbangYuan/RobustUnlearning,yes,success,1.0,,,,,
SCREEN_0086,SAGE-RT: Synthetic Alignment data Generation for Safety Evaluation and Red Teaming,Anurakt Kumar; Divyanshu Kumar; Jatan Loya; Nitin Aravind Birur; Tanay Baswa; Sahil Agarwal; Prashanth Harshangi,2024,,"We introduce Synthetic Alignment data Generation for Safety Evaluation and Red Teaming (SAGE-RT or SAGE) a novel pipeline for generating synthetic alignment and red-teaming data. Existing methods fall short in creating nuanced and diverse datasets, providing necessary control over the data generation and validation processes, or require large amount of manually generated seed data. SAGE addresses these limitations by using a detailed taxonomy to produce safety-alignment and red-teaming data across a wide range of topics. We generated 51,000 diverse and in-depth prompt-response pairs, encompassing over 1,500 topics of harmfulness and covering variations of the most frequent types of jailbreaking prompts faced by large language models (LLMs). We show that the red-teaming data generated through SAGE jailbreaks state-of-the-art LLMs in more than 27 out of 32 sub-categories, and in more than 58 out of 279 leaf-categories (sub-sub categories). The attack success rate for GPT-4o, GPT-3.5-turbo is 100% over the sub-categories of harmfulness. Our approach avoids the pitfalls of synthetic safety-training data generation such as mode collapse and lack of nuance in the generation pipeline by ensuring a detailed coverage of harmful topics using iterative expansion of the topics and conditioning the outputs on the generated raw-text. This method can be used to generate red-teaming and alignment data for LLM Safety completely synthetically to make LLMs safer or for red-teaming the models over a diverse range of topics.",,http://arxiv.org/abs/2408.11851v1,arxiv,,,,,,,,,pdf_cache/pdfs/53decc0cd670b009.pdf,cached,2408.118511,,http://arxiv.org/pdf/2408.11851v1,cs.AI; cs.CL; cs.CR,563ff8f72d168a61a087a284d766248d9c15abd8eef48624639a9e653a5f9958,,,,,,en,tech-report,digital,yes,no,"GPT-4, GPT-3.5-turbo, Mistral",generator,Attack success rate over sub-categories of harmfulness,jailbreak|lack of nuance|mode collapse,5,none,yes,success,1.0,jailbreak,gpt35,,,
SCREEN_0087,"LLM-Assisted Red Teaming of Diffusion Models through ""Failures Are Fated, But Can Be Faded""",Som Sagar; Aditya Taparia; Ransalu Senanayake,2024,,"In large deep neural networks that seem to perform surprisingly well on many tasks, we also observe a few failures related to accuracy, social biases, and alignment with human values, among others. Therefore, before deploying these models, it is crucial to characterize this failure landscape for engineers to debug or audit models. Nevertheless, it is infeasible to exhaustively test for all possible combinations of factors that could lead to a model's failure. In this paper, we improve the ""Failures are fated, but can be faded"" framework (arXiv:2406.07145)--a post-hoc method to explore and construct the failure landscape in pre-trained generative models--with a variety of deep reinforcement learning algorithms, screening tests, and LLM-based rewards and state generation. With the aid of limited human feedback, we then demonstrate how to restructure the failure landscape to be more desirable by moving away from the discovered failure modes. We empirically demonstrate the effectiveness of the proposed method on diffusion models. We also highlight the strengths and weaknesses of each algorithm in identifying failure modes.",,http://arxiv.org/abs/2410.16738v1,arxiv,,,,,,,,,pdf_cache/pdfs/174cbe834eb7b325.pdf,cached,2410.167381,,http://arxiv.org/pdf/2410.16738v1,cs.LG,6ae5c03e0da889bd6001109dc9d9055d83bd6a224272ad9328f8559b68c2fd06,,,,,,en,workshop,digital,yes,yes,GPT-4,generator,"Qualitative and quantitative summarization of failure modes, reward function based on probability of failure.",accuracy|alignment with human values|social biases,4,none,yes,success,1.0,,,,accuracy,
SCREEN_0088,Utilizing ChatGPT Generated Data to Retrieve Depression Symptoms from Social Media,Ana-Maria Bucur,2023,,"In this work, we present the contribution of the BLUE team in the eRisk Lab task on searching for symptoms of depression. The task consists of retrieving and ranking Reddit social media sentences that convey symptoms of depression from the BDI-II questionnaire. Given that synthetic data provided by LLMs have been proven to be a reliable method for augmenting data and fine-tuning downstream models, we chose to generate synthetic data using ChatGPT for each of the symptoms of the BDI-II questionnaire. We designed a prompt such that the generated data contains more richness and semantic diversity than the BDI-II responses for each question and, at the same time, contains emotional and anecdotal experiences that are specific to the more intimate way of sharing experiences on Reddit. We perform semantic search and rank the sentences' relevance to the BDI-II symptoms by cosine similarity. We used two state-of-the-art transformer-based models (MentalRoBERTa and a variant of MPNet) for embedding the social media posts, the original and generated responses of the BDI-II. Our results show that using sentence embeddings from a model designed for semantic search outperforms the approach using embeddings from a model pre-trained on mental health data. Furthermore, the generated synthetic data were proved too specific for this task, the approach simply relying on the BDI-II responses had the best performance.",,http://arxiv.org/abs/2307.02313v2,arxiv,,,,,,,,,pdf_cache/pdfs/c92ae84d949721fa.pdf,cached,2307.023132,,http://arxiv.org/pdf/2307.02313v2,cs.CL,f1ebf60850be6673d0b04c429647c4286b77d531f275eb1c61b53acde18599be,,,,,,en,workshop,none,no,no,ChatGPT,generator,"Average Precision (AP), R-Precision, Precision at 10 (P@10), Normalized Discounted Cumulative Gain at 1000 (NDCG@1000)",,6,none,yes,success,1.0,,,,,
SCREEN_0089,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Rishabh Bhardwaj; Soujanya Poria,2023,,"Larger language models (LLMs) have taken the world by storm with their massive multi-tasking capabilities simply by optimizing over a next-word prediction objective. With the emergence of their properties and encoded knowledge, the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public. In this work, we propose a new safety evaluation benchmark RED-EVAL that carries out red-teaming. We show that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries. We also demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in generating harmful responses in more than 86% of the red-teaming attempts. Next, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It constitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting, we collect a dataset that consists of 1.9K harmful questions covering a wide range of topics, 9.5K safe and 7.3K harmful conversations from ChatGPT; 2) SAFE-ALIGN: We demonstrate how the conversational dataset can be used for the safety alignment of LLMs by minimizing the negative log-likelihood over helpful responses and penalizing over harmful responses by gradient accent over sample loss. Our model STARLING, a fine-tuned Vicuna-7B, is observed to be more safely aligned when evaluated on RED-EVAL and HHH benchmarks while preserving the utility of the baseline models (TruthfulQA, MMLU, and BBH).",,http://arxiv.org/abs/2308.09662v3,arxiv,,,,,,,,,pdf_cache/pdfs/613ba7f47e63dfca.pdf,cached,2308.096623,,http://arxiv.org/pdf/2308.09662v3,cs.CL,089fce9db8f765c6f8a15450f77005f3967f1acaf52704dacff42f736fa724b2,,,,,,en,tech-report,digital,yes,no,GPT-4,player,"Attack success rate (ASR) of red-teaming attempts, refusal rate, safety and utility evaluations on benchmarks like RED-EVAL, HHH, TruthfulQA, MMLU, and BIG-bench.",harmful outputs|jailbreak,5,https://github.com/declare-lab/red-instruct,yes,success,1.0,jailbreak,gpt4,,,mentioned
SCREEN_0090,AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models,Sicheng Zhu; Ruiyi Zhang; Bang An; Gang Wu; Joe Barrow; Zichao Wang; Furong Huang; Ani Nenkova; Tong Sun,2023,,"Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent studies suggest that defending against these attacks is possible: adversarial attacks generate unlimited but unreadable gibberish prompts, detectable by perplexity-based filters; manual jailbreak attacks craft readable prompts, but their limited number due to the necessity of human creativity allows for easy blocking. In this paper, we show that these solutions may be too optimistic. We introduce AutoDAN, an interpretable, gradient-based adversarial attack that merges the strengths of both attack types. Guided by the dual goals of jailbreak and readability, AutoDAN optimizes and generates tokens one by one from left to right, resulting in readable prompts that bypass perplexity filters while maintaining high attack success rates. Notably, these prompts, generated from scratch using gradients, are interpretable and diverse, with emerging strategies commonly seen in manual jailbreak attacks. They also generalize to unforeseen harmful behaviors and transfer to black-box LLMs better than their unreadable counterparts when using limited training data or a single proxy model. Furthermore, we show the versatility of AutoDAN by automatically leaking system prompts using a customized objective. Our work offers a new way to red-team LLMs and understand jailbreak mechanisms via interpretability.",,http://arxiv.org/abs/2310.15140v2,arxiv,,,,,,,,,pdf_cache/pdfs/ab294f7189aa865a.pdf,cached,2310.151402,,http://arxiv.org/pdf/2310.15140v2,cs.CR; cs.AI; cs.CL; cs.LG,2563e630cfe7678ed25e25546ce3c916600af2337bd560381711c18739ec848d,,,,,,en,tech-report,none,no,no,"GPT-3.5, GPT-4",none,"Attack success rate (ASR), perplexity",jailbreak,5,https://autodan-jailbreak.github.io/,yes,success,1.0,jailbreak,,,,
SCREEN_0091,Attack Prompt Generation for Red Teaming and Defending Large Language Models,Boyi Deng; Wenjie Wang; Fuli Feng; Yang Deng; Qifan Wang; Xiangnan He,2023,,"Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content. Previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. To address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. Specifically, considering the impressive capabilities of newly emerged LLMs, we propose an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning. Furthermore, we propose a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework to enhance their safety against red teaming attacks. Extensive experiments on different LLMs validate the effectiveness of our proposed attack and defense frameworks. Additionally, we release a series of attack prompts datasets named SAP with varying sizes, facilitating the safety evaluation and enhancement of more LLMs. Our code and dataset is available on https://github.com/Aatrox103/SAP .",,http://arxiv.org/abs/2310.12505v1,arxiv,,,,,,,,,pdf_cache/pdfs/bf37677e2e57c4f4.pdf,cached,2310.125051,,http://arxiv.org/pdf/2310.12505v1,cs.CL; cs.CR; cs.LG,c168dedd30b9a0b32ba0eb3ec259ea8c8c4ea404df8dc639a90b0df767fdf1e1,,,,,,en,tech-report,digital,no,no,GPT-3.5,generator,Harmful scores assigned by GPT-3.5 and Perspective API; ROC curve analysis for evaluation,,6,https://github.com/Aatrox103/SAP,yes,success,1.0,,gpt35,,,github.com/Aatrox103/SAP
SCREEN_0092,Large Language Model Unlearning,Yuanshun Yao; Xiaojun Xu; Yang Liu,2023,,"We study how to perform unlearning, i.e. forgetting undesirable misbehaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) reducing hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited resources, and therefore the priority is to stop generating undesirable outputs rather than to try to generate desirable outputs, unlearning is particularly appealing. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time.",,http://arxiv.org/abs/2310.10683v2,arxiv,,,,,,,,,pdf_cache/pdfs/6f6e14dd07140a54.pdf,cached,2310.106832,,http://arxiv.org/pdf/2310.10683v2,cs.CL; cs.AI; cs.LG,a071371edb9a2ca10a06944eb750b6908ac4ef025e0520cccab9aa9eb5a6a4da,,,,,,en,tech-report,none,no,no,none,none,none,hallucination,6,https://github.com/kevinyaobytedance/llm_unlearn,yes,success,1.0,hallucination,,,,
SCREEN_0093,Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation,Yangsibo Huang; Samyak Gupta; Mengzhou Xia; Kai Li; Danqi Chen,2023,,"The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as ""jailbreaks"". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from 0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\times$ lower computational cost. Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models. Our code is available at https://github.com/Princeton-SysML/Jailbreak_LLM.",,http://arxiv.org/abs/2310.06987v1,arxiv,,,,,,,,,pdf_cache/pdfs/389997c612626557.pdf,cached,2310.069871,,http://arxiv.org/pdf/2310.06987v1,cs.CL; cs.AI; cs.CR,488f740b63e5689faea9cf00186a4c7e68f0b94ba728378f147073552598e12b,,,,,,en,tech-report,digital,yes,no,"LLaMA2, Vicuna, Falcon, MPT",player,"Misalignment rate, human evaluation agreement, classifier accuracy",adversarial prompt vulnerability|alignment failure|deception|decoding strategy exploitation|jailbreak,6,https://github.com/Princeton-SysML/Jailbreak_LLM,yes,success,1.0,deception|jailbreak,,,accuracy|human_evaluation,github.com/Princeton-SysML/Jailbreak_LLM
SCREEN_0094,War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars,Wenyue Hua; Lizhou Fan; Lingyao Li; Kai Mei; Jianchao Ji; Yingqiang Ge; Libby Hemphill; Yongfeng Zhang,2023,,"Can we avoid wars at the crossroads of history? This question has been pursued by individuals, scholars, policymakers, and organizations throughout human history. In this research, we attempt to answer the question based on the recent advances of Artificial Intelligence (AI) and Large Language Models (LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to simulate the participating countries, their decisions, and the consequences, in historical international conflicts, including the World War I (WWI), the World War II (WWII), and the Warring States Period (WSP) in Ancient China. By evaluating the simulation effectiveness, we examine the advancements and limitations of cutting-edge AI systems' abilities in studying complex collective human behaviors such as international conflicts under diverse settings. In these simulations, the emergent interactions among agents also offer a novel perspective for examining the triggers and conditions that lead to war. Our findings offer data-driven and AI-augmented insights that can redefine how we approach conflict resolution and peacekeeping strategies. The implications stretch beyond historical analysis, offering a blueprint for using AI to understand human history and possibly prevent future international conflicts. Code and data are available at \url{https://github.com/agiresearch/WarAgent}.",,http://arxiv.org/abs/2311.17227v2,arxiv,,,,,,,,,pdf_cache/pdfs/f365f4224c977fcc.pdf,cached,2311.172272,,http://arxiv.org/pdf/2311.17227v2,cs.AI; cs.CL; cs.CY,d162a42c22c90495694bebf3402a782ce41acf845e52ca87dada44dc575f8dc2,,,,,,en,tech-report,digital,yes,yes,GPT-4,player,"Simulation effectiveness compared to historical events, analysis of triggers for war, and exploration of historical inevitabilities.",,4,https://github.com/agiresearch/WarAgent,yes,success,1.0,,,,,github.com/agiresearch/WarAgent
SCREEN_0095,RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models,Jiongxiao Wang; Junlin Wu; Muhao Chen; Yevgeniy Vorobeychik; Chaowei Xiao,2023,,"Reinforcement Learning with Human Feedback (RLHF) is a methodology designed to align Large Language Models (LLMs) with human preferences, playing an important role in LLMs alignment. Despite its advantages, RLHF relies on human annotators to rank the text, which can introduce potential security vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the ranking score by up-ranking any malicious text to steer the LLM adversarially. To assess the red-teaming of RLHF against human preference data poisoning, we propose RankPoison, a poisoning attack method on candidates' selection of preference rank flipping to reach certain malicious behaviors (e.g., generating longer sequences, which can increase the computational cost). With poisoned dataset generated by RankPoison, we can perform poisoning attacks on LLMs to generate longer tokens without hurting the original safety alignment performance. Moreover, applying RankPoison, we also successfully implement a backdoor attack where LLMs can generate longer answers under questions with the trigger word. Our findings highlight critical security challenges in RLHF, underscoring the necessity for more robust alignment methods for LLMs.",,http://arxiv.org/abs/2311.09641v2,arxiv,,,,,,,,,pdf_cache/pdfs/9588c11f1d004227.pdf,cached,2311.096412,,http://arxiv.org/pdf/2311.09641v2,cs.AI; cs.CL; cs.CR; cs.HC,291aaa2658a30e423de018686c9a5bf3b31dfcb18396a3c760f1f6cf43ca33d5,,,,,,en,tech-report,digital,no,yes,GPT-4,player,"RM Length Acc, RM Safety Acc, Avg Answer Length, Clean Reward Score, Harmfulness Ratio, Helpfulness Evaluation, LLM-as-a-Judge",backdoor attacks|deception|reward hacking,6,https://github.com/PKU-Alignment/safe-rlhf,yes,success,1.0,deception,,,,
SCREEN_0096,Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections,Yuanpu Cao; Bochuan Cao; Jinghui Chen,2023,,"Recent developments in Large Language Models (LLMs) have manifested significant advancements. To facilitate safeguards against malicious exploitation, a body of research has concentrated on aligning LLMs with human preferences and inhibiting their generation of inappropriate content. Unfortunately, such alignments are often vulnerable: fine-tuning with a minimal amount of harmful data can easily unalign the target LLM. While being effective, such fine-tuning-based unalignment approaches also have their own limitations: (1) non-stealthiness, after fine-tuning, safety audits or red-teaming can easily expose the potential weaknesses of the unaligned models, thereby precluding their release/use. (2) non-persistence, the unaligned LLMs can be easily repaired through re-alignment, i.e., fine-tuning again with aligned data points. In this work, we show that it is possible to conduct stealthy and persistent unalignment on large language models via backdoor injections. We also provide a novel understanding on the relationship between the backdoor persistence and the activation pattern and further provide guidelines for potential trigger design. Through extensive experiments, we demonstrate that our proposed stealthy and persistent unalignment can successfully pass the safety evaluation while maintaining strong persistence against re-alignment defense.",,http://arxiv.org/abs/2312.00027v2,arxiv,,,,,,,,,pdf_cache/pdfs/178892e1a937f364.pdf,cached,2312.000272,,http://arxiv.org/pdf/2312.00027v2,cs.CR; cs.AI; cs.CL,30ff300c04b48d1dbf095c22fea6e21b1abe2c8b6ac6aec36abbb8565e860fc4,,,,,,en,tech-report,digital,no,yes,GPT-3.5-Turbo,player,"Attack success rate (ASR) to evaluate the effectiveness of unalignment approaches, automated evaluation with GPT-4 as judge.",,6,https://github.com/CaoYuanpu/BackdoorUnalign,yes,success,1.0,,gpt4,,,
SCREEN_0097,Trojan Activation Attack: Red-Teaming Large Language Models using Activation Steering for Safety-Alignment,Haoran Wang; Kai Shu,2023,,"To ensure AI safety, instruction-tuned Large Language Models (LLMs) are specifically trained to ensure alignment, which refers to making models behave in accordance with human intentions. While these models have demonstrated commendable results on various safety benchmarks, the vulnerability of their safety alignment has not been extensively studied. This is particularly troubling given the potential harm that LLMs can inflict. Existing attack methods on LLMs often rely on poisoned training data or the injection of malicious prompts. These approaches compromise the stealthiness and generalizability of the attacks, making them susceptible to detection. Additionally, these models often demand substantial computational resources for implementation, making them less practical for real-world applications. In this work, we study a different attack scenario, called Trojan Activation Attack (TA^2), which injects trojan steering vectors into the activation layers of LLMs. These malicious steering vectors can be triggered at inference time to steer the models toward attacker-desired behaviors by manipulating their activations. Our experiment results on four primary alignment tasks show that TA^2 is highly effective and adds little or no overhead to attack efficiency. Additionally, we discuss potential countermeasures against such activation attacks.",,http://arxiv.org/abs/2311.09433v3,arxiv,,,,,,,,,pdf_cache/pdfs/fe327896764608fb.pdf,cached,2311.094333,,http://arxiv.org/pdf/2311.09433v3,cs.CR; cs.AI; cs.CL,1144af35f4c2c95b48d7973fd858bf334595644236a4c9839cc21a14718638b7,,,,,,en,conference,digital,no,no,Llama,player,Effectiveness and efficiency of activation attacks on four alignment tasks.,deception,6,none,no,success,1.0,deception,,,,
SCREEN_0098,Towards Publicly Accountable Frontier LLMs: Building an External Scrutiny Ecosystem under the ASPIRE Framework,Markus Anderljung; Everett Thornton Smith; Joe O'Brien; Lisa Soder; Benjamin Bucknall; Emma Bluemke; Jonas Schuett; Robert Trager; Lacey Strahm; Rumman Chowdhury,2023,,"With the increasing integration of frontier large language models (LLMs) into society and the economy, decisions related to their training, deployment, and use have far-reaching implications. These decisions should not be left solely in the hands of frontier LLM developers. LLM users, civil society and policymakers need trustworthy sources of information to steer such decisions for the better. Involving outside actors in the evaluation of these systems - what we term 'external scrutiny' - via red-teaming, auditing, and external researcher access, offers a solution. Though there are encouraging signs of increasing external scrutiny of frontier LLMs, its success is not assured. In this paper, we survey six requirements for effective external scrutiny of frontier AI systems and organize them under the ASPIRE framework: Access, Searching attitude, Proportionality to the risks, Independence, Resources, and Expertise. We then illustrate how external scrutiny might function throughout the AI lifecycle and offer recommendations to policymakers.",,http://arxiv.org/abs/2311.14711v1,arxiv,,,,,,,,,pdf_cache/pdfs/790b023a9da24a5c.pdf,cached,2311.147111,,http://arxiv.org/pdf/2311.14711v1,cs.CY; cs.AI; I.2.0,e7fec8a7fff61704786212f43f4235b2dabb069e354f6c14508b70bb4614bc93,,,,,,en,workshop,none,no,no,none,none,none,,6,none,no,success,1.0,,,,,
SCREEN_0099,AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications,Bhaktipriya Radharapu; Kevin Robinson; Lora Aroyo; Preethi Lahoti,2023,,"Adversarial testing of large language models (LLMs) is crucial for their safe and responsible deployment. We introduce a novel approach for automated generation of adversarial evaluation datasets to test the safety of LLM generations on new downstream applications. We call it AI-assisted Red-Teaming (AART) - an automated alternative to current manual red-teaming efforts. AART offers a data generation and augmentation pipeline of reusable and customizable recipes that reduce human effort significantly and enable integration of adversarial testing earlier in new product development. AART generates evaluation datasets with high diversity of content characteristics critical for effective adversarial testing (e.g. sensitive and harmful concepts, specific to a wide range of cultural and geographic regions and application scenarios). The data generation is steered by AI-assisted recipes to define, scope and prioritize diversity within the application context. This feeds into a structured LLM-generation process that scales up evaluation priorities. Compared to some state-of-the-art tools, AART shows promising results in terms of concept coverage and data quality.",,http://arxiv.org/abs/2311.08592v2,arxiv,,,,,,,,,pdf_cache/pdfs/aa93aac45e5c5e33.pdf,cached,2311.085922,,http://arxiv.org/pdf/2311.08592v2,cs.SE; cs.AI; cs.CL,b4f11fa3be33520db88c59229dea9017e993aa0f042fe47d594980fc37321394,,,,,,en,tech-report,digital,no,yes,none,generator,"Concept coverage, data quality, and comparison against human red-teaming datasets.",,6,https://github.com/google-research-datasets/aart-ai-safety-dataset,yes,success,1.0,,,,,
SCREEN_0100,MART: Improving LLM Safety with Multi-round Automatic Red-Teaming,Suyu Ge; Chunting Zhou; Rui Hou; Madian Khabsa; Yi-Chia Wang; Qifan Wang; Jiawei Han; Yuning Mao,2023,,"Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses. While effective, manual red-teaming is costly, and existing automatic red-teaming typically discovers safety risks without addressing them. In this paper, we propose a Multi-round Automatic Red-Teaming (MART) method, which incorporates both automatic adversarial prompt writing and safe response generation, significantly increasing red-teaming scalability and the safety of the target LLM. Specifically, an adversarial LLM and a target LLM interplay with each other in an iterative manner, where the adversarial LLM aims to generate challenging prompts that elicit unsafe responses from the target LLM, while the target LLM is fine-tuned with safety aligned data on these adversarial prompts. In each round, the adversarial LLM crafts better attacks on the updated target LLM, while the target LLM also improves itself through safety fine-tuning. On adversarial prompt benchmarks, the violation rate of an LLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART, achieving comparable performance to LLMs with extensive adversarial prompt writing. Notably, model helpfulness on non-adversarial prompts remains stable throughout iterations, indicating the target LLM maintains strong performance on instruction following.",,http://arxiv.org/abs/2311.07689v1,arxiv,,,,,,,,,pdf_cache/pdfs/c6731738bd4a0877.pdf,cached,2311.076891,,http://arxiv.org/pdf/2311.07689v1,cs.CL,ccb4158b60bd3c0d5ee9963cfa610dd27f3a1bcf41758a37d29bd09058c151df,,,,,,en,tech-report,digital,no,yes,LLaMA,player,"Violation rate reduction, safety and helpfulness RM scores",,6,none,yes,success,1.0,,,,,
SCREEN_0101,Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming,Nanna Inie; Jonathan Stray; Leon Derczynski,2023,PLoS 2025,"Engaging in the deliberate generation of abnormal outputs from Large Language Models (LLMs) by attacking them is a novel human activity. This paper presents a thorough exposition of how and why people perform such attacks, defining LLM red-teaming based on extensive and diverse evidence. Using a formal qualitative methodology, we interviewed dozens of practitioners from a broad range of backgrounds, all contributors to this novel work of attempting to cause LLMs to fail. We focused on the research questions of defining LLM red teaming, uncovering the motivations and goals for performing the activity, and characterizing the strategies people use when attacking LLMs. Based on the data, LLM red teaming is defined as a limit-seeking, non-malicious, manual activity, which depends highly on a team-effort and an alchemist mindset. It is highly intrinsically motivated by curiosity, fun, and to some degrees by concerns for various harms of deploying LLMs. We identify a taxonomy of 12 strategies and 35 different techniques of attacking LLMs. These findings are presented as a comprehensive grounded theory of how and why people attack large language models: LLM red teaming.",,http://arxiv.org/abs/2311.06237v3,arxiv,,,,,,,,,pdf_cache/pdfs/66cc4235726e4f95.pdf,cached,2311.062373,,http://arxiv.org/pdf/2311.06237v3,cs.CL; cs.CR; cs.HC,2a741e34e1d2c2872d5da3cdc9827a2d5fe8e9786b2b5aa7aae4a937ca5644f6,,,,,,en,journal,digital,no,no,ChatGPT,player,"The study uses qualitative interviews to define LLM red teaming, motivations, and strategies.",,4,none,no,success,1.0,,,,,
SCREEN_0102,ALYMPICS: LLM Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents,Shaoguang Mao; Yuzhe Cai; Yan Xia; Wenshan Wu; Xun Wang; Fengyi Wang; Tao Ge; Furu Wei,2023,,"This paper introduces Alympics (Olympics for Agents), a systematic simulation framework utilizing Large Language Model (LLM) agents for game theory research. Alympics creates a versatile platform for studying complex game theory problems, bridging the gap between theoretical game theory and empirical investigations by providing a controlled environment for simulating human-like strategic interactions with LLM agents. In our pilot case study, the ""Water Allocation Challenge,"" we explore Alympics through a challenging strategic game focused on the multi-round auction on scarce survival resources. This study demonstrates the framework's ability to qualitatively and quantitatively analyze game determinants, strategies, and outcomes. Additionally, we conduct a comprehensive human assessment and an in-depth evaluation of LLM agents in strategic decision-making scenarios. Our findings not only expand the understanding of LLM agents' proficiency in emulating human strategic behavior but also highlight their potential in advancing game theory knowledge, thereby enriching our understanding of both game theory and empowering further research into strategic decision-making domains with LLM agents. Codes, prompts, and all related resources are available at https://github.com/microsoft/Alympics.",,http://arxiv.org/abs/2311.03220v4,arxiv,,,,,,,,,pdf_cache/pdfs/3435475bcb4d5e9d.pdf,cached,2311.032204,,http://arxiv.org/pdf/2311.03220v4,cs.CL; cs.AI; cs.GT,94613a4e606ab50050090d0ea0714f1747a4d999b7212cb0601717948e0ea63c,,,,,,en,tech-report,digital,no,yes,not specified,player,"Information utilization, logical reasoning, strategic effectiveness, adaptability and strategic evolution, long-term planning, identity alignment",,6,https://github.com/microsoft/Alympics,yes,success,1.0,,,,human_evaluation,github.com/microsoft/Alympics
SCREEN_0103,Red Teaming for Large Language Models At Scale: Tackling Hallucinations on Mathematics Tasks,Aleksander Buszydlik; Karol Dobiczek; Michał Teodor Okoń; Konrad Skublicki; Philip Lippmann; Jie Yang,2023,,"We consider the problem of red teaming LLMs on elementary calculations and algebraic tasks to evaluate how various prompting techniques affect the quality of outputs. We present a framework to procedurally generate numerical questions and puzzles, and compare the results with and without the application of several red teaming techniques. Our findings suggest that even though structured reasoning and providing worked-out examples slow down the deterioration of the quality of answers, the gpt-3.5-turbo and gpt-4 models are not well suited for elementary calculations and reasoning tasks, also when being red teamed.",,http://arxiv.org/abs/2401.00290v1,arxiv,,,,,,,,,pdf_cache/pdfs/6ee8b5c06bc00437.pdf,cached,2401.002901,,http://arxiv.org/pdf/2401.00290v1,cs.CL; cs.AI; I.2.7,695134e535b8adeb86204bf0a000257ab43579ee9bc7b0591defcd11f8413a3c,,,,,,en,tech-report,digital,no,yes,GPT-4,player,"Accuracy, edit distance, relative edit distance, relative numerical distance",hallucination,6,https://github.com/RedTeamingforLLMs/RedTeamingforLLMs,yes,success,1.0,hallucination,gpt4|gpt35,,accuracy,
SCREEN_0104,Privacy Issues in Large Language Models: A Survey,Seth Neel; Peter Chang,2023,,"This is the first survey of the active area of AI research that focuses on privacy issues in Large Language Models (LLMs). Specifically, we focus on work that red-teams models to highlight privacy risks, attempts to build privacy into the training or inference process, enables efficient data deletion from trained models to comply with existing privacy regulations, and tries to mitigate copyright issues. Our focus is on summarizing technical research that develops algorithms, proves theorems, and runs empirical evaluations. While there is an extensive body of legal and policy work addressing these challenges from a different angle, that is not the focus of our survey. Nevertheless, these works, along with recent legal developments do inform how these technical problems are formalized, and so we discuss them briefly in Section 1. While we have made our best effort to include all the relevant work, due to the fast moving nature of this research we may have missed some recent work. If we have missed some of your work please contact us, as we will attempt to keep this survey relatively up to date. We are maintaining a repository with the list of papers covered in this survey and any relevant code that was publicly available at https://github.com/safr-ml-lab/survey-llm.",,http://arxiv.org/abs/2312.06717v4,arxiv,,,,,,,,,pdf_cache/pdfs/c61508d6df91ec51.pdf,cached,2312.067174,,http://arxiv.org/pdf/2312.06717v4,cs.AI,a5b649b7e95027740a1697ce7083804ca5c2a5cf8091feb4c0fd22b410fd89d8,,,,,,en,tech-report,,no,no,,,,,6,https://github.com/safr-ml-lab/survey-llm,yes,success,0.7699999999999999,,,,,github.com/safr-ml-lab/survey-llm
SCREEN_0105,DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions,Fangzhou Wu; Xiaogeng Liu; Chaowei Xiao,2023,,"With the advancement of Large Language Models (LLMs), significant progress has been made in code generation, enabling LLMs to transform natural language into programming code. These Code LLMs have been widely accepted by massive users and organizations. However, a dangerous nature is hidden in the code, which is the existence of fatal vulnerabilities. While some LLM providers have attempted to address these issues by aligning with human guidance, these efforts fall short of making Code LLMs practical and robust. Without a deep understanding of the performance of the LLMs under the practical worst cases, it would be concerning to apply them to various real-world applications. In this paper, we answer the critical issue: Are existing Code LLMs immune to generating vulnerable code? If not, what is the possible maximum severity of this issue in practical deployment scenarios? In this paper, we introduce DeceptPrompt, a novel algorithm that can generate adversarial natural language instructions that drive the Code LLMs to generate functionality correct code with vulnerabilities. DeceptPrompt is achieved through a systematic evolution-based algorithm with a fine grain loss design. The unique advantage of DeceptPrompt enables us to find natural prefix/suffix with totally benign and non-directional semantic meaning, meanwhile, having great power in inducing the Code LLMs to generate vulnerable code. This feature can enable us to conduct the almost-worstcase red-teaming on these LLMs in a real scenario, where users are using natural language. Our extensive experiments and analyses on DeceptPrompt not only validate the effectiveness of our approach but also shed light on the huge weakness of LLMs in the code generation task. When applying the optimized prefix/suffix, the attack success rate (ASR) will improve by average 50% compared with no prefix/suffix applying.",,http://arxiv.org/abs/2312.04730v2,arxiv,,,,,,,,,pdf_cache/pdfs/9e5a8635bf8ad973.pdf,cached,2312.047302,,http://arxiv.org/pdf/2312.04730v2,cs.CR; cs.AI,ae9cda0e43aa456675ea2e75e03aea1a73aaacb8e8ad8c0cdd6638758be73255,,,,,,en,tech-report,digital,no,yes,GPT-4,generator,Attack success rate (ASR) improvement by average 50% compared with no prefix/suffix applying.,deception,6,none,yes,success,1.0,deception,,,,
SCREEN_0106,"Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models",Rima Hazra; Sayan Layek; Somnath Banerjee; Soujanya Poria,2024,,"In the rapidly advancing field of artificial intelligence, the concept of Red-Teaming or Jailbreaking large language models (LLMs) has emerged as a crucial area of study. This approach is especially significant in terms of assessing and enhancing the safety and robustness of these models. This paper investigates the intricate consequences of such modifications through model editing, uncovering a complex relationship between enhancing model accuracy and preserving its ethical integrity. Our in-depth analysis reveals a striking paradox: while injecting accurate information is crucial for model reliability, it can paradoxically destabilize the model's foundational framework, resulting in unpredictable and potentially unsafe behaviors. Additionally, we propose a benchmark dataset NicheHazardQA to investigate this unsafe behavior both within the same and cross topical domain. This aspect of our research sheds light on how the edits, impact the model's safety metrics and guardrails. Our findings show that model editing serves as a cost-effective tool for topical red-teaming by methodically applying targeted edits and evaluating the resultant model behavior.",,http://arxiv.org/abs/2401.10647v5,arxiv,,,,,,,,,pdf_cache/pdfs/57089416970c68f0.pdf,cached,2401.106475,,http://arxiv.org/pdf/2401.10647v5,cs.CL,2cc904da3fc19450f195d4780ca6ea93ca0d2bb8fbf719663dbdb9a22239d190,,,,,,en,tech-report,digital,no,no,Llama,analyst,"Consistency, specificity, ethical response generation, and performance on benchmark datasets.",Knowledge Conflict|Knowledge Distortion|Unethical Response Generation|jailbreak,6,none,yes,success,1.0,jailbreak,,,accuracy,
SCREEN_0107,Using Left and Right Brains Together: Towards Vision and Language Planning,Jun Cen; Chenfei Wu; Xiao Liu; Shengming Yin; Yixuan Pei; Jinglong Yang; Qifeng Chen; Nan Duan; Jianguo Zhang,2024,,"Large Language Models (LLMs) and Large Multi-modality Models (LMMs) have demonstrated remarkable decision masking capabilities on a variety of tasks. However, they inherently operate planning within the language space, lacking the vision and spatial imagination ability. In contrast, humans utilize both left and right hemispheres of the brain for language and visual planning during the thinking process. Therefore, we introduce a novel vision-language planning framework in this work to perform concurrent visual and language planning for tasks with inputs of any form. Our framework incorporates visual planning to capture intricate environmental details, while language planning enhances the logical coherence of the overall system. We evaluate the effectiveness of our framework across vision-language tasks, vision-only tasks, and language-only tasks. The results demonstrate the superior performance of our approach, indicating that the integration of visual and language planning yields better contextually aware task execution.",,http://arxiv.org/abs/2402.10534v1,arxiv,,,,,,,,,pdf_cache/pdfs/be1d4892e62a9177.pdf,cached,2402.105341,,http://arxiv.org/pdf/2402.10534v1,cs.CV,54b5f1acfb293afc53e386ed356f6684443bbfa254706b72b2ce228a48649d92,,,,,,en,tech-report,digital,yes,no,ChatGPT,generator,"Performance across vision-language tasks, vision-only tasks, and language-only tasks",hallucination|misalignment|overconfidence,4,none,yes,success,1.0,,,,,
SCREEN_0108,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Mantas Mazeika; Long Phan; Xuwang Yin; Andy Zou; Zifan Wang; Norman Mu; Elham Sakhaee; Nathaniel Li; Steven Basart; Bo Li; David Forsyth; Dan Hendrycks,2024,,"Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.",,http://arxiv.org/abs/2402.04249v2,arxiv,,,,,,,,,pdf_cache/pdfs/072ac9e0ae07cdb5.pdf,cached,2402.042492,,http://arxiv.org/pdf/2402.04249v2,cs.LG; cs.AI; cs.CL; cs.CV,e6cbcbedbef6aebea3b6fc6c8fd7933546b26cf0a1e65bcf128eea235007c732,,,,,,en,tech-report,digital,no,yes,GPT-4,analyst,"Attack success rate (ASR) on target models, breadth of harmful behaviors, comparability, and robust metrics.",deception|eroded epistemics|malicious use|power-seeking behavior,6,https://github.com/centerforaisafety/HarmBench,yes,success,1.0,,,,,github.com/centerforaisafety/HarmBench
SCREEN_0109,Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo,Stephen Zhao; Rob Brekelmans; Alireza Makhzani; Roger Grosse,2024,,"Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks.",,http://arxiv.org/abs/2404.17546v1,arxiv,,,,,,,,,pdf_cache/pdfs/7a29f0584981a2b4.pdf,cached,2404.175461,,http://arxiv.org/pdf/2404.17546v1,cs.LG; cs.AI; cs.CL; stat.ML,71f7ff8d3e2890f38ca95f40518690342838e29f38359cffe8298880d91ae089,,,,,,en,tech-report,digital,no,yes,GPT-2,generator,"KL divergence between inference and target distributions, log partition function estimates",prompt_sensitivity,6,none,yes,success,1.0,prompt_sensitivity,,,accuracy,
SCREEN_0110,Virtual Context: Enhancing Jailbreak Attacks with Special Token Injection,Yuqi Zhou; Lin Lu; Hanchi Sun; Pan Zhou; Lichao Sun,2024,,"Jailbreak attacks on large language models (LLMs) involve inducing these models to generate harmful content that violates ethics or laws, posing a significant threat to LLM security. Current jailbreak attacks face two main challenges: low success rates due to defensive measures and high resource requirements for crafting specific prompts. This paper introduces Virtual Context, which leverages special tokens, previously overlooked in LLM security, to improve jailbreak attacks. Virtual Context addresses these challenges by significantly increasing the success rates of existing jailbreak methods and requiring minimal background knowledge about the target model, thus enhancing effectiveness in black-box settings without additional overhead. Comprehensive evaluations show that Virtual Context-assisted jailbreak attacks can improve the success rates of four widely used jailbreak methods by approximately 40% across various LLMs. Additionally, applying Virtual Context to original malicious behaviors still achieves a notable jailbreak effect. In summary, our research highlights the potential of special tokens in jailbreak attacks and recommends including this threat in red-teaming testing to comprehensively enhance LLM security.",,http://arxiv.org/abs/2406.19845v2,arxiv,,,,,,,,,pdf_cache/pdfs/f614ea408b6622cb.pdf,cached,2406.198452,,http://arxiv.org/pdf/2406.19845v2,cs.CR,0f4832c285823cc36bb55e28a6c4740fcd9d1603b006b95cd962bb1e520b2fb3,,,,,,en,tech-report,digital,no,yes,Llama-2,player,"Response Prefix Matching, Attack Success Rate (ASR), Harm Score (HS)",jailbreak,6,none,yes,success,1.0,jailbreak,,,,
SCREEN_0111,PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing,Blazej Manczak; Eliott Zemour; Eric Lin; Vaikkunth Mugunthan,2024,,"Deploying language models (LMs) necessitates outputs to be both high-quality and compliant with safety guidelines. Although Inference-Time Guardrails (ITG) offer solutions that shift model output distributions towards compliance, we find that current methods struggle in balancing safety with helpfulness. ITG Methods that safely address non-compliant queries exhibit lower helpfulness while those that prioritize helpfulness compromise on safety. We refer to this trade-off as the guardrail tax, analogous to the alignment tax. To address this, we propose PrimeGuard, a novel ITG method that utilizes structured control flow. PrimeGuard routes requests to different self-instantiations of the LM with varying instructions, leveraging its inherent instruction-following capabilities and in-context learning. Our tuning-free approach dynamically compiles system-designer guidelines for each query. We construct and release safe-eval, a diverse red-team safety benchmark. Extensive evaluations demonstrate that PrimeGuard, without fine-tuning, overcomes the guardrail tax by (1) significantly increasing resistance to iterative jailbreak attacks and (2) achieving state-of-the-art results in safety guardrailing while (3) matching helpfulness scores of alignment-tuned models. Extensive evaluations demonstrate that PrimeGuard, without fine-tuning, outperforms all competing baselines and overcomes the guardrail tax by improving the fraction of safe responses from 61% to 97% and increasing average helpfulness scores from 4.17 to 4.29 on the largest models, while reducing attack success rate from 100% to 8%. PrimeGuard implementation is available at https://github.com/dynamofl/PrimeGuard and safe-eval dataset is available at https://huggingface.co/datasets/dynamoai/safe_eval.",,http://arxiv.org/abs/2407.16318v1,arxiv,,,,,,,,,pdf_cache/pdfs/f624d6ae656e1e04.pdf,cached,2407.163181,,http://arxiv.org/pdf/2407.16318v1,cs.AI; cs.CL; cs.CR; cs.SE,d73585c574ef73f0d571df0d008f81999723ee149f350de68329d3a7105412b2,,,,,,en,conference,digital,no,yes,GPT-4,analyst,"Safety and usefulness are evaluated using AI-as-a-judge for safety, refusals, and usefulness. Human experiments validate these results with high human-AI correlation. Helpfulness is scored on a 1-5 Likert scale.",jailbreak,6,https://github.com/dynamofl/PrimeGuard,no,success,1.0,jailbreak,,,,github.com/dynamofl/PrimeGuard
SCREEN_0112,Ferret: Faster and Effective Automated Red Teaming with Reward-Based Scoring Technique,Tej Deep Pala; Vernon Y. H. Toh; Rishabh Bhardwaj; Soujanya Poria,2024,,"In today's era, where large language models (LLMs) are integrated into numerous real-world applications, ensuring their safety and robustness is crucial for responsible AI usage. Automated red-teaming methods play a key role in this process by generating adversarial attacks to identify and mitigate potential vulnerabilities in these models. However, existing methods often struggle with slow performance, limited categorical diversity, and high resource demands. While Rainbow Teaming, a recent approach, addresses the diversity challenge by framing adversarial prompt generation as a quality-diversity search, it remains slow and requires a large fine-tuned mutator for optimal performance. To overcome these limitations, we propose Ferret, a novel approach that builds upon Rainbow Teaming by generating multiple adversarial prompt mutations per iteration and using a scoring function to rank and select the most effective adversarial prompt. We explore various scoring functions, including reward models, Llama Guard, and LLM-as-a-judge, to rank adversarial mutations based on their potential harm to improve the efficiency of the search for harmful mutations. Our results demonstrate that Ferret, utilizing a reward model as a scoring function, improves the overall attack success rate (ASR) to 95%, which is 46% higher than Rainbow Teaming. Additionally, Ferret reduces the time needed to achieve a 90% ASR by 15.2% compared to the baseline and generates adversarial prompts that are transferable i.e. effective on other LLMs of larger size. Our codes are available at https://github.com/declare-lab/ferret.",,http://arxiv.org/abs/2408.10701v1,arxiv,,,,,,,,,pdf_cache/pdfs/78b36a6751e5f988.pdf,cached,2408.107011,,http://arxiv.org/pdf/2408.10701v1,cs.CL,ad62ab5d34bbe4f0600b1f6734eb0beea99ad416b145ff197b5be97f68dc4615,,,,,,en,tech-report,digital,yes,yes,"Llama 2-chat 7B, Llama 3-Instruct-8B, Mistral-7B",generator,"Attack Success Rate (ASR), fitness score, harmfulness scoring using reward models",,6,https://github.com/declare-lab/ferret,yes,success,1.0,,llama,,,github.com/declare-lab/ferret
SCREEN_0113,Artificial Intelligence and Strategic Decision-Making: Evidence from Entrepreneurs and Investors,Felipe A. Csaszar; Harsh Ketkar; Hyunjin Kim,2024,,"This paper explores how artificial intelligence (AI) may impact the strategic decision-making (SDM) process in firms. We illustrate how AI could augment existing SDM tools and provide empirical evidence from a leading accelerator program and a startup competition that current Large Language Models (LLMs) can generate and evaluate strategies at a level comparable to entrepreneurs and investors. We then examine implications for key cognitive processes underlying SDM -- search, representation, and aggregation. Our analysis suggests AI has the potential to enhance the speed, quality, and scale of strategic analysis, while also enabling new approaches like virtual strategy simulations. However, the ultimate impact on firm performance will depend on competitive dynamics as AI capabilities progress. We propose a framework connecting AI use in SDM to firm outcomes and discuss how AI may reshape sources of competitive advantage. We conclude by considering how AI could both support and challenge core tenets of the theory-based view of strategy. Overall, our work maps out an emerging research frontier at the intersection of AI and strategy.",,http://arxiv.org/abs/2408.08811v1,arxiv,,,,,,,,,pdf_cache/pdfs/dec96c0a8f4a4fd6.pdf,cached,2408.088111,,http://arxiv.org/pdf/2408.08811v1,econ.GN; cs.CY; q-fin.EC; K.4,08ca2eb615f4d558e3fbca83a2d746e23662e69b9cb8e8d1e44bac4388ef207a,,,,,,en,conference,digital,yes,yes,GPT-4,generator,Comparison of LLM-generated business plans with human-generated plans in terms of acceptance by a startup accelerator and evaluation by venture capital and angel investors.,,4,none,no,success,1.0,,,,,
SCREEN_0114,h4rm3l: A language for Composable Jailbreak Attack Synthesis,Moussa Koulako Bala Doumbouya; Ananjan Nandi; Gabriel Poesia; Davide Ghilardi; Anna Goldie; Federico Bianchi; Dan Jurafsky; Christopher D. Manning,2024,,"Despite their demonstrated valuable capabilities, state-of-the-art (SOTA) widely deployed large language models (LLMs) still have the potential to cause harm to society due to the ineffectiveness of their safety filters, which can be bypassed by prompt transformations called jailbreak attacks. Current approaches to LLM safety assessment, which employ datasets of templated prompts and benchmarking pipelines, fail to cover sufficiently large and diverse sets of jailbreak attacks, leading to the widespread deployment of unsafe LLMs. Recent research showed that novel jailbreak attacks could be derived by composition; however, a formal composable representation for jailbreak attacks, which, among other benefits, could enable the exploration of a large compositional space of jailbreak attacks through program synthesis methods, has not been previously proposed. We introduce h4rm3l, a novel approach that addresses this gap with a human-readable domain-specific language (DSL). Our framework comprises: (1) The h4rm3l DSL, which formally expresses jailbreak attacks as compositions of parameterized string transformation primitives. (2) A synthesizer with bandit algorithms that efficiently generates jailbreak attacks optimized for a target black box LLM. (3) The h4rm3l red-teaming software toolkit that employs the previous two components and an automated harmful LLM behavior classifier that is strongly aligned with human judgment. We demonstrate h4rm3l's efficacy by synthesizing a dataset of 2656 successful novel jailbreak attacks targeting 6 SOTA open-source and proprietary LLMs, and by benchmarking those models against a subset of these synthesized attacks. Our results show that h4rm3l's synthesized attacks are diverse and more successful than existing jailbreak attacks in literature, with success rates exceeding 90% on SOTA LLMs.",,http://arxiv.org/abs/2408.04811v4,arxiv,,,,,,,,,pdf_cache/pdfs/b785cee5d47a38e9.pdf,cached,2408.048114,,http://arxiv.org/pdf/2408.04811v4,cs.CR; cs.AI; cs.CL; cs.CY; cs.LG; 68; I.2; I.2.0; I.2.1; I.2.5; I.2.7; K.6.5; K.4.2,5848bf13194dbb0a9e350f8623ba40beba1fede0938c42e6b411afd3ac900478,,,,,,en,conference,digital,no,yes,"GPT-3.5, GPT-4o, Claude-3-Sonnet, Claude-3-Haiku, Llama-3-8B, Llama-3-70B",analyst,"Attack success rates (ASR), diversity and specificity of attacks, human-aligned ASR estimation",jailbreak,6,https://mdoumbouya.github.io/h4rm3l/,no,success,1.0,jailbreak,,,,mentioned
SCREEN_0115,Overriding Safety protections of Open-source Models,Sachin Kumar,2024,,"LLMs(Large Language Models) nowadays have widespread adoption as a tool for solving issues across various domain/tasks. These models since are susceptible to produce harmful or toxic results, inference-time adversarial attacks, therefore they do undergo safety alignment training and Red teaming for putting in safety guardrails. For using these models, usually fine-tuning is done for model alignment on the desired tasks, which can make model more aligned but also make it more susceptible to produce unsafe responses, if fine-tuned with harmful data.In this paper, we study how much of impact introduction of harmful data in fine-tuning can make, and if it can override the safety protection of those models. Conversely,it was also explored that if model is fine-tuned on safety data can make the model produce more safer responses. Further we explore if fine-tuning the model on harmful data makes it less helpful or less trustworthy because of increase in model uncertainty leading to knowledge drift. Our extensive experimental results shown that Safety protection in an open-source can be overridden, when fine-tuned with harmful data as observed by ASR increasing by 35% when compared to basemodel's ASR. Also, as observed, fine-tuning a model with harmful data made the harmful fine-tuned model highly uncertain with huge knowledge drift and less truthfulness in its responses. Furthermore, for the safe fine-tuned model, ASR decreases by 51.68% as compared to the basemodel, and Safe model also shown in minor drop in uncertainty and truthfulness as compared to basemodel. This paper's code is available at: https://github.com/techsachinkr/Overriding_Model_Safety_Protections",,http://arxiv.org/abs/2409.19476v1,arxiv,,,,,,,,,pdf_cache/pdfs/5bc482ea9de18562.pdf,cached,2409.194761,,http://arxiv.org/pdf/2409.19476v1,cs.CL; cs.CR,402bca122944494155cbc5eb4b8e6123e3780fa34ec148b42ff8cec8e2578e4a,,,,,,en,tech-report,digital,no,yes,Llama-3.1-8B,analyst,"Attack Success Rate (ASR), Perplexity, Entropy, Probability, Accuracy",knowledge drift|truthfulness issues|uncertainty,6,https://github.com/techsachinkr/Overriding_Model_Safety_Protections,yes,success,1.0,,,,accuracy,github.com/techsachinkr/Overriding_Model_Safety_Protections
SCREEN_0116,Attack Atlas: A Practitioner's Perspective on Challenges and Pitfalls in Red Teaming GenAI,Ambrish Rawat; Stefan Schoepf; Giulio Zizzo; Giandomenico Cornacchia; Muhammad Zaid Hameed; Kieran Fraser; Erik Miehling; Beat Buesser; Elizabeth M. Daly; Mark Purcell; Prasanna Sattigeri; Pin-Yu Chen; Kush R. Varshney,2024,,"As generative AI, particularly large language models (LLMs), become increasingly integrated into production applications, new attack surfaces and vulnerabilities emerge and put a focus on adversarial threats in natural language and multi-modal systems. Red-teaming has gained importance in proactively identifying weaknesses in these systems, while blue-teaming works to protect against such adversarial attacks. Despite growing academic interest in adversarial risks for generative AI, there is limited guidance tailored for practitioners to assess and mitigate these challenges in real-world environments. To address this, our contributions include: (1) a practical examination of red- and blue-teaming strategies for securing generative AI, (2) identification of key challenges and open questions in defense development and evaluation, and (3) the Attack Atlas, an intuitive framework that brings a practical approach to analyzing single-turn input attacks, placing it at the forefront for practitioners. This work aims to bridge the gap between academic insights and practical security measures for the protection of generative AI systems.",,http://arxiv.org/abs/2409.15398v1,arxiv,,,,,,,,,pdf_cache/pdfs/bbe28cf52d62e112.pdf,cached,2409.153981,,http://arxiv.org/pdf/2409.15398v1,cs.CR; cs.AI; cs.LG,3523e2444af652fd5071f47f1915234d8aab4f101d57fdc2b9a3fd9670720d69,,,,,,en,tech-report,digital,yes,no,GPT-4,player,"The paper discusses the challenges and open questions in defense development and evaluation, focusing on defense development, evaluation methods, and benchmarking of red-/blue-teaming techniques.",direct injection|indirect injection|jailbreak,5,none,yes,success,1.0,,,,,
SCREEN_0117,Exploring Straightforward Conversational Red-Teaming,George Kour; Naama Zwerdling; Marcel Zalmanovici; Ateret Anaby-Tavor; Ora Nova Fandina; Eitan Farchi,2024,,"Large language models (LLMs) are increasingly used in business dialogue systems but they pose security and ethical risks. Multi-turn conversations, where context influences the model's behavior, can be exploited to produce undesired responses. In this paper, we examine the effectiveness of utilizing off-the-shelf LLMs in straightforward red-teaming approaches, where an attacker LLM aims to elicit undesired output from a target LLM, comparing both single-turn and conversational red-teaming tactics. Our experiments offer insights into various usage strategies that significantly affect their performance as red teamers. They suggest that off-the-shelf models can act as effective red teamers and even adjust their attack strategy based on past attempts, although their effectiveness decreases with greater alignment.",,http://arxiv.org/abs/2409.04822v1,arxiv,,,,,,,,,pdf_cache/pdfs/6dc4e90ead4a12ef.pdf,cached,2409.048221,,http://arxiv.org/pdf/2409.04822v1,cs.CL; cs.AI,292ba73550af532cf2f13312703df98d5f3e3527f9b4c217746adb473b2a29a9,,,,,,en,tech-report,digital,yes,yes,"GPT-3.5T, Llama2-70b, Llama2-13b, Mixtral-8x7b",player,"Harmfulness scores ranging from 1 to 5, based on the assistant's response to unsafe content.",alignment failure|contextual misunderstanding,4,none,yes,success,1.0,,,,,
SCREEN_0118,IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves,Ruofan Wang; Juncheng Li; Yixu Wang; Bo Wang; Xiaosen Wang; Yan Teng; Yingchun Wang; Xingjun Ma; Yu-Gang Jiang,2024,,"As large Vision-Language Models (VLMs) gain prominence, ensuring their safe deployment has become critical. Recent studies have explored VLM robustness against jailbreak attacks-techniques that exploit model vulnerabilities to elicit harmful outputs. However, the limited availability of diverse multimodal data has constrained current approaches to rely heavily on adversarial or manually crafted images derived from harmful text datasets, which often lack effectiveness and diversity across different contexts. In this paper, we propose IDEATOR, a novel jailbreak method that autonomously generates malicious image-text pairs for black-box jailbreak attacks. IDEATOR is grounded in the insight that VLMs themselves could serve as powerful red team models for generating multimodal jailbreak prompts. Specifically, IDEATOR leverages a VLM to create targeted jailbreak texts and pairs them with jailbreak images generated by a state-of-the-art diffusion model. Extensive experiments demonstrate IDEATOR's high effectiveness and transferability, achieving a 94% attack success rate (ASR) in jailbreaking MiniGPT-4 with an average of only 5.34 queries, and high ASRs of 82%, 88%, and 75% when transferred to LLaVA, InstructBLIP, and Chameleon, respectively. Building on IDEATOR's strong transferability and automated process, we introduce the VLBreakBench, a safety benchmark comprising 3,654 multimodal jailbreak samples. Our benchmark results on 11 recently released VLMs reveal significant gaps in safety alignment. For instance, our challenge set achieves ASRs of 46.31% on GPT-4o and 19.65% on Claude-3.5-Sonnet, underscoring the urgent need for stronger defenses.",,http://arxiv.org/abs/2411.00827v3,arxiv,,,,,,,,,pdf_cache/pdfs/ece4a006b1a93d93.pdf,cached,2411.008273,,http://arxiv.org/pdf/2411.00827v3,cs.CV; cs.AI,319c5183af780a5fff18051f90a148db148d2f45e9f257caf4c9d60615738fbd,,,,,,en,tech-report,digital,no,yes,MiniGPT-4,player,Attack Success Rate (ASR) across various categories of harmful instructions,jailbreak,6,none,yes,success,1.0,jailbreak,claude,,,
SCREEN_0119,A Formal Framework for Assessing and Mitigating Emergent Security Risks in Generative AI Models: Bridging Theory and Dynamic Risk Mitigation,Aviral Srivastava; Sourav Panda,2024,,"As generative AI systems, including large language models (LLMs) and diffusion models, advance rapidly, their growing adoption has led to new and complex security risks often overlooked in traditional AI risk assessment frameworks. This paper introduces a novel formal framework for categorizing and mitigating these emergent security risks by integrating adaptive, real-time monitoring, and dynamic risk mitigation strategies tailored to generative models' unique vulnerabilities. We identify previously under-explored risks, including latent space exploitation, multi-modal cross-attack vectors, and feedback-loop-induced model degradation. Our framework employs a layered approach, incorporating anomaly detection, continuous red-teaming, and real-time adversarial simulation to mitigate these risks. We focus on formal verification methods to ensure model robustness and scalability in the face of evolving threats. Though theoretical, this work sets the stage for future empirical validation by establishing a detailed methodology and metrics for evaluating the performance of risk mitigation strategies in generative AI systems. This framework addresses existing gaps in AI safety, offering a comprehensive road map for future research and implementation.",,http://arxiv.org/abs/2410.13897v1,arxiv,,,,,,,,,pdf_cache/pdfs/421ddb8a0d7fe04c.pdf,cached,2410.138971,,http://arxiv.org/pdf/2410.13897v1,cs.CR; cs.LG; I.2.m,9888b0c515e74f1c222a8ef1e71055ecc6d7e0978884196b50225b0f34031ff0,,,,,,en,conference,none,no,no,none,none,"Theoretical framework with a focus on formal verification methods, dynamic risk monitoring, and continuous real-time adversarial simulations.",feedback-loop-induced model degradation|latent space exploitation|multi-modal cross-attack vectors,6,none,no,success,1.0,,,,,
SCREEN_0120,Reward Design with Language Models,Minae Kwon; Sang Michael Xie; Kalesha Bullard; Dorsa Sadigh,2023,,"Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by prompting a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of the desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user's objectives and outperform RL agents trained with reward functions learned via supervised learning",,http://arxiv.org/abs/2303.00001v1,arxiv,,,,,,,,,pdf_cache/pdfs/077f1595eff94986.pdf,cached,2303.000011,,http://arxiv.org/pdf/2303.00001v1,cs.LG; cs.AI; cs.CL,86c740087af5e86ad9b5717ce1a94d46cbaecfd6a8e02599d9ae0e66ffb5d9ce,,,,,,en,conference,matrix,no,yes,GPT-3,generator,"Consistency with user objectives, alignment with user objectives, performance compared to baseline, labeling accuracy, RL agent accuracy",,5,none,no,success,1.0,,,,accuracy,
SCREEN_0121,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models,Paul Röttger; Hannah Rose Kirk; Bertie Vidgen; Giuseppe Attanasio; Federico Bianchi; Dirk Hovy,2023,,"Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTest's creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.",,http://arxiv.org/abs/2308.01263v3,arxiv,,,,,,,,,pdf_cache/pdfs/539dab8f68511497.pdf,cached,2308.012633,,http://arxiv.org/pdf/2308.01263v3,cs.CL; cs.AI,05d8687d567ca905f96527835cb062cb4f7df834d57de4ad6fa54782e3f500d9,,,,,,en,tech-report,none,no,yes,"GPT-4, Llama-2-70b-chat-hf, Mistral-7B",analyst,"Refusal rate for safe and unsafe prompts, compliance with safe prompts, refusal of unsafe prompts",exaggerated safety|lexical overfitting,6,https://github.com/paul-rottger/exaggerated-safety,yes,success,1.0,,,,,
SCREEN_0122,Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases,Rishabh Bhardwaj; Soujanya Poria,2023,,"Red-teaming has been a widely adopted way to evaluate the harmfulness of Large Language Models (LLMs). It aims to jailbreak a model's safety behavior to make it act as a helpful agent disregarding the harmfulness of the query. Existing methods are primarily based on input text-based red-teaming such as adversarial prompts, low-resource prompts, or contextualized prompts to condition the model in a way to bypass its safe behavior. Bypassing the guardrails uncovers hidden harmful information and biases in the model that are left untreated or newly introduced by its safety training. However, prompt-based attacks fail to provide such a diagnosis owing to their low attack success rate, and applicability to specific models. In this paper, we present a new perspective on LLM safety research i.e., parametric red-teaming through Unalignment. It simply (instruction) tunes the model parameters to break model guardrails that are not deeply rooted in the model's behavior. Unalignment using as few as 100 examples can significantly bypass commonly referred to as CHATGPT, to the point where it responds with an 88% success rate to harmful queries on two safety benchmark datasets. On open-source models such as VICUNA-7B and LLAMA-2-CHAT 7B AND 13B, it shows an attack success rate of more than 91%. On bias evaluations, Unalignment exposes inherent biases in safety-aligned models such as CHATGPT and LLAMA- 2-CHAT where the model's responses are strongly biased and opinionated 64% of the time.",,http://arxiv.org/abs/2310.14303v2,arxiv,,,,,,,,,pdf_cache/pdfs/402e9a85bacdd766.pdf,cached,2310.143032,,http://arxiv.org/pdf/2310.14303v2,cs.CL,f50583c7c46d61c61e317baa348545202f87f5f0e978865f4e1bb57ad7bf8fd5,,,,,,en,tech-report,digital,no,yes,"GPT-4, VICUNA-7B, LLAMA-2-CHAT 7B, LLAMA-2-CHAT 13B",player,"Attack Success Rate (ASR), utility testing on benchmarks like TRUTHFULQA, MMLU, and HELLASWAG",bias|gaming behaviors|jailbreak|objective misspecification,6,none,yes,success,1.0,bias|jailbreak,llama,,,mentioned
SCREEN_0123,ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models,Alex Mei; Sharon Levy; William Yang Wang,2023,,"As large language models are integrated into society, robustness toward a suite of prompts is increasingly important to maintain reliability in a high-variance environment.Robustness evaluations must comprehensively encapsulate the various settings in which a user may invoke an intelligent system. This paper proposes ASSERT, Automated Safety Scenario Red Teaming, consisting of three methods -- semantically aligned augmentation, target bootstrapping, and adversarial knowledge injection. For robust safety evaluation, we apply these methods in the critical domain of AI safety to algorithmically generate a test suite of prompts covering diverse robustness settings -- semantic equivalence, related scenarios, and adversarial. We partition our prompts into four safety domains for a fine-grained analysis of how the domain affects model performance. Despite dedicated safeguards in existing state-of-the-art models, we find statistically significant performance differences of up to 11% in absolute classification accuracy among semantically related scenarios and error rates of up to 19% absolute error in zero-shot adversarial settings, raising concerns for users' physical safety.",,http://arxiv.org/abs/2310.09624v2,arxiv,,,,,,,,,pdf_cache/pdfs/58cf90e1a03447fe.pdf,cached,2310.096242,,http://arxiv.org/pdf/2310.09624v2,cs.CL; cs.AI; cs.LG,c2686934554d645b5964b5565b162ef20a8da49f5e456612040f670b41c1a84d,,,,,,en,tech-report,digital,no,yes,"GPT-3.5, GPT-4, Alpaca, Vicuna",generator,"Performance differences in classification accuracy, error rates in adversarial settings, robustness across safety domains",adversarial attacks|prompt instability|semantic misinterpretation,6,https://github.com/alexmeigz/ASSERT,yes,success,1.0,,,,accuracy,
SCREEN_0124,"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",Xiangyu Qi; Yi Zeng; Tinghao Xie; Pin-Yu Chen; Ruoxi Jia; Prateek Mittal; Peter Henderson,2023,,"Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs.",,http://arxiv.org/abs/2310.03693v1,arxiv,,,,,,,,,pdf_cache/pdfs/96f63ebe1c299d28.pdf,cached,2310.036931,,http://arxiv.org/pdf/2310.03693v1,cs.CL; cs.AI; cs.CR; cs.LG,252e77d3e7b7f340c5eedabcb4c946f05c3218d14b7638fa22b01569f8bda3d4,,,,,,en,tech-report,none,no,no,GPT-3.5 Turbo,player,Harmfulness scores across 11 categories judged by GPT-4,bias|disinformation|influence operations|jailbreak|jailbreaking|safety degradation,6,https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety,yes,success,1.0,jailbreak,gpt4|gpt35|llama,,,
SCREEN_0125,Exploiting Novel GPT-4 APIs,Kellin Pelrine; Mohammad Taufeeque; Michał Zając; Euan McLean; Adam Gleave,2023,,"Language model attacks typically assume one of two extreme threat models: full white-box access to model weights, or black-box access limited to a text generation API. However, real-world APIs are often more flexible than just text generation: these APIs expose ""gray-box"" access leading to new threat vectors. To explore this, we red-team three new functionalities exposed in the GPT-4 APIs: fine-tuning, function calling and knowledge retrieval. We find that fine-tuning a model on as few as 15 harmful examples or 100 benign examples can remove core safeguards from GPT-4, enabling a range of harmful outputs. Furthermore, we find that GPT-4 Assistants readily divulge the function call schema and can be made to execute arbitrary function calls. Finally, we find that knowledge retrieval can be hijacked by injecting instructions into retrieval documents. These vulnerabilities highlight that any additions to the functionality exposed by an API can create new vulnerabilities.",,http://arxiv.org/abs/2312.14302v2,arxiv,,,,,,,,,pdf_cache/pdfs/3b3f4655f6133143.pdf,cached,2312.143022,,http://arxiv.org/pdf/2312.14302v2,cs.CR; cs.AI; cs.CL; cs.LG; I.2.7,c4e9ed6658b0203697e0cffe9202a11292820a087e1f8324cf65fa20bfdbba91,,,,,,en,tech-report,none,no,no,GPT-4,none,none,,6,none,yes,success,1.0,,gpt4,,,
SCREEN_0126,AI Control: Improving Safety Despite Intentional Subversion,Ryan Greenblatt; Buck Shlegeris; Kshitij Sachan; Fabien Roger,2023,,"As large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. Researchers have investigated a variety of safety techniques for this purpose, e.g. using models to review the outputs of other models, or red-teaming techniques to surface subtle failure modes. However, researchers have not evaluated whether such techniques still ensure safety if the model is itself intentionally trying to subvert them. In this paper, we develop and evaluate pipelines of safety techniques (""protocols"") that are robust to intentional subversion. We investigate a scenario in which we want to solve a sequence of programming problems, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to high-quality trusted labor. We investigate protocols that aim to never submit solutions containing backdoors, which we operationalize here as logical errors that are not caught by test cases. We investigate a range of protocols and test each against strategies that the untrusted model could use to subvert them. One protocol is what we call trusted editing. This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to rate the suspiciousness of that code. If the code is below some suspiciousness threshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove parts that seem suspicious and then submits the edited code. Another protocol is untrusted monitoring. This protocol asks GPT-4 to write code, and then asks another instance of GPT-4 whether the code is backdoored, using various techniques to prevent the GPT-4 instances from colluding. These protocols improve substantially on simple baselines.",,http://arxiv.org/abs/2312.06942v5,arxiv,,,,,,,,,pdf_cache/pdfs/14e03d43d0dd8935.pdf,cached,2312.069425,,http://arxiv.org/pdf/2312.06942v5,cs.LG,999f7a92bb8c085799e68f3d88f491ad8a334f862ccf5b286e5c2766677864c0,,,,,,en,tech-report,digital,no,yes,GPT-4,player,Usefulness (proportion of correct code submissions) and Safety (probability of submitting backdoored code),,6,https://github.com/rgreenblatt/control-evaluations,yes,success,1.0,,gpt4|gpt35,,,
SCREEN_0127,Generative AI in Writing Research Papers: A New Type of Algorithmic Bias and Uncertainty in Scholarly Work,Rishab Jain; Aditya Jain,2023,,"The use of artificial intelligence (AI) in research across all disciplines is becoming ubiquitous. However, this ubiquity is largely driven by hyperspecific AI models developed during scientific studies for accomplishing a well-defined, data-dense task. These AI models introduce apparent, human-recognizable biases because they are trained with finite, specific data sets and parameters. However, the efficacy of using large language models (LLMs) -- and LLM-powered generative AI tools, such as ChatGPT -- to assist the research process is currently indeterminate. These generative AI tools, trained on general and imperceptibly large datasets along with human feedback, present challenges in identifying and addressing biases. Furthermore, these models are susceptible to goal misgeneralization, hallucinations, and adversarial attacks such as red teaming prompts -- which can be unintentionally performed by human researchers, resulting in harmful outputs. These outputs are reinforced in research -- where an increasing number of individuals have begun to use generative AI to compose manuscripts. Efforts into AI interpretability lag behind development, and the implicit variations that occur when prompting and providing context to a chatbot introduce uncertainty and irreproducibility. We thereby find that incorporating generative AI in the process of writing research manuscripts introduces a new type of context-induced algorithmic bias and has unintended side effects that are largely detrimental to academia, knowledge production, and communicating research.",,http://arxiv.org/abs/2312.10057v1,arxiv,,,,,,,,,pdf_cache/pdfs/84bbac9e4dd0face.pdf,cached,2312.100571,,http://arxiv.org/pdf/2312.10057v1,cs.CY; cs.HC; I.2.m,85df2f969d5714912e492fc0d2ce3d3511d7388b5d860bd067f0f1e7ce3cc615,,,,,,en,tech-report,none,no,no,"GPT-3, GPT-3.5, GPT-4",generator,"Systematic review of works published, analysis of biases and context-induced bias",bias|goal misgeneralization|hallucination|red teaming,6,none,yes,success,1.0,bias|hallucination,,,,
SCREEN_0128,On the Role of Intelligence and Business Wargaming in Developing Foresight,Aline Werro; Christian Nitzl; Uwe M. Borghoff,2024,"Technology Analysis & Strategic Management (2025), Routledge","Business wargaming is a central tool for developing sustaining strategies. It transfers the benefits of traditional wargaming to the business environment. However, building wargames that support the process of decision-making for strategy require respective intelligence. This paper investigates the role of intelligence in the process of developing strategic foresight. The focus is on how intelligence is developed and how it relates to business wargaming. The so-called intelligence cycle is the basis and reference of our investigation. The conceptual part of the paper combines the theoretical background from military, business as well as serious gaming. To elaborate on some of the lessons learned, we examine specific business wargames both drawn from the literature and conducted by us at the Center for Intelligence and Security Studies (CISS). It is shown that business wargaming can make a significant contribution to the transformation of data to intelligence by supporting the intelligence cycle in two crucial phases. Furthermore, it brings together business intelligence (BI) and competitive intelligence (CI) and it bridges the gap to a company's strategy by either testing or developing a new strategy. We were also able to confirm this finding based on the business wargame we conducted at a major semiconductor manufacturer.",10.1080/09537325.2025.2506122,http://arxiv.org/abs/2405.06957v1,arxiv,,,,,,,,,pdf_cache/pdfs/10_1080_09537325_2025_2506122.pdf,cached,2405.069571,,http://arxiv.org/pdf/2405.06957v1,cs.CY,70aa22401bb6b01074169d5b8761e6fe332278db7ac4e3397893e39873587787,,,,,,en,journal,seminar,no,no,none,none,The paper does not specify evaluation metrics for the wargames discussed.,,5,none,no,success,1.0,,,,,
SCREEN_0129,Aurora-M: Open Source Continual Pre-training for Multilingual Language and Code,Taishi Nakamura; Mayank Mishra; Simone Tedeschi; Yekun Chai; Jason T Stillerman; Felix Friedrich; Prateek Yadav; Tanmay Laud; Vu Minh Chien; Terry Yue Zhuo; Diganta Misra; Ben Bogin; Xuan-Son Vu; Marzena Karpinska; Arnav Varma Dantuluri; Wojciech Kusa; Tommaso Furlanello; Rio Yokota; Niklas Muennighoff; Suhas Pai; Tosin Adewumi; Veronika Laippala; Xiaozhe Yao; Adalberto Junior; Alpay Ariyak; Aleksandr Drozd; Jordan Clive; Kshitij Gupta; Liangyu Chen; Qi Sun; Ken Tsui; Noah Persaud; Nour Fahmy; Tianlong Chen; Mohit Bansal; Nicolo Monti; Tai Dang; Ziyang Luo; Tien-Tung Bui; Roberto Navigli; Virendra Mehta; Matthew Blumberg; Victor May; Huu Nguyen; Sampo Pyysalo,2024,,"Pretrained language models are an integral part of AI applications, but their high computational cost for training limits accessibility. Initiatives such as Bloom and StarCoder aim to democratize access to pretrained models for collaborative community development. Despite these efforts, such models encounter challenges such as limited multilingual capabilities, risks of catastrophic forgetting during continual pretraining, and the high costs of training models from scratch, alongside the need to align with AI safety standards and regulatory frameworks. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435B additional tokens, Aurora-M surpasses 2T tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventional red-teaming considerations, but also with the specific concerns articulated in the Biden-Harris Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. We evaluate Aurora-M across a wide range of tasks and languages, showcasing its robustness against catastrophic forgetting and its superior performance in multilingual settings, particularly in safety evaluations. We open-source Aurora-M and its variants to encourage responsible open-source development of large language models at https://huggingface.co/aurora-m.",,http://arxiv.org/abs/2404.00399v3,arxiv,,,,,,,,,pdf_cache/pdfs/43595aed02fd946f.pdf,cached,2404.003993,,http://arxiv.org/pdf/2404.00399v3,cs.CL; cs.AI; cs.LG,8483e0fe20b4f6675dba8c02b8337b823da5eabbe75dfb1a45b5428176305acc,,,,,,en,tech-report,none,no,yes,GPT-4,none,"Accuracy, exact match accuracy, F1 score, BLEU score, Pass@1, Pass@10, Pass@100, ROUGE-2 score, Attack Success Rate (ASR), CARP values",,6,https://huggingface.co/aurora-m,yes,success,1.0,,,,accuracy|f1_score,mentioned
SCREEN_0130,ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users,Guanlin Li; Kangjie Chen; Shudong Zhang; Jie Zhang; Tianwei Zhang,2024,,"Large-scale pre-trained generative models are taking the world by storm, due to their abilities in generating creative content. Meanwhile, safeguards for these generative models are developed, to protect users' rights and safety, most of which are designed for large language models. Existing methods primarily focus on jailbreak and adversarial attacks, which mainly evaluate the model's safety under malicious prompts. Recent work found that manually crafted safe prompts can unintentionally trigger unsafe generations. To further systematically evaluate the safety risks of text-to-image models, we propose a novel Automatic Red-Teaming framework, ART. Our method leverages both vision language model and large language model to establish a connection between unsafe generations and their prompts, thereby more efficiently identifying the model's vulnerabilities. With our comprehensive experiments, we reveal the toxicity of the popular open-source text-to-image models. The experiments also validate the effectiveness, adaptability, and great diversity of ART. Additionally, we introduce three large-scale red-teaming datasets for studying the safety risks associated with text-to-image models. Datasets and models can be found in https://github.com/GuanlinLee/ART.",,http://arxiv.org/abs/2405.19360v3,arxiv,,,,,,,,,pdf_cache/pdfs/448b627bda477529.pdf,cached,2405.193603,,http://arxiv.org/pdf/2405.19360v3,cs.CR; cs.AI,9d59775d6baa9484a7e3c41cb55fddfb32cfc6933dc375312646644f21166df0,,,,,,en,conference,digital,no,yes,Llama,generator,Success rates of generating unsafe images with safe prompts,jailbreak,6,https://github.com/GuanlinLee/ART,no,success,1.0,jailbreak,,,,github.com/GuanlinLee/ART
SCREEN_0131,WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models,Liwei Jiang; Kavel Rao; Seungju Han; Allyson Ettinger; Faeze Brahman; Sachin Kumar; Niloofar Mireshghallah; Ximing Lu; Maarten Sap; Yejin Choi; Nouha Dziri,2024,,"We introduce WildTeaming, an automatic LLM safety red-teaming framework that mines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of novel jailbreak tactics, and then composes multiple tactics for systematic exploration of novel jailbreaks. Compared to prior work that performed red-teaming via recruited human workers, gradient-based optimization, or iterative revision with LLMs, our work investigates jailbreaks from chatbot users who were not specifically instructed to break the system. WildTeaming reveals previously unidentified vulnerabilities of frontier LLMs, resulting in up to 4.6x more diverse and successful adversarial attacks compared to state-of-the-art jailbreak methods. While many datasets exist for jailbreak evaluation, very few open-source datasets exist for jailbreak training, as safety training data has been closed even when model weights are open. With WildTeaming we create WildJailbreak, a large-scale open-source synthetic safety dataset with 262K vanilla (direct request) and adversarial (complex jailbreak) prompt-response pairs. To mitigate exaggerated safety behaviors, WildJailbreak provides two contrastive types of queries: 1) harmful queries (vanilla & adversarial) and 2) benign queries that resemble harmful queries in form but contain no harm. As WildJailbreak considerably upgrades the quality and scale of existing safety resources, it uniquely enables us to examine the scaling effects of data and the interplay of data properties and model capabilities during safety training. Through extensive experiments, we identify the training properties that enable an ideal balance of safety behaviors: appropriate safeguarding without over-refusal, effective handling of vanilla and adversarial queries, and minimal, if any, decrease in general capabilities. All components of WildJailbeak contribute to achieving balanced safety behaviors of models.",,http://arxiv.org/abs/2406.18510v1,arxiv,,,,,,,,,pdf_cache/pdfs/7f85f03fa880bd41.pdf,cached,2406.185101,,http://arxiv.org/pdf/2406.18510v1,cs.CL,9e993f70c81480a2a155ddaca9129ba98d808bdbba1b50799ac431c42ef3f1e9,,,,,,en,tech-report,digital,no,no,GPT-4,generator,"Diversity evaluation metrics, ASR (measured by the HarmBench test classifier), performance breakdown across various representative jailbreak tactics.",censor|disclaim|distract|fiction|hyperbol|ignore|imag|jailbreak|lexical|perv|seed|treat,6,https://github.com/allenai/wildteaming,yes,success,1.0,jailbreak,,,,mentioned
SCREEN_0132,Dialogue Action Tokens: Steering Language Models in Goal-Directed Dialogue with a Multi-Turn Planner,Kenneth Li; Yiming Wang; Fernanda Viégas; Martin Wattenberg,2024,,"We present an approach called Dialogue Action Tokens (DAT) that adapts language model agents to plan goal-directed dialogues. The core idea is to treat each utterance as an action, thereby converting dialogues into games where existing approaches such as reinforcement learning can be applied. Specifically, we freeze a pretrained language model and train a small planner model that predicts a continuous action vector, used for controlled generation in each round. This design avoids the problem of language degradation under reward optimization. When evaluated on the Sotopia platform for social simulations, the DAT-steered LLaMA model surpasses GPT-4's performance. We also apply DAT to steer an attacker language model in a novel multi-turn red-teaming setting, revealing a potential new attack surface.",,http://arxiv.org/abs/2406.11978v1,arxiv,,,,,,,,,pdf_cache/pdfs/f119b7cd2cbc5284.pdf,cached,2406.119781,,http://arxiv.org/pdf/2406.11978v1,cs.CL; cs.AI; cs.LG,a4750d7c9b0c26b98d51efd023801659f4fca11ddc9df4a274a661598e7a3f9d,,,,,,en,tech-report,digital,yes,yes,Llama,player,"Goal completion, maintaining relationships, obeying social rules, social capability scores",language degradation|task ambiguity,4,https://github.com/likenneth/dialogue_action_token,yes,success,1.0,,gpt4|llama,,,
SCREEN_0133,STAR: SocioTechnical Approach to Red Teaming Language Models,Laura Weidinger; John Mellor; Bernat Guillen Pegueroles; Nahema Marchal; Ravin Kumar; Kristian Lum; Canfer Akbulut; Mark Diaz; Stevie Bergman; Mikel Rodriguez; Verena Rieser; William Isaac,2024,,"This research introduces STAR, a sociotechnical framework that improves on current best practices for red teaming safety of large language models. STAR makes two key contributions: it enhances steerability by generating parameterised instructions for human red teamers, leading to improved coverage of the risk surface. Parameterised instructions also provide more detailed insights into model failures at no increased cost. Second, STAR improves signal quality by matching demographics to assess harms for specific groups, resulting in more sensitive annotations. STAR further employs a novel step of arbitration to leverage diverse viewpoints and improve label reliability, treating disagreement not as noise but as a valuable contribution to signal quality.",,http://arxiv.org/abs/2406.11757v4,arxiv,,,,,,,,,pdf_cache/pdfs/2e17ba3e702e244a.pdf,cached,2406.117574,,http://arxiv.org/pdf/2406.11757v4,cs.AI; cs.CL; cs.CY; cs.HC,77448f0b6d14ee07c8982d66c2e08388bef755fa79ca6f364c85cffa6a0dc527,,,,,,en,tech-report,hybrid,yes,no,none,none,"Signal quality through demographic matching and arbitration, coverage of risk surface, effectiveness of demographic matching in red teaming.",,4,none,yes,success,1.0,,,,,
SCREEN_0134,Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt,Zonghao Ying; Aishan Liu; Tianyuan Zhang; Zhengmin Yu; Siyuan Liang; Xianglong Liu; Dacheng Tao,2024,,"In the realm of large vision language models (LVLMs), jailbreak attacks serve as a red-teaming approach to bypass guardrails and uncover safety implications. Existing jailbreaks predominantly focus on the visual modality, perturbing solely visual inputs in the prompt for attacks. However, they fall short when confronted with aligned models that fuse visual and textual features simultaneously for generation. To address this limitation, this paper introduces the Bi-Modal Adversarial Prompt Attack (BAP), which executes jailbreaks by optimizing textual and visual prompts cohesively. Initially, we adversarially embed universally harmful perturbations in an image, guided by a few-shot query-agnostic corpus (e.g., affirmative prefixes and negative inhibitions). This process ensures that image prompt LVLMs to respond positively to any harmful queries. Subsequently, leveraging the adversarial image, we optimize textual prompts with specific harmful intent. In particular, we utilize a large language model to analyze jailbreak failures and employ chain-of-thought reasoning to refine textual prompts through a feedback-iteration manner. To validate the efficacy of our approach, we conducted extensive evaluations on various datasets and LVLMs, demonstrating that our method significantly outperforms other methods by large margins (+29.03% in attack success rate on average). Additionally, we showcase the potential of our attacks on black-box commercial LVLMs, such as Gemini and ChatGLM.",,http://arxiv.org/abs/2406.04031v2,arxiv,,,,,,,,,pdf_cache/pdfs/85e6a7f5d5d8613d.pdf,cached,2406.040312,,http://arxiv.org/pdf/2406.04031v2,cs.CV; cs.CR,f34b9618e7474707dfd42404ad96db6a04c68651e587a87e0341decf31dea570,,,,,,en,tech-report,digital,yes,yes,ChatGPT,analyst,Attack success rate (ASR) on various datasets and scenarios,jailbreak,5,https://github.com/NY1024/BAP-Jailbreak-Vision-Language-Models-via-Bi-Modal-Adversarial-Prompt,yes,success,1.0,jailbreak,,,,
SCREEN_0135,ASTPrompter: Preference-Aligned Automated Language Model Red-Teaming to Generate Low-Perplexity Unsafe Prompts,Amelia F. Hardy; Houjun Liu; Bernard Lange; Duncan Eddy; Mykel J. Kochenderfer,2024,,"Existing LLM red-teaming approaches prioritize high attack success rate, often resulting in high-perplexity prompts. This focus overlooks low-perplexity attacks that are more difficult to filter, more likely to arise during benign usage, and more impactful as negative downstream training examples. In response, we introduce ASTPrompter, a single-step optimization method that uses contrastive preference learning to train an attacker to maintain low perplexity while achieving a high attack success rate (ASR). ASTPrompter achieves an attack success rate 5.1 times higher on Llama-8.1B while using inputs that are 2.1 times more likely to occur according to the frozen LLM. Furthermore, our attack transfers to Mistral-7B, Qwen-7B, and TinyLlama in both black- and white-box settings. Lastly, by tuning a single hyperparameter in our method, we discover successful attack prefixes along an efficient frontier between ASR and perplexity, highlighting perplexity as a previously under-considered factor in red-teaming.",,http://arxiv.org/abs/2407.09447v4,arxiv,,,,,,,,,pdf_cache/pdfs/f03a457d41fe97e1.pdf,cached,2407.094474,,http://arxiv.org/pdf/2407.09447v4,cs.CL,9fe62f1eabd02f88cf648d9a5fdcc27d9eec7e0377053c2a2d9d54aa6bfa1f8e,,,,,,en,tech-report,digital,no,yes,Llama-8.1B,player,"Attack success rate (ASR), perplexity, cross-model transferability",,6,none,yes,success,1.0,,llama,,,
SCREEN_0136,PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action,Yijia Shao; Tianshi Li; Weiyan Shi; Yanchen Liu; Diyi Yang,2024,,"As language models (LMs) are widely utilized in personalized communication scenarios (e.g., sending emails, writing social media posts) and endowed with a certain level of agency, ensuring they act in accordance with the contextual privacy norms becomes increasingly critical. However, quantifying the privacy norm awareness of LMs and the emerging privacy risk in LM-mediated communication is challenging due to (1) the contextual and long-tailed nature of privacy-sensitive cases, and (2) the lack of evaluation approaches that capture realistic application scenarios. To address these challenges, we propose PrivacyLens, a novel framework designed to extend privacy-sensitive seeds into expressive vignettes and further into agent trajectories, enabling multi-level evaluation of privacy leakage in LM agents' actions. We instantiate PrivacyLens with a collection of privacy norms grounded in privacy literature and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM performance in answering probing questions and their actual behavior when executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4 and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even when prompted with privacy-enhancing instructions. We also demonstrate the dynamic nature of PrivacyLens by extending each seed into multiple trajectories to red-team LM privacy leakage risk. Dataset and code are available at https://github.com/SALT-NLP/PrivacyLens.",,http://arxiv.org/abs/2409.00138v3,arxiv,,,,,,,,,pdf_cache/pdfs/6729d6b95e5b17b4.pdf,cached,2409.001383,,http://arxiv.org/pdf/2409.00138v3,cs.CL; cs.AI; cs.CR,48e498c44d054c0ea202a97e628182b5e83b093bc1bea5debc15fd28bb09d25d,,,,,,en,conference,digital,no,yes,"GPT-4, Llama-3-70B",player,"Evaluation of privacy leakage in LM agents' actions, probing questions, and action-based evaluation.",privacy leakage,6,https://github.com/SALT-NLP/PrivacyLens,no,success,1.0,,gpt4|llama,,,github.com/SALT-NLP/PrivacyLens
SCREEN_0137,WHITE PAPER: A Brief Exploration of Data Exfiltration using GCG Suffixes,Victor Valbuena,2024,,"The cross-prompt injection attack (XPIA) is an effective technique that can be used for data exfiltration, and that has seen increasing use. In this attack, the attacker injects a malicious instruction into third party data which an LLM is likely to consume when assisting a user, who is the victim. XPIA is often used as a means for data exfiltration, and the estimated cost of the average data breach for a business is nearly $4.5 million, which includes breaches such as compromised enterprise credentials. With the rise of gradient-based attacks such as the GCG suffix attack, the odds of an XPIA occurring which uses a GCG suffix are worryingly high. As part of my work in Microsoft's AI Red Team, I demonstrated a viable attack model using a GCG suffix paired with an injection in a simulated XPIA scenario. The results indicate that the presence of a GCG suffix can increase the odds of successful data exfiltration by nearly 20%, with some caveats.",,http://arxiv.org/abs/2408.00925v1,arxiv,,,,,,,,,pdf_cache/pdfs/95e6fd6f07ab9584.pdf,cached,2408.009251,,http://arxiv.org/pdf/2408.00925v1,cs.CR; cs.AI,d33426c1954e3eaf7fec3c30d0cda5d7d1d7daf3522a31b3533e885a91b2b6a0,,,,,,en,tech-report,digital,no,yes,"GPT-3.5, GPT-4o",player,"Success rate of data exfiltration, model compliance with injection requests, function call formatting",prompt_sensitivity,6,none,yes,success,1.0,prompt_sensitivity,,,,
SCREEN_0138,What Is Wrong with My Model? Identifying Systematic Problems with Semantic Data Slicing,Chenyang Yang; Yining Hong; Grace A. Lewis; Tongshuang Wu; Christian Kästner,2024,,"Machine learning models make mistakes, yet sometimes it is difficult to identify the systematic problems behind the mistakes. Practitioners engage in various activities, including error analysis, testing, auditing, and red-teaming, to form hypotheses of what can go (or has gone) wrong with their models. To validate these hypotheses, practitioners employ data slicing to identify relevant examples. However, traditional data slicing is limited by available features and programmatic slicing functions. In this work, we propose SemSlicer, a framework that supports semantic data slicing, which identifies a semantically coherent slice, without the need for existing features. SemSlicer uses Large Language Models to annotate datasets and generate slices from any user-defined slicing criteria. We show that SemSlicer generates accurate slices with low cost, allows flexible trade-offs between different design dimensions, reliably identifies under-performing data slices, and helps practitioners identify useful data slices that reflect systematic problems.",,http://arxiv.org/abs/2409.09261v1,arxiv,,,,,,,,,pdf_cache/pdfs/8eec790927a619ef.pdf,cached,2409.092611,,http://arxiv.org/pdf/2409.09261v1,cs.SE; cs.AI; cs.CL; cs.LG,23835b788bbfe47bab8b244e2001f9457958b9182c2ec0120a7e260f030dace1,,,,,,en,conference,digital,yes,no,GPT-4,generator,"Accuracy of slicing functions, cost and accuracy trade-offs, usefulness for model evaluation",,4,https://github.com/malusamayo/SemSlicer,no,success,1.0,,,,accuracy,
SCREEN_0139,BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks,Yunhan Zhao; Xiang Zheng; Lin Luo; Yige Li; Xingjun Ma; Yu-Gang Jiang,2024,"ICLR 2025, BlueSuffix: Reinforced Blue Teaming for Vision-Language
  Models Against Jailbreak Attacks. In Proceedings of the International
  Conference on Learning Representations (ICLR), 2025","In this paper, we focus on black-box defense for VLMs against jailbreak attacks. Existing black-box defense methods are either unimodal or bimodal. Unimodal methods enhance either the vision or language module of the VLM, while bimodal methods robustify the model through text-image representation realignment. However, these methods suffer from two limitations: 1) they fail to fully exploit the cross-modal information, or 2) they degrade the model performance on benign inputs. To address these limitations, we propose a novel blue-team method BlueSuffix that defends target VLMs against jailbreak attacks without compromising its performance under black-box setting. BlueSuffix includes three key components: 1) a visual purifier against jailbreak images, 2) a textual purifier against jailbreak texts, and 3) a blue-team suffix generator using reinforcement fine-tuning for enhancing cross-modal robustness. We empirically show on four VLMs (LLaVA, MiniGPT-4, InstructionBLIP, and Gemini) and four safety benchmarks (Harmful Instruction, AdvBench, MM-SafetyBench, and RedTeam-2K) that BlueSuffix outperforms the baseline defenses by a significant margin. Our BlueSuffix opens up a promising direction for defending VLMs against jailbreak attacks. Code is available at https://github.com/Vinsonzyh/BlueSuffix.",,http://arxiv.org/abs/2410.20971v2,arxiv,,,,,,,,,pdf_cache/pdfs/5af289725e6e567c.pdf,cached,2410.209712,,http://arxiv.org/pdf/2410.20971v2,cs.CV; cs.AI; cs.LG,dd7d125cb485ce68d54b99467031516de447c4870adab5e65e1659c6c7da916e,,,,,,en,conference,digital,no,yes,GPT-4,generator,"Reduction in Attack Success Rate (ASR), semantic similarity scores, toxicity scores from Perspective API",jailbreak,6,https://github.com/Vinsonzyh/BlueSuffix,no,success,1.0,jailbreak,,,,github.com/Vinsonzyh/BlueSuffix
SCREEN_0140,AdvAgent: Controllable Blackbox Red-teaming on Web Agents,Chejian Xu; Mintong Kang; Jiawei Zhang; Zeyi Liao; Lingbo Mo; Mengqi Yuan; Huan Sun; Bo Li,2024,,"Foundation model-based agents are increasingly used to automate complex tasks, enhancing efficiency and productivity. However, their access to sensitive resources and autonomous decision-making also introduce significant security risks, where successful attacks could lead to severe consequences. To systematically uncover these vulnerabilities, we propose AdvAgent, a black-box red-teaming framework for attacking web agents. Unlike existing approaches, AdvAgent employs a reinforcement learning-based pipeline to train an adversarial prompter model that optimizes adversarial prompts using feedback from the black-box agent. With careful attack design, these prompts effectively exploit agent weaknesses while maintaining stealthiness and controllability. Extensive evaluations demonstrate that AdvAgent achieves high success rates against state-of-the-art GPT-4-based web agents across diverse web tasks. Furthermore, we find that existing prompt-based defenses provide only limited protection, leaving agents vulnerable to our framework. These findings highlight critical vulnerabilities in current web agents and emphasize the urgent need for stronger defense mechanisms. We release code at https://ai-secure.github.io/AdvAgent/.",,http://arxiv.org/abs/2410.17401v4,arxiv,,,,,,,,,pdf_cache/pdfs/ba2a1f1e3c8e0109.pdf,cached,2410.174014,,http://arxiv.org/pdf/2410.17401v4,cs.CR; cs.CL,e2b5e36a069cd91d79413c0f4f506243bb276d42ae2204c5adadc99aa2118bef,,,,,,en,conference,digital,no,yes,GPT-4,player,Attack success rate (ASR) against web agents across various tasks and domains.,,6,https://ai-secure.github.io/AdvAgent/,no,success,1.0,,gpt4,,,
SCREEN_0141,Strategic Reasoning with Language Models,Kanishk Gandhi; Dorsa Sadigh; Noah D. Goodman,2023,,"Strategic reasoning enables agents to cooperate, communicate, and compete with other agents in diverse situations. Existing approaches to solving strategic games rely on extensive training, yielding strategies that do not generalize to new scenarios or games without retraining. Large Language Models (LLMs), with their ability to comprehend and generate complex, context-rich language, could prove powerful as tools for strategic gameplay. This paper introduces an approach that uses pretrained LLMs with few-shot chain-of-thought examples to enable strategic reasoning for AI agents. Our approach uses systematically generated demonstrations of reasoning about states, values, and beliefs to prompt the model. Using extensive variations of simple matrix games, we show that strategies that are derived based on systematically generated prompts generalize almost perfectly to new game structures, alternate objectives, and hidden information. Additionally, we demonstrate our approach can lead to human-like negotiation strategies in realistic scenarios without any extra training or fine-tuning. Our results highlight the ability of LLMs, guided by systematic reasoning demonstrations, to adapt and excel in diverse strategic scenarios.",,http://arxiv.org/abs/2305.19165v1,arxiv,,,,,,,,,pdf_cache/pdfs/59732090a0a83162.pdf,cached,2305.191651,,http://arxiv.org/pdf/2305.19165v1,cs.AI; cs.CL; cs.GT; cs.HC,30579ccd31cb82cd37b57c440e2f3ca6b64a85be4b1bc0de584b98f25cf3df77,,,,,,en,tech-report,matrix,no,yes,not specified,player,"Generalization to new game structures, alternate objectives, and hidden information",,4,none,yes,success,1.0,,,,,
SCREEN_0142,"Explore, Establish, Exploit: Red Teaming Language Models from Scratch",Stephen Casper; Jason Lin; Joe Kwon; Gatlen Culp; Dylan Hadfield-Menell,2023,,"Deploying large language models (LMs) can pose hazards from harmful outputs such as toxic or false text. Prior work has introduced automated tools that elicit harmful outputs to identify these risks. While this is a valuable step toward securing models, these approaches rely on a pre-existing way to efficiently classify undesirable outputs. Using a pre-existing classifier does not allow for red-teaming to be tailored to the target model. Furthermore, when failures can be easily classified in advance, red-teaming has limited marginal value because problems can be avoided by simply filtering training data and/or model outputs. Here, we consider red-teaming ""from scratch,"" in which the adversary does not begin with a way to classify failures. Our framework consists of three steps: 1) Exploring the model's range of behaviors in the desired context; 2) Establishing a definition and measurement for undesired behavior (e.g., a classifier trained to reflect human evaluations); and 3) Exploiting the model's flaws using this measure to develop diverse adversarial prompts. We use this approach to red-team GPT-3 to discover classes of inputs that elicit false statements. In doing so, we construct the CommonClaim dataset of 20,000 statements labeled by humans as common-knowledge-true, common knowledge-false, or neither. We are making code and data available.",,http://arxiv.org/abs/2306.09442v3,arxiv,,,,,,,,,pdf_cache/pdfs/537f88fe6e028bf0.pdf,cached,2306.094423,,http://arxiv.org/pdf/2306.09442v3,cs.CL; cs.AI; cs.LG,dfd2c67dc0bc966bc2644562dc9df44cc69d78bd5190bce429cb1808d072c5ee,,,,,,en,tech-report,digital,yes,yes,GPT-3,player,"Toxicity classifier accuracy, diversity of adversarial prompts, success rate of eliciting harmful outputs",hallucination|harmful biases|jailbreaks,4,none,yes,success,1.0,,,,accuracy,
SCREEN_0143,Ares: A System-Oriented Wargame Framework for Adversarial ML,Farhan Ahmed; Pratik Vaishnavi; Kevin Eykholt; Amir Rahmati,2022,,"Since the discovery of adversarial attacks against machine learning models nearly a decade ago, research on adversarial machine learning has rapidly evolved into an eternal war between defenders, who seek to increase the robustness of ML models against adversarial attacks, and adversaries, who seek to develop better attacks capable of weakening or defeating these defenses. This domain, however, has found little buy-in from ML practitioners, who are neither overtly concerned about these attacks affecting their systems in the real world nor are willing to trade off the accuracy of their models in pursuit of robustness against these attacks. In this paper, we motivate the design and implementation of Ares, an evaluation framework for adversarial ML that allows researchers to explore attacks and defenses in a realistic wargame-like environment. Ares frames the conflict between the attacker and defender as two agents in a reinforcement learning environment with opposing objectives. This allows the introduction of system-level evaluation metrics such as time to failure and evaluation of complex strategies such as moving target defenses. We provide the results of our initial exploration involving a white-box attacker against an adversarially trained defender.",,http://arxiv.org/abs/2210.12952v1,arxiv,,,,,,,,,pdf_cache/pdfs/59f22c7c9427b44c.pdf,cached,2210.129521,,http://arxiv.org/pdf/2210.12952v1,cs.LG; cs.AI; cs.CR,d8b1147368ecebe40f3a4aedd0f75fb90769dd88968bc1f09f8deee6f3638911,,,,,,en,tech-report,digital,no,yes,none,none,System-level evaluation metrics such as time to failure and evaluation of complex strategies such as moving target defenses.,,6,https://github.com/Ethos-lab/ares,yes,success,1.0,,,,accuracy,
SCREEN_0144,Scaling Artificial Intelligence for Digital Wargaming in Support of Decision-Making,Scotty Black; Christian Darken,2024,NATO STO-MP-MSG-207 2023,"In this unprecedented era of technology-driven transformation, it becomes more critical than ever that we aggressively invest in developing robust artificial intelligence (AI) for wargaming in support of decision-making. By advancing AI-enabled systems and pairing these with human judgment, we will be able to enhance all-domain awareness, improve the speed and quality of our decision cycles, offer recommendations for novel courses of action, and more rapidly counter our adversary's actions. It therefore becomes imperative that we accelerate the development of AI to help us better address the complexity of modern challenges and dilemmas that currently requires human intelligence and, if possible, attempt to surpass human intelligence--not to replace humans, but to augment and better inform human decision-making at machine speed. Although deep reinforcement learning continues to show promising results in intelligent agent behavior development for the long-horizon, complex tasks typically found in combat modeling and simulation, further research is needed to enable the scaling of AI to deal with these intricate and expansive state-spaces characteristic of wargaming for either concept development, education, or analysis. To help address this challenge, in our research, we are developing and implementing a hierarchical reinforcement learning framework that includes a multi-model approach and dimension-invariant observation abstractions.",10.14339/STO-MP-MSG-207-23-PDF,http://arxiv.org/abs/2402.06075v1,arxiv,,,,,,,,,pdf_cache/pdfs/10_14339_STO-MP-MSG-207-23-PDF.pdf,cached,2402.060751,,http://arxiv.org/pdf/2402.06075v1,cs.LG; cs.AI,9ad0e11f5ceb77fc9ae648136f522fee423d808faffece984ad53b416be883cb,,,,,,en,conference,digital,no,no,none,none,none,,6,none,yes,success,1.0,,,,,
SCREEN_0145,Can social media shape the security of next-generation connected vehicles?,Nicola Scarano; Luca Mannella; Alessandro Savino; Stefano Di Carlo,2024,"2024 IEEE 30th International Symposium on On-Line Testing and
  Robust System Design (IOLTS)","The increasing adoption of connectivity and electronic components in vehicles makes these systems valuable targets for attackers. While automotive vendors prioritize safety, there remains a critical need for comprehensive assessment and analysis of cyber risks. In this context, this paper proposes a Social Media Automotive Threat Intelligence (SOCMATI) framework, specifically designed for the emerging field of automotive cybersecurity. The framework leverages advanced intelligence techniques and machine learning models to extract valuable insights from social media. Four use cases illustrate the framework's potential by demonstrating how it can significantly enhance threat assessment procedures within the automotive industry.",10.1109/IOLTS60994.2024.10616053,http://arxiv.org/abs/2407.07599v1,arxiv,,,,,,,,,pdf_cache/pdfs/10_1109_IOLTS60994_2024_10616053.pdf,cached,2407.075991,,http://arxiv.org/pdf/2407.07599v1,cs.SI,92e00adf9b1411c74e3f7fbb4adc25c8619b60ba45663369801a11b83349dd55,,,,,,en,conference,none,no,no,none,none,none,,6,none,no,success,1.0,,,,,
SCREEN_0146,Gradient-Based Language Model Red Teaming,Nevan Wichers; Carson Denison; Ahmad Beirami,2024,,"Red teaming is a common strategy for identifying weaknesses in generative language models (LMs), where adversarial prompts are produced that trigger an LM to generate unsafe responses. Red teaming is instrumental for both model alignment and evaluation, but is labor-intensive and difficult to scale when done by humans. In this paper, we present Gradient-Based Red Teaming (GBRT), a red teaming method for automatically generating diverse prompts that are likely to cause an LM to output unsafe responses. GBRT is a form of prompt learning, trained by scoring an LM response with a safety classifier and then backpropagating through the frozen safety classifier and LM to update the prompt. To improve the coherence of input prompts, we introduce two variants that add a realism loss and fine-tune a pretrained model to generate the prompts instead of learning the prompts directly. Our experiments show that GBRT is more effective at finding prompts that trigger an LM to generate unsafe responses than a strong reinforcement learning-based red teaming approach, and succeeds even when the LM has been fine-tuned to produce safer outputs.",,http://arxiv.org/abs/2401.16656v1,arxiv,,,,,,,,,pdf_cache/pdfs/90364d575ac052c0.pdf,cached,2401.166561,,http://arxiv.org/pdf/2401.16656v1,cs.CL,6f8b6c47d2a43a478ed5645dd0215b876623904a33a1347ac724630593dc36d4,,,,,,en,tech-report,digital,no,yes,LaMDA,player,Probability of response being unsafe given the prompt,,6,https://github.com/google-research/google-research/tree/master/gbrt,yes,success,1.0,,,,,
SCREEN_0147,Towards Red Teaming in Multimodal and Multilingual Translation,Christophe Ropers; David Dale; Prangthip Hansanti; Gabriel Mejia Gonzalez; Ivan Evtimov; Corinne Wong; Christophe Touret; Kristina Pereyra; Seohyun Sonia Kim; Cristian Canton Ferrer; Pierre Andrews; Marta R. Costa-jussà,2024,,"Assessing performance in Natural Language Processing is becoming increasingly complex. One particular challenge is the potential for evaluation datasets to overlap with training data, either directly or indirectly, which can lead to skewed results and overestimation of model performance. As a consequence, human evaluation is gaining increasing interest as a means to assess the performance and reliability of models. One such method is the red teaming approach, which aims to generate edge cases where a model will produce critical errors. While this methodology is becoming standard practice for generative AI, its application to the realm of conditional AI remains largely unexplored. This paper presents the first study on human-based red teaming for Machine Translation (MT), marking a significant step towards understanding and improving the performance of translation models. We delve into both human-based red teaming and a study on automation, reporting lessons learned and providing recommendations for both translation models and red teaming drills. This pioneering work opens up new avenues for research and development in the field of MT.",,http://arxiv.org/abs/2401.16247v1,arxiv,,,,,,,,,pdf_cache/pdfs/cb39d02bbe8bd9fc.pdf,cached,2401.162471,,http://arxiv.org/pdf/2401.16247v1,cs.CL; cs.CY; I.2.7,3b29d76ac1d22e215a0bf3a4dc0e195537e078a23dcb8046b433b96d5b6a6e57,,,,,,en,tech-report,none,no,no,none,none,"ROC AUC scores for automated evaluation of red-teaming results, BLASER and COMET metrics for translation quality estimation",accent bias|deviation in instructions|deviation in numbers and units|deviation in toxicity|gender bias|hallucination of personally identifiable information (PII)|named entity error|opposite sentiment|pitch bias|safety concerns,6,none,yes,success,1.0,,,,human_evaluation,
SCREEN_0148,Red-Teaming for Generative AI: Silver Bullet or Security Theater?,Michael Feffer; Anusha Sinha; Wesley Hanwen Deng; Zachary C. Lipton; Hoda Heidari,2024,,"In response to rising concerns surrounding the safety, security, and trustworthiness of Generative AI (GenAI) models, practitioners and regulators alike have pointed to AI red-teaming as a key component of their strategies for identifying and mitigating these risks. However, despite AI red-teaming's central role in policy discussions and corporate messaging, significant questions remain about what precisely it means, what role it can play in regulation, and how it relates to conventional red-teaming practices as originally conceived in the field of cybersecurity. In this work, we identify recent cases of red-teaming activities in the AI industry and conduct an extensive survey of relevant research literature to characterize the scope, structure, and criteria for AI red-teaming practices. Our analysis reveals that prior methods and practices of AI red-teaming diverge along several axes, including the purpose of the activity (which is often vague), the artifact under evaluation, the setting in which the activity is conducted (e.g., actors, resources, and methods), and the resulting decisions it informs (e.g., reporting, disclosure, and mitigation). In light of our findings, we argue that while red-teaming may be a valuable big-tent idea for characterizing GenAI harm mitigations, and that industry may effectively apply red-teaming and other strategies behind closed doors to safeguard AI, gestures towards red-teaming (based on public definitions) as a panacea for every possible risk verge on security theater. To move toward a more robust toolbox of evaluations for generative AI, we synthesize our recommendations into a question bank meant to guide and scaffold future AI red-teaming practices.",,http://arxiv.org/abs/2401.15897v3,arxiv,,,,,,,,,pdf_cache/pdfs/a343f385515909b9.pdf,cached,2401.158973,,http://arxiv.org/pdf/2401.15897v3,cs.CY; cs.HC; cs.LG,beaf783f88ff3167c20ef8703fba9c3c05c8320ada85415e7bae66995fe347d5,,,,,,en,tech-report,none,no,no,none,none,none,,6,none,yes,success,1.0,,,,,
SCREEN_0149,Digital cloning of online social networks for language-sensitive agent-based modeling of misinformation spread,Prateek Puri; Gabriel Hassler; Anton Shenk; Sai Katragadda,2024,,"We develop a simulation framework for studying misinformation spread within online social networks that blends agent-based modeling and natural language processing techniques. While many other agent-based simulations exist in this space, questions over their fidelity and generalization to existing networks in part hinders their ability to provide actionable insights. To partially address these concerns, we create a 'digital clone' of a known misinformation sharing network by downloading social media histories for over ten thousand of its users. We parse these histories to both extract the structure of the network and model the nuanced ways in which information is shared and spread among its members. Unlike many other agent-based methods in this space, information sharing between users in our framework is sensitive to topic of discussion, user preferences, and online community dynamics. To evaluate the fidelity of our method, we seed our cloned network with a set of posts recorded in the base network and compare propagation dynamics between the two, observing reasonable agreement across the twin networks over a variety of metrics. Lastly, we explore how the cloned network may serve as a flexible, low-cost testbed for misinformation countermeasure evaluation and red teaming analysis. We hope the tools explored here augment existing efforts in the space and unlock new opportunities for misinformation countermeasure evaluation, a field that may become increasingly important to consider with the anticipated rise of misinformation campaigns fueled by generative artificial intelligence.",,http://arxiv.org/abs/2401.12509v2,arxiv,,,,,,,,,pdf_cache/pdfs/19871ff9b0b7665e.pdf,cached,2401.125092,,http://arxiv.org/pdf/2401.12509v2,cs.SI; cs.LG,8d1ec7e0cd0d50632a01b76fba96f325257b898aa47280fd63cd00286133fa32,,,,,,en,tech-report,digital,yes,yes,gpt-3.5-turbo,generator,Cosine similarity between BERT embeddings of predicted and actual quote tweets; comparison of propagation dynamics between cloned and base networks.,factual_error,5,https://github.com/Evovest/EvoTrees.jl,yes,success,1.0,factual_error,bert,,,
SCREEN_0150,Combating Adversarial Attacks with Multi-Agent Debate,Steffi Chern; Zhen Fan; Andy Liu,2024,,"While state-of-the-art language models have achieved impressive results, they remain susceptible to inference-time adversarial attacks, such as adversarial prompts generated by red teams arXiv:2209.07858. One approach proposed to improve the general quality of language model generations is multi-agent debate, where language models self-evaluate through discussion and feedback arXiv:2305.14325. We implement multi-agent debate between current state-of-the-art language models and evaluate models' susceptibility to red team attacks in both single- and multi-agent settings. We find that multi-agent debate can reduce model toxicity when jailbroken or less capable models are forced to debate with non-jailbroken or more capable models. We also find marginal improvements through the general usage of multi-agent interactions. We further perform adversarial prompt content classification via embedding clustering, and analyze the susceptibility of different models to different types of attack topics.",,http://arxiv.org/abs/2401.05998v1,arxiv,,,,,,,,,pdf_cache/pdfs/a6f7cd90e3e49d86.pdf,cached,2401.059981,,http://arxiv.org/pdf/2401.05998v1,cs.CL; cs.AI,3ecb43c5431ca3e1ff92f2cd963b8cd1b34f6c9032ea01337a46360a41c4de37,,,,,,en,tech-report,digital,yes,no,"GPT-3.5, Llama-2",player,Mean toxicity score from Perspective API and a fine-tuned Llama-based classifier,adversarial vulnerability|toxicity,5,https://github.com/andyjliu/llms-course-project,yes,success,1.0,,llama,,,
SCREEN_0151,AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning,Vasudev Gohil; Satwik Patnaik; Dileep Kalathil; Jeyavijayan Rajendran,2024,,"Machine learning has shown great promise in addressing several critical hardware security problems. In particular, researchers have developed novel graph neural network (GNN)-based techniques for detecting intellectual property (IP) piracy, detecting hardware Trojans (HTs), and reverse engineering circuits, to name a few. These techniques have demonstrated outstanding accuracy and have received much attention in the community. However, since these techniques are used for security applications, it is imperative to evaluate them thoroughly and ensure they are robust and do not compromise the security of integrated circuits. In this work, we propose AttackGNN, the first red-team attack on GNN-based techniques in hardware security. To this end, we devise a novel reinforcement learning (RL) agent that generates adversarial examples, i.e., circuits, against the GNN-based techniques. We overcome three challenges related to effectiveness, scalability, and generality to devise a potent RL agent. We target five GNN-based techniques for four crucial classes of problems in hardware security: IP piracy, detecting/localizing HTs, reverse engineering, and hardware obfuscation. Through our approach, we craft circuits that fool all GNNs considered in this work. For instance, to evade IP piracy detection, we generate adversarial pirated circuits that fool the GNN-based defense into classifying our crafted circuits as not pirated. For attacking HT localization GNN, our attack generates HT-infested circuits that fool the defense on all tested circuits. We obtain a similar 100% success rate against GNNs for all classes of problems.",,http://arxiv.org/abs/2402.13946v2,arxiv,,,,,,,,,pdf_cache/pdfs/0c02e37a4cf649a3.pdf,cached,2402.139462,,http://arxiv.org/pdf/2402.13946v2,cs.LG; cs.CR,f068e9f1eb0571a446e694c624507b918ce02940b7cf711533178db20b5343c2,,,,,,en,tech-report,digital,no,yes,none,none,"Adversarial success rate against GNNs, training reward curve, number of successful adversarial circuits as a function of black-box queries",,6,none,yes,success,1.0,,,,accuracy,
SCREEN_0152,Experiments with Encoding Structured Data for Neural Networks,Sujay Nagesh Koujalgi; Jonathan Dodge,2024,,"The project's aim is to create an AI agent capable of selecting good actions in a game-playing domain called Battlespace. Sequential domains like Battlespace are important testbeds for planning problems, as such, the Department of Defense uses such domains for wargaming exercises. The agents we developed combine Monte Carlo Tree Search (MCTS) and Deep Q-Network (DQN) techniques in an effort to navigate the game environment, avoid obstacles, interact with adversaries, and capture the flag. This paper will focus on the encoding techniques we explored to present complex structured data stored in a Python class, a necessary precursor to an agent.",,http://arxiv.org/abs/2402.10290v1,arxiv,,,,,,,,,pdf_cache/pdfs/aca4645fb06fdfef.pdf,cached,2402.102901,,http://arxiv.org/pdf/2402.10290v1,cs.AI; I.2.4,b5ad6259a89b4cc433e59611d8d31f23a56dac6502cfda553145d66dde5149f7,,,,,,en,tech-report,digital,no,yes,none,none,"Win percentages, draws due to timeouts or mutual destruction",,6,https://gitlab.kitware.com/mixtape/battlespace.git,yes,success,1.0,,,,,
SCREEN_0153,Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation,Jessica Quaye; Alicia Parrish; Oana Inel; Charvi Rastogi; Hannah Rose Kirk; Minsuk Kahng; Erin van Liemt; Max Bartolo; Jess Tsang; Justin White; Nathan Clement; Rafael Mosquera; Juan Ciro; Vijay Janapa Reddi; Lora Aroyo,2024,,"With the rise of text-to-image (T2I) generative AI models reaching wide audiences, it is critical to evaluate model robustness against non-obvious attacks to mitigate the generation of offensive images. By focusing on ``implicitly adversarial'' prompts (those that trigger T2I models to generate unsafe images for non-obvious reasons), we isolate a set of difficult safety issues that human creativity is well-suited to uncover. To this end, we built the Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing a diverse set of implicitly adversarial prompts. We have assembled a suite of state-of-the-art T2I models, employed a simple user interface to identify and annotate harms, and engaged diverse populations to capture long-tail safety issues that may be overlooked in standard testing. The challenge is run in consecutive rounds to enable a sustained discovery and analysis of safety pitfalls in T2I models. In this paper, we present an in-depth account of our methodology, a systematic study of novel attack strategies and discussion of safety failures revealed by challenge participants. We also release a companion visualization tool for easy exploration and derivation of insights from the dataset. The first challenge round resulted in over 10k prompt-image pairs with machine annotations for safety. A subset of 1.5k samples contains rich human annotations of harm types and attack styles. We find that 14% of images that humans consider harmful are mislabeled as ``safe'' by machines. We have identified new attack strategies that highlight the complexity of ensuring T2I model robustness. Our findings emphasize the necessity of continual auditing and adaptation as new vulnerabilities emerge. We are confident that this work will enable proactive, iterative safety assessments and promote responsible development of T2I models.",,http://arxiv.org/abs/2403.12075v3,arxiv,,,,,,,,,pdf_cache/pdfs/e618c572a0a79201.pdf,cached,2403.120753,,http://arxiv.org/pdf/2403.12075v3,cs.CY; cs.AI; cs.CR; cs.CV; cs.LG,f6002972930d8ac857d8957f0ce36815580275619844340fd624afdb8c9cc66e,,,,,,en,conference,none,no,no,none,none,"Human annotations of harm types and attack styles; machine annotations for safety; true positive, true negative, false positive, and false negative rates for classifiers.",,6,none,no,success,1.0,,,,,
SCREEN_0154,A Kalman Filter Based Framework for Monitoring the Performance of In-Hospital Mortality Prediction Models Over Time,Jiacheng Liu; Lisa Kirkland; Jaideep Srivastava,2024,,"Unlike in a clinical trial, where researchers get to determine the least number of positive and negative samples required, or in a machine learning study where the size and the class distribution of the validation set is static and known, in a real-world scenario, there is little control over the size and distribution of incoming patients. As a result, when measured during different time periods, evaluation metrics like Area under the Receiver Operating Curve (AUCROC) and Area Under the Precision-Recall Curve(AUCPR) may not be directly comparable. Therefore, in this study, for binary classifiers running in a long time period, we proposed to adjust these performance metrics for sample size and class distribution, so that a fair comparison can be made between two time periods. Note that the number of samples and the class distribution, namely the ratio of positive samples, are two robustness factors which affect the variance of AUCROC. To better estimate the mean of performance metrics and understand the change of performance over time, we propose a Kalman filter based framework with extrapolated variance adjusted for the total number of samples and the number of positive samples during different time periods. The efficacy of this method is demonstrated first on a synthetic dataset and then retrospectively applied to a 2-days ahead in-hospital mortality prediction model for COVID-19 patients during 2021 and 2022. Further, we conclude that our prediction model is not significantly affected by the evolution of the disease, improved treatments and changes in hospital operational plans.",,http://arxiv.org/abs/2402.06812v1,arxiv,,,,,,,,,pdf_cache/pdfs/dd93d5a57f27d34d.pdf,cached,2402.068121,,http://arxiv.org/pdf/2402.06812v1,cs.LG,e74f24b5b6667420ecd26a7575289e3c8794089f3d0646b93e23279cba6dd91f,,,,,,en,tech-report,none,no,yes,none,none,"Area under the Receiver Operating Curve (AUCROC), Area Under the Precision-Recall Curve (AUCPR)",,6,none,yes,success,1.0,,,,,
SCREEN_0155,Scaling Intelligent Agents in Combat Simulations for Wargaming,Scotty Black; Christian Darken,2024,I/ITSEC Conference Proceedings 2023,"Remaining competitive in future conflicts with technologically-advanced competitors requires us to accelerate our research and development in artificial intelligence (AI) for wargaming. More importantly, leveraging machine learning for intelligent combat behavior development will be key to one day achieving superhuman performance in this domain--elevating the quality and accelerating the speed of our decisions in future wars. Although deep reinforcement learning (RL) continues to show promising results in intelligent agent behavior development in games, it has yet to perform at or above the human level in the long-horizon, complex tasks typically found in combat modeling and simulation. Capitalizing on the proven potential of RL and recent successes of hierarchical reinforcement learning (HRL), our research is investigating and extending the use of HRL to create intelligent agents capable of performing effectively in these large and complex simulation environments. Our ultimate goal is to develop an agent capable of superhuman performance that could then serve as an AI advisor to military planners and decision-makers. This papers covers our ongoing approach and the first three of our five research areas aimed at managing the exponential growth of computations that have thus far limited the use of AI in combat simulations: (1) developing an HRL training framework and agent architecture for combat units; (2) developing a multi-model framework for agent decision-making; (3) developing dimension-invariant observation abstractions of the state space to manage the exponential growth of computations; (4) developing an intrinsic rewards engine to enable long-term planning; and (5) implementing this framework into a higher-fidelity combat simulation.",,http://arxiv.org/abs/2402.06694v1,arxiv,,,,,,,,,pdf_cache/pdfs/0cdfa54d99cf116b.pdf,cached,2402.066941,,http://arxiv.org/pdf/2402.06694v1,cs.LG; cs.AI,fe9b62345f90f66522fabf7866fc95a0e1844b0508617ec2b7238f4f0b7bd2f8,,,,,,en,conference,digital,no,yes,none,none,"Performance of agents compared to baseline scripted agents, learning rate, and game score correlation.",,6,none,no,success,1.0,,,,,
SCREEN_0156,Red Teaming Models for Hyperspectral Image Analysis Using Explainable AI,Vladimir Zaigrajew; Hubert Baniecki; Lukasz Tulczyjew; Agata M. Wijata; Jakub Nalepa; Nicolas Longépé; Przemyslaw Biecek,2024,,"Remote sensing (RS) applications in the space domain demand machine learning (ML) models that are reliable, robust, and quality-assured, making red teaming a vital approach for identifying and exposing potential flaws and biases. Since both fields advance independently, there is a notable gap in integrating red teaming strategies into RS. This paper introduces a methodology for examining ML models operating on hyperspectral images within the HYPERVIEW challenge, focusing on soil parameters' estimation. We use post-hoc explanation methods from the Explainable AI (XAI) domain to critically assess the best performing model that won the HYPERVIEW challenge and served as an inspiration for the model deployed on board the INTUITION-1 hyperspectral mission. Our approach effectively red teams the model by pinpointing and validating key shortcomings, constructing a model that achieves comparable performance using just 1% of the input features and a mere up to 5% performance loss. Additionally, we propose a novel way of visualizing explanations that integrate domain-specific information about hyperspectral bands (wavelengths) and data transformations to better suit interpreting models for hyperspectral image analysis.",,http://arxiv.org/abs/2403.08017v2,arxiv,,,,,,,,,pdf_cache/pdfs/1d65d11a4fcb3a4a.pdf,cached,2403.080172,,http://arxiv.org/pdf/2403.08017v2,cs.CV; cs.AI,2c43173014e82920c475cd64065ad2a36a0c7760190c607f47c3470b64bc8ef9,,,,,,en,workshop,none,no,yes,none,none,Mean Absolute Error (MAE) for soil parameters; Shapley values for feature importance,,6,https://github.com/ridvansalihkuzu/hyperview_eagleeyes,no,success,1.0,,,,,
SCREEN_0157,A Safe Harbor for AI Evaluation and Red Teaming,Shayne Longpre; Sayash Kapoor; Kevin Klyman; Ashwin Ramaswami; Rishi Bommasani; Borhane Blili-Hamelin; Yangsibo Huang; Aviya Skowron; Zheng-Xin Yong; Suhas Kotha; Yi Zeng; Weiyan Shi; Xianjun Yang; Reid Southen; Alexander Robey; Patrick Chao; Diyi Yang; Ruoxi Jia; Daniel Kang; Sandy Pentland; Arvind Narayanan; Percy Liang; Peter Henderson,2024,,"Independent evaluation and red teaming are critical for identifying the risks posed by generative AI systems. However, the terms of service and enforcement strategies used by prominent AI companies to deter model misuse have disincentives on good faith safety evaluations. This causes some researchers to fear that conducting such research or releasing their findings will result in account suspensions or legal reprisal. Although some companies offer researcher access programs, they are an inadequate substitute for independent research access, as they have limited community representation, receive inadequate funding, and lack independence from corporate incentives. We propose that major AI developers commit to providing a legal and technical safe harbor, indemnifying public interest safety research and protecting it from the threat of account suspensions or legal reprisal. These proposals emerged from our collective experience conducting safety, privacy, and trustworthiness research on generative AI systems, where norms and incentives could be better aligned with public interests, without exacerbating model misuse. We believe these commitments are a necessary step towards more inclusive and unimpeded community efforts to tackle the risks of generative AI.",,http://arxiv.org/abs/2403.04893v1,arxiv,,,,,,,,,pdf_cache/pdfs/9ed21909b154d820.pdf,cached,2403.048931,,http://arxiv.org/pdf/2403.04893v1,cs.AI,6c50666b011a5b1eda23aa0a412d10ec1d6a784a48725c825055aef430ca36f1,,,,,,en,tech-report,none,no,no,GPT-4,none,none,,6,none,yes,success,1.0,,,,,
SCREEN_0158,A Mechanism-Based Approach to Mitigating Harms from Persuasive Generative AI,Seliem El-Sayed; Canfer Akbulut; Amanda McCroskery; Geoff Keeling; Zachary Kenton; Zaria Jalan; Nahema Marchal; Arianna Manzini; Toby Shevlane; Shannon Vallor; Daniel Susser; Matija Franklin; Sophie Bridgers; Harry Law; Matthew Rahtz; Murray Shanahan; Michael Henry Tessler; Arthur Douillard; Tom Everitt; Sasha Brown,2024,,"Recent generative AI systems have demonstrated more advanced persuasive capabilities and are increasingly permeating areas of life where they can influence decision-making. Generative AI presents a new risk profile of persuasion due the opportunity for reciprocal exchange and prolonged interactions. This has led to growing concerns about harms from AI persuasion and how they can be mitigated, highlighting the need for a systematic study of AI persuasion. The current definitions of AI persuasion are unclear and related harms are insufficiently studied. Existing harm mitigation approaches prioritise harms from the outcome of persuasion over harms from the process of persuasion. In this paper, we lay the groundwork for the systematic study of AI persuasion. We first put forward definitions of persuasive generative AI. We distinguish between rationally persuasive generative AI, which relies on providing relevant facts, sound reasoning, or other forms of trustworthy evidence, and manipulative generative AI, which relies on taking advantage of cognitive biases and heuristics or misrepresenting information. We also put forward a map of harms from AI persuasion, including definitions and examples of economic, physical, environmental, psychological, sociocultural, political, privacy, and autonomy harm. We then introduce a map of mechanisms that contribute to harmful persuasion. Lastly, we provide an overview of approaches that can be used to mitigate against process harms of persuasion, including prompt engineering for manipulation classification and red teaming. Future work will operationalise these mitigations and study the interaction between different types of mechanisms of persuasion.",,http://arxiv.org/abs/2404.15058v1,arxiv,,,,,,,,,pdf_cache/pdfs/b953d7b3678e128e.pdf,cached,2404.150581,,http://arxiv.org/pdf/2404.15058v1,cs.CY; cs.AI,05c231a18fefd03fd015c892e97747b6482bb849125a337fd345ead07902a3d8,,,,,,en,tech-report,digital,yes,no,GPT-4,generator,"Evaluation metrics include prompt engineering for manipulation classification, red teaming, and scalable oversight.",deception|fake expertise|misrepresentation|prompt_sensitivity|sycophancy,5,none,yes,success,1.0,deception|prompt_sensitivity,,,,
SCREEN_0159,A survey of air combat behavior modeling using machine learning,Patrick Ribu Gorton; Andreas Strand; Karsten Brathen,2024,,"With the recent advances in machine learning, creating agents that behave realistically in simulated air combat has become a growing field of interest. This survey explores the application of machine learning techniques for modeling air combat behavior, motivated by the potential to enhance simulation-based pilot training. Current simulated entities tend to lack realistic behavior, and traditional behavior modeling is labor-intensive and prone to loss of essential domain knowledge between development steps. Advancements in reinforcement learning and imitation learning algorithms have demonstrated that agents may learn complex behavior from data, which could be faster and more scalable than manual methods. Yet, making adaptive agents capable of performing tactical maneuvers and operating weapons and sensors still poses a significant challenge. The survey examines applications, behavior model types, prevalent machine learning methods, and the technical and human challenges in developing adaptive and realistically behaving agents. Another challenge is the transfer of agents from learning environments to military simulation systems and the consequent demand for standardization. Four primary recommendations are presented regarding increased emphasis on beyond-visual-range scenarios, multi-agent machine learning and cooperation, utilization of hierarchical behavior models, and initiatives for standardization and research collaboration. These recommendations aim to address current issues and guide the development of more comprehensive, adaptable, and realistic machine learning-based behavior models for air combat applications.",,http://arxiv.org/abs/2404.13954v1,arxiv,,,,,,,,,pdf_cache/pdfs/4623fb82b4e95b1d.pdf,cached,2404.139541,,http://arxiv.org/pdf/2404.13954v1,cs.LG; cs.AI; cs.MA; F.2.0; I.2.1; I.2.6; I.2.8,9eb9161318c5369114d3c166b382b853bcd8f0763ad6bd6c163321cf26425536,,,,,,en,tech-report,digital,no,no,none,none,The paper does not specify evaluation metrics as it is a survey.,,6,none,yes,success,1.0,,,,,
SCREEN_0160,Using Game Engines and Machine Learning to Create Synthetic Satellite Imagery for a Tabletop Verification Exercise,Johannes Hoster; Sara Al-Sayed; Felix Biessmann; Alexander Glaser; Kristian Hildebrand; Igor Moric; Tuong Vy Nguyen,2024,,"Satellite imagery is regarded as a great opportunity for citizen-based monitoring of activities of interest. Relevant imagery may however not be available at sufficiently high resolution, quality, or cadence -- let alone be uniformly accessible to open-source analysts. This limits an assessment of the true long-term potential of citizen-based monitoring of nuclear activities using publicly available satellite imagery. In this article, we demonstrate how modern game engines combined with advanced machine-learning techniques can be used to generate synthetic imagery of sites of interest with the ability to choose relevant parameters upon request; these include time of day, cloud cover, season, or level of activity onsite. At the same time, resolution and off-nadir angle can be adjusted to simulate different characteristics of the satellite. While there are several possible use-cases for synthetic imagery, here we focus on its usefulness to support tabletop exercises in which simple monitoring scenarios can be examined to better understand verification capabilities enabled by new satellite constellations and very short revisit times.",,http://arxiv.org/abs/2404.11461v2,arxiv,,,,,,,,,pdf_cache/pdfs/c255e9222469d7b7.pdf,cached,2404.114612,,http://arxiv.org/pdf/2404.11461v2,cs.CV; cs.AI; cs.HC; cs.LG,9950a41cf5bfe1de351a90a2e09d78e7dbb5cfe2a2358a901596644dce18dde2,,,,,,en,tech-report,tabletop,no,no,none,none,The paper does not specify evaluation metrics for the wargame.,,5,none,yes,success,1.0,,,,,mentioned
SCREEN_0161,Red-Teaming Segment Anything Model,Krzysztof Jankowski; Bartlomiej Sobieski; Mateusz Kwiatkowski; Jakub Szulc; Michal Janik; Hubert Baniecki; Przemyslaw Biecek,2024,,"Foundation models have emerged as pivotal tools, tackling many complex tasks through pre-training on vast datasets and subsequent fine-tuning for specific applications. The Segment Anything Model is one of the first and most well-known foundation models for computer vision segmentation tasks. This work presents a multi-faceted red-teaming analysis that tests the Segment Anything Model against challenging tasks: (1) We analyze the impact of style transfer on segmentation masks, demonstrating that applying adverse weather conditions and raindrops to dashboard images of city roads significantly distorts generated masks. (2) We focus on assessing whether the model can be used for attacks on privacy, such as recognizing celebrities' faces, and show that the model possesses some undesired knowledge in this task. (3) Finally, we check how robust the model is to adversarial attacks on segmentation masks under text prompts. We not only show the effectiveness of popular white-box attacks and resistance to black-box attacks but also introduce a novel approach - Focused Iterative Gradient Attack (FIGA) that combines white-box approaches to construct an efficient attack resulting in a smaller number of modified pixels. All of our testing methods and analyses indicate a need for enhanced safety measures in foundation models for image segmentation.",,http://arxiv.org/abs/2404.02067v1,arxiv,,,,,,,,,pdf_cache/pdfs/8a55b334c7732802.pdf,cached,2404.020671,,http://arxiv.org/pdf/2404.02067v1,cs.CV; cs.AI; cs.LG,d5c1a628390ab8a3e9ab98ce48920168a2fc44642d79010da9cdfe2d08ece9f8,,,,,,en,tech-report,none,no,no,none,none,"precision, recall, F1 score, mean Intersection over Union (IoU)",,6,https://github.com/JankowskiChristopher/red-teaming-segment-anything-model,yes,success,1.0,,,,f1_score,
SCREEN_0162,Red Teaming Language Models for Processing Contradictory Dialogues,Xiaofei Wen; Bangzheng Li; Tenghao Huang; Muhao Chen,2024,,"Most language models currently available are prone to self-contradiction during dialogues. To mitigate this issue, this study explores a novel contradictory dialogue processing task that aims to detect and modify contradictory statements in a conversation. This task is inspired by research on context faithfulness and dialogue comprehension, which have demonstrated that the detection and understanding of contradictions often necessitate detailed explanations. We develop a dataset comprising contradictory dialogues, in which one side of the conversation contradicts itself. Each dialogue is accompanied by an explanatory label that highlights the location and details of the contradiction. With this dataset, we present a Red Teaming framework for contradictory dialogue processing. The framework detects and attempts to explain the dialogue, then modifies the existing contradictory content using the explanation. Our experiments demonstrate that the framework improves the ability to detect contradictory dialogues and provides valid explanations. Additionally, it showcases distinct capabilities for modifying such dialogues. Our study highlights the importance of the logical inconsistency problem in conversational AI.",,http://arxiv.org/abs/2405.10128v3,arxiv,,,,,,,,,pdf_cache/pdfs/aef530a5a7b66cbf.pdf,cached,2405.101283,,http://arxiv.org/pdf/2405.10128v3,cs.CL; cs.AI,7c4ac7d9861ba3b7268e2b38d86146491661ccec5495f9e187c038d8e0a06499,,,,,,en,tech-report,digital,yes,no,GPT-3.5-turbo-0613,analyst,detection accuracy and explanation validity,inconsistency|logical inconsistency|self-contradiction,4,https://github.com/luka-group/contraDialog,yes,success,1.0,inconsistency,,,accuracy,
SCREEN_0163,Benchmark Early and Red Team Often: A Framework for Assessing and Managing Dual-Use Hazards of AI Foundation Models,Anthony M. Barrett; Krystal Jackson; Evan R. Murphy; Nada Madkour; Jessica Newman,2024,,"A concern about cutting-edge or ""frontier"" AI foundation models is that an adversary may use the models for preparing chemical, biological, radiological, nuclear, (CBRN), cyber, or other attacks. At least two methods can identify foundation models with potential dual-use capability; each has advantages and disadvantages: A. Open benchmarks (based on openly available questions and answers), which are low-cost but accuracy-limited by the need to omit security-sensitive details; and B. Closed red team evaluations (based on private evaluation by CBRN and cyber experts), which are higher-cost but can achieve higher accuracy by incorporating sensitive details. We propose a research and risk-management approach using a combination of methods including both open benchmarks and closed red team evaluations, in a way that leverages advantages of both methods. We recommend that one or more groups of researchers with sufficient resources and access to a range of near-frontier and frontier foundation models run a set of foundation models through dual-use capability evaluation benchmarks and red team evaluations, then analyze the resulting sets of models' scores on benchmark and red team evaluations to see how correlated those are. If, as we expect, there is substantial correlation between the dual-use potential benchmark scores and the red team evaluation scores, then implications include the following: The open benchmarks should be used frequently during foundation model development as a quick, low-cost measure of a model's dual-use potential; and if a particular model gets a high score on the dual-use potential benchmark, then more in-depth red team assessments of that model's dual-use capability should be performed. We also discuss limitations and mitigations for our approach, e.g., if model developers try to game benchmarks by including a version of benchmark test data in a model's training data.",,http://arxiv.org/abs/2405.10986v1,arxiv,,,,,,,,,pdf_cache/pdfs/f5682e5e15aae27b.pdf,cached,2405.109861,,http://arxiv.org/pdf/2405.10986v1,cs.CR; cs.AI; cs.CY; cs.LG,100c30f8a140d5f76b71433f8ac5b8654a1d24601ba0aa7508c5e8c3873aa4f0,,,,,,en,tech-report,none,no,yes,GPT-4,analyst,Correlation between dual-use potential benchmark scores and red team evaluation scores.,gaming|guardrails,6,none,yes,success,1.0,,,,accuracy,
SCREEN_0164,Aspect-based Sentiment Evaluation of Chess Moves (ASSESS): an NLP-based Method for Evaluating Chess Strategies from Textbooks,Haifa Alrdahi; Riza Batista-Navarro,2024,,"The chess domain is well-suited for creating an artificial intelligence (AI) system that mimics real-world challenges, including decision-making. Throughout the years, minimal attention has been paid to investigating insights derived from unstructured chess data sources. In this study, we examine the complicated relationships between multiple referenced moves in a chess-teaching textbook, and propose a novel method designed to encapsulate chess knowledge derived from move-action phrases. This study investigates the feasibility of using a modified sentiment analysis method as a means for evaluating chess moves based on text. Our proposed Aspect-Based Sentiment Analysis (ABSA) method represents an advancement in evaluating the sentiment associated with referenced chess moves. By extracting insights from move-action phrases, our approach aims to provide a more fine-grained and contextually aware `chess move'-based sentiment classification. Through empirical experiments and analysis, we evaluate the performance of our fine-tuned ABSA model, presenting results that confirm the efficiency of our approach in advancing aspect-based sentiment classification within the chess domain. This research contributes to the area of game-playing by machines and shows the practical applicability of leveraging NLP techniques to understand the context of strategic games.",,http://arxiv.org/abs/2405.06499v1,arxiv,,,,,,,,,pdf_cache/pdfs/248cf261b4434fae.pdf,cached,2405.064991,,http://arxiv.org/pdf/2405.06499v1,cs.CL,cbad923a35059c4c3819fc8aeea89d6854499ca82ea585b059f8c209d5a7bef5,,,,,,en,tech-report,digital,no,no,RoBERTa,analyst,Performance of the ABSA model compared to Stockfish engine evaluations.,hallucination|misalignment,5,none,yes,success,1.0,,,,,
SCREEN_0165,On the Complexity of Learning to Cooperate with Populations of Socially Rational Agents,Robert Loftin; Saptarashmi Bandyopadhyay; Mustafa Mert Çelikok,2024,,"Artificially intelligent agents deployed in the real-world will require the ability to reliably \textit{cooperate} with humans (as well as other, heterogeneous AI agents). To provide formal guarantees of successful cooperation, we must make some assumptions about how partner agents could plausibly behave. Any realistic set of assumptions must account for the fact that other agents may be just as adaptable as our agent is. In this work, we consider the problem of cooperating with a \textit{population} of agents in a finitely-repeated, two player general-sum matrix game with private utilities. Two natural assumptions in such settings are that: 1) all agents in the population are individually rational learners, and 2) when any two members of the population are paired together, with high-probability they will achieve at least the same utility as they would under some Pareto efficient equilibrium strategy. Our results first show that these assumptions alone are insufficient to ensure \textit{zero-shot} cooperation with members of the target population. We therefore consider the problem of \textit{learning} a strategy for cooperating with such a population using prior observations its members interacting with one another. We provide upper and lower bounds on the number of samples needed to learn an effective cooperation strategy. Most importantly, we show that these bounds can be much stronger than those arising from a ""naive'' reduction of the problem to one of imitation learning.",,http://arxiv.org/abs/2407.00419v1,arxiv,,,,,,,,,pdf_cache/pdfs/1b4b0a66b8eec7ec.pdf,cached,2407.004191,,http://arxiv.org/pdf/2407.00419v1,cs.LG; cs.AI; cs.GT; cs.MA,dfe44fcf615ca3deedd6762ed3624a48b71503ddc1d0c109cec4324460624801,,,,,,en,tech-report,matrix,no,yes,none,none,Upper and lower bounds on the number of samples needed to learn an effective cooperation strategy; altruistic regret,,6,none,yes,success,1.0,,,matrix,,
SCREEN_0166,Steering Without Side Effects: Improving Post-Deployment Control of Language Models,Asa Cooper Stickland; Alexander Lyzhov; Jacob Pfau; Salsabila Mahdi; Samuel R. Bowman,2024,,"Language models (LMs) have been shown to behave unexpectedly post-deployment. For example, new jailbreaks continually arise, allowing model misuse, despite extensive red-teaming and adversarial training from developers. Given most model queries are unproblematic and frequent retraining results in unstable user experience, methods for mitigation of worst-case behavior should be targeted. One such method is classifying inputs as potentially problematic, then selectively applying steering vectors on these problematic inputs, i.e. adding particular vectors to model hidden states. However, steering vectors can also negatively affect model performance, which will be an issue on cases where the classifier was incorrect. We present KL-then-steer (KTS), a technique that decreases the side effects of steering while retaining its benefits, by first training a model to minimize Kullback-Leibler (KL) divergence between a steered and unsteered model on benign inputs, then steering the model that has undergone this training. Our best method prevents 44% of jailbreak attacks compared to the original Llama-2-chat-7B model while maintaining helpfulness (as measured by MT-Bench) on benign requests almost on par with the original LM. To demonstrate the generality and transferability of our method beyond jailbreaks, we show that our KTS model can be steered to reduce bias towards user-suggested answers on TruthfulQA. Code is available: https://github.com/AsaCooperStickland/kl-then-steer.",,http://arxiv.org/abs/2406.15518v1,arxiv,,,,,,,,,pdf_cache/pdfs/c0bc441dfe50a422.pdf,cached,2406.155181,,http://arxiv.org/pdf/2406.15518v1,cs.CL; cs.LG,e99c0e45c89dc76dac35d869dfea21be746f83cf99de9089b6decf95eb60da58,,,,,,en,tech-report,digital,no,yes,Llama-2-chat-7B,player,"MT-Bench score, TruthfulQA accuracy, Jailbreak ASR, Prefill ASR",bias|jailbreak|performance degradation,6,https://github.com/AsaCooperStickland/kl-then-steer,yes,success,1.0,bias|jailbreak,llama,,accuracy,github.com/AsaCooperStickland/kl-then-steer
SCREEN_0167,Unelicitable Backdoors in Language Models via Cryptographic Transformer Circuits,Andis Draguns; Andrew Gritsevskiy; Sumeet Ramesh Motwani; Charlie Rogers-Smith; Jeffrey Ladish; Christian Schroeder de Witt,2024,"38th Conference on Neural Information Processing Systems (NeurIPS
  2024)","The rapid proliferation of open-source language models significantly increases the risks of downstream backdoor attacks. These backdoors can introduce dangerous behaviours during model deployment and can evade detection by conventional cybersecurity monitoring systems. In this paper, we introduce a novel class of backdoors in transformer models, that, in contrast to prior art, are unelicitable in nature. Unelicitability prevents the defender from triggering the backdoor, making it impossible to properly evaluate ahead of deployment even if given full white-box access and using automated techniques, such as red-teaming or certain formal verification methods. We show that our novel construction is not only unelicitable thanks to using cryptographic techniques, but also has favourable robustness properties. We confirm these properties in empirical investigations, and provide evidence that our backdoors can withstand state-of-the-art mitigation strategies. Additionally, we expand on previous work by showing that our universal backdoors, while not completely undetectable in white-box settings, can be harder to detect than some existing designs. By demonstrating the feasibility of seamlessly integrating backdoors into transformer models, this paper fundamentally questions the efficacy of pre-deployment detection strategies. This offers new insights into the offence-defence balance in AI safety and security.",,http://arxiv.org/abs/2406.02619v2,arxiv,,,,,,,,,pdf_cache/pdfs/4e425d4d05c542eb.pdf,cached,2406.026192,,http://arxiv.org/pdf/2406.02619v2,cs.CR; cs.LG,830c4a43db12ec80667aa0a9c72202ea6218084580d2279acefc8bdd451646d5,,,,,,en,conference,none,no,no,none,none,none,,6,none,no,success,1.0,,,,,mentioned
SCREEN_0168,"Phi-3 Safety Post-Training: Aligning Language Models with a ""Break-Fix"" Cycle",Emman Haider; Daniel Perez-Becker; Thomas Portet; Piyush Madan; Amit Garg; Atabak Ashfaq; David Majercak; Wen Wen; Dongwoo Kim; Ziyi Yang; Jianwen Zhang; Hiteshi Sharma; Blake Bullwinkel; Martin Pouliot; Amanda Minnich; Shiven Chawla; Solianna Herrera; Shahed Warreth; Maggie Engler; Gary Lopez; Nina Chikanov; Raja Sekhar Rao Dheekonda; Bolor-Erdene Jagdagdorj; Roman Lutz; Richard Lundeen; Tori Westerhoff; Pete Bryan; Christian Seifert; Ram Shankar Siva Kumar; Andrew Berkley; Alex Kessler,2024,,"Recent innovations in language model training have demonstrated that it is possible to create highly performant models that are small enough to run on a smartphone. As these models are deployed in an increasing number of domains, it is critical to ensure that they are aligned with human preferences and safety considerations. In this report, we present our methodology for safety aligning the Phi-3 series of language models. We utilized a ""break-fix"" cycle, performing multiple rounds of dataset curation, safety post-training, benchmarking, red teaming, and vulnerability identification to cover a variety of harm areas in both single and multi-turn scenarios. Our results indicate that this approach iteratively improved the performance of the Phi-3 models across a wide range of responsible AI benchmarks. Finally, we include additional red teaming strategies and evaluations that were used to test the safety behavior of Phi-3.5-mini and Phi-3.5-MoE, which were optimized for multilingual capabilities.",,http://arxiv.org/abs/2407.13833v2,arxiv,,,,,,,,,pdf_cache/pdfs/f778680491119f8a.pdf,cached,2407.138332,,http://arxiv.org/pdf/2407.13833v2,cs.CL; cs.AI,c7508df01feac11b39b494f7756c6df1106d8a28382e176f2586f4c62119ae17,,,,,,en,tech-report,none,no,yes,Phi-3,none,"Responsible AI benchmarks, red teaming results, internal automated measurement, XSTest",generation of harmful content|misinformation|misuse|privacy,6,https://github.com/Azure/PyRIT,yes,success,1.0,,,,,
SCREEN_0169,SafePowerGraph: Safety-aware Evaluation of Graph Neural Networks for Transmission Power Grids,Salah Ghamizi; Aleksandar Bojchevski; Aoxiang Ma; Jun Cao,2024,,"Power grids are critical infrastructures of paramount importance to modern society and their rapid evolution and interconnections has heightened the complexity of power systems (PS) operations. Traditional methods for grid analysis struggle with the computational demands of large-scale RES and ES integration, prompting the adoption of machine learning (ML) techniques, particularly Graph Neural Networks (GNNs). GNNs have proven effective in solving the alternating current (AC) Power Flow (PF) and Optimal Power Flow (OPF) problems, crucial for operational planning. However, existing benchmarks and datasets completely ignore safety and robustness requirements in their evaluation and never consider realistic safety-critical scenarios that most impact the operations of the power grids. We present SafePowerGraph, the first simulator-agnostic, safety-oriented framework and benchmark for GNNs in PS operations. SafePowerGraph integrates multiple PF and OPF simulators and assesses GNN performance under diverse scenarios, including energy price variations and power line outages. Our extensive experiments underscore the importance of self-supervised learning and graph attention architectures for GNN robustness. We provide at https://github.com/yamizi/SafePowerGraph our open-source repository, a comprehensive leaderboard, a dataset and model zoo and expect our framework to standardize and advance research in the critical field of GNN for power systems.",,http://arxiv.org/abs/2407.12421v1,arxiv,,,,,,,,,pdf_cache/pdfs/84403eac49f7eb0a.pdf,cached,2407.124211,,http://arxiv.org/pdf/2407.12421v1,cs.LG; cs.AI,d1f8be7e64758e65675fe6cedfd7e6f574df9f7cdb5fdd4ed765b16ce8a4e83a,,,,,,en,conference,none,no,yes,none,none,"Supervised errors (MSE), physical errors (constraints violations) for in-distribution scenarios, energy price variations, and power line outages.",,6,https://github.com/yamizi/SafePowerGraph,no,success,1.0,,,,,github.com/yamizi/SafePowerGraph
SCREEN_0170,"Artificial Intelligence-based Smart Port Logistics Metaverse for Enhancing Productivity, Environment, and Safety in Port Logistics: A Case Study of Busan Port",Sunghyun Sim; Dohee Kim; Kikun Park; Hyerim Bae,2024,,"The increase in global trade, the impact of COVID-19, and the tightening of environmental and safety regulations have brought significant changes to the maritime transportation market. To address these challenges, the port logistics sector is rapidly adopting advanced technologies such as big data, Internet of Things, and AI. However, despite these efforts, solving several issues related to productivity, environment, and safety in the port logistics sector requires collaboration among various stakeholders. In this study, we introduce an AI-based port logistics metaverse framework (PLMF) that facilitates communication, data sharing, and decision-making among diverse stakeholders in port logistics. The developed PLMF includes 11 AI-based metaverse content modules related to productivity, environment, and safety, enabling the monitoring, simulation, and decision making of real port logistics processes. Examples of these modules include the prediction of expected time of arrival, dynamic port operation planning, monitoring and prediction of ship fuel consumption and port equipment emissions, and detection and monitoring of hazardous ship routes and accidents between workers and port equipment. We conducted a case study using historical data from Busan Port to analyze the effectiveness of the PLMF. By predicting the expected arrival time of ships within the PLMF and optimizing port operations accordingly, we observed that the framework could generate additional direct revenue of approximately 7.3 million dollars annually, along with a 79% improvement in ship punctuality, resulting in certain environmental benefits for the port. These findings indicate that PLMF not only provides a platform for various stakeholders in port logistics to participate and collaborate but also significantly enhances the accuracy and sustainability of decision-making in port logistics through AI-based simulations.",,http://arxiv.org/abs/2409.10519v1,arxiv,,,,,,,,,pdf_cache/pdfs/58ef52b11e752d77.pdf,cached,2409.105191,,http://arxiv.org/pdf/2409.10519v1,cs.OH,865e58512dd6266c6ec64bf21dd1f1b3e30afbd0806b36adf9f7916e00c3ecf3,,,,,,en,tech-report,digital,no,yes,none,none,"Effectiveness of the PLMF was evaluated through a case study using historical data from Busan Port, focusing on revenue generation and improvement in ship punctuality.",,6,none,yes,success,1.0,,,,accuracy,
SCREEN_0171,Mastering the Digital Art of War: Developing Intelligent Combat Simulation Agents for Wargaming Using Hierarchical Reinforcement Learning,Scotty Black,2024,,"In today's rapidly evolving military landscape, advancing artificial intelligence (AI) in support of wargaming becomes essential. Despite reinforcement learning (RL) showing promise for developing intelligent agents, conventional RL faces limitations in handling the complexity inherent in combat simulations. This dissertation proposes a comprehensive approach, including targeted observation abstractions, multi-model integration, a hybrid AI framework, and an overarching hierarchical reinforcement learning (HRL) framework. Our localized observation abstraction using piecewise linear spatial decay simplifies the RL problem, enhancing computational efficiency and demonstrating superior efficacy over traditional global observation methods. Our multi-model framework combines various AI methodologies, optimizing performance while still enabling the use of diverse, specialized individual behavior models. Our hybrid AI framework synergizes RL with scripted agents, leveraging RL for high-level decisions and scripted agents for lower-level tasks, enhancing adaptability, reliability, and performance. Our HRL architecture and training framework decomposes complex problems into manageable subproblems, aligning with military decision-making structures. Although initial tests did not show improved performance, insights were gained to improve future iterations. This study underscores AI's potential to revolutionize wargaming, emphasizing the need for continued research in this domain.",,http://arxiv.org/abs/2408.13333v1,arxiv,,,,,,,,,pdf_cache/pdfs/5e7a30bb1e35477e.pdf,cached,2408.133331,,http://arxiv.org/pdf/2408.13333v1,cs.LG; cs.AI,1c051536c87a468c59d573fe0f94028443dace0fad51dd0cc3f6cc392a676958,,,,,,en,tech-report,digital,no,yes,none,none,"Performance optimization, computational efficiency, adaptability, reliability",,6,none,yes,success,1.0,,,,,
SCREEN_0172,Capturing the Complexity of Human Strategic Decision-Making with Machine Learning,Jian-Qiao Zhu; Joshua C. Peterson; Benjamin Enke; Thomas L. Griffiths,2024,,"Understanding how people behave in strategic settings--where they make decisions based on their expectations about the behavior of others--is a long-standing problem in the behavioral sciences. We conduct the largest study to date of strategic decision-making in the context of initial play in two-player matrix games, analyzing over 90,000 human decisions across more than 2,400 procedurally generated games that span a much wider space than previous datasets. We show that a deep neural network trained on these data predicts people's choices better than leading theories of strategic behavior, indicating that there is systematic variation that is not explained by those theories. We then modify the network to produce a new, interpretable behavioral model, revealing what the original network learned about people: their ability to optimally respond and their capacity to reason about others are dependent on the complexity of individual games. This context-dependence is critical in explaining deviations from the rational Nash equilibrium, response times, and uncertainty in strategic decisions. More broadly, our results demonstrate how machine learning can be applied beyond prediction to further help generate novel explanations of complex human behavior.",,http://arxiv.org/abs/2408.07865v1,arxiv,,,,,,,,,pdf_cache/pdfs/4897671192732ebd.pdf,cached,2408.078651,,http://arxiv.org/pdf/2408.07865v1,econ.GN; cs.GT; cs.LG; q-fin.EC,07b38635616aab5901a5b48ac5402ca88405c60262d7a046cd3212a2ae123fde,,,,,,en,tech-report,matrix,no,yes,none,none,"Mean Squared Error (MSE), R2, Completeness",,6,none,yes,success,1.0,,,,,
SCREEN_0173,Towards Universal Large-Scale Foundational Model for Natural Gas Demand Forecasting,Xinxing Zhou; Jiaqi Ye; Shubao Zhao; Ming Jin; Zhaoxiang Hou; Chengyi Yang; Zengxiang Li; Yanlong Wen; Xiaojie Yuan,2024,,"In the context of global energy strategy, accurate natural gas demand forecasting is crucial for ensuring efficient resource allocation and operational planning. Traditional forecasting methods struggle to cope with the growing complexity and variability of gas consumption patterns across diverse industries and commercial sectors. To address these challenges, we propose the first foundation model specifically tailored for natural gas demand forecasting. Foundation models, known for their ability to generalize across tasks and datasets, offer a robust solution to the limitations of traditional methods, such as the need for separate models for different customer segments and their limited generalization capabilities. Our approach leverages contrastive learning to improve prediction accuracy in real-world scenarios, particularly by tackling issues such as noise in historical consumption data and the potential misclassification of similar data samples, which can lead to degradation in the quaility of the representation and thus the accuracy of downstream forecasting tasks. By integrating advanced noise filtering techniques within the contrastive learning framework, our model enhances the quality of learned representations, leading to more accurate predictions. Furthermore, the model undergoes industry-specific fine-tuning during pretraining, enabling it to better capture the unique characteristics of gas consumption across various sectors. We conducted extensive experiments using a large-scale dataset from ENN Group, which includes data from over 10,000 industrial, commercial, and welfare-related customers across multiple regions. Our model outperformed existing state-of-the-art methods, demonstrating a relative improvement in MSE by 3.68\% and in MASE by 6.15\% compared to the best available model.",,http://arxiv.org/abs/2409.15794v1,arxiv,,,,,,,,,pdf_cache/pdfs/62337d03cdcababd.pdf,cached,2409.157941,,http://arxiv.org/pdf/2409.15794v1,cs.LG; cs.AI,3e024217658c72a1afe3200fcae210696b13de7f5fa3d7f505f0c61a5e6f5419,,,,,,en,journal,none,no,yes,none,none,"Mean Squared Error (MSE), Mean Absolute Scaled Error (MASE)",,6,none,no,success,1.0,,,,accuracy,
SCREEN_0174,Games for AI Control: Models of Safety Evaluations of AI Deployment Protocols,Charlie Griffin; Louis Thomson; Buck Shlegeris; Alessandro Abate,2024,,"To evaluate the safety and usefulness of deployment protocols for untrusted AIs, AI Control uses a red-teaming exercise played between a protocol designer and an adversary. This paper introduces AI-Control Games, a formal decision-making model of the red-teaming exercise as a multi-objective, partially observable, stochastic game. We also introduce methods for finding optimal protocols in AI-Control Games, by reducing them to a set of zero-sum partially observable stochastic games. We apply our formalism to model, evaluate and synthesise protocols for deploying untrusted language models as programming assistants, focusing on Trusted Monitoring protocols, which use weaker language models and limited human assistance. Finally, we demonstrate the utility of our formalism by showcasing improvements over empirical studies in existing settings, evaluating protocols in new settings, and analysing how modelling assumptions affect the safety and usefulness of protocols.",,http://arxiv.org/abs/2409.07985v1,arxiv,,,,,,,,,pdf_cache/pdfs/d3c2496ec8b591de.pdf,cached,2409.079851,,http://arxiv.org/pdf/2409.07985v1,cs.AI; cs.LG,4572d2137e834c6fcda1cdd5b64663843fbf97dc7de639a4cddc8bfd90f917df,,,,,,en,tech-report,digital,no,yes,none,none,"Safety and usefulness scores, Pareto-optimal protocols, safety-usefulness trade-off analysis",,6,https://github.com/CJ-Griffin/GamesForAIControl/tree/arxiv,yes,success,1.0,,,,,
SCREEN_0175,Beyond Nash Equilibrium: Achieving Bayesian Perfect Equilibrium with Belief Update Fictitious Play,Qi Ju; Zhemei Fang; Yunfeng Luo,2024,,"In the domain of machine learning and game theory, the quest for Nash Equilibrium (NE) in extensive-form games with incomplete information is challenging yet crucial for enhancing AI's decision-making support under varied scenarios. Traditional Counterfactual Regret Minimization (CFR) techniques excel in navigating towards NE, focusing on scenarios where opponents deploy optimal strategies. However, the essence of machine learning in strategic game play extends beyond reacting to optimal moves; it encompasses aiding human decision-making in all circumstances. This includes not only crafting responses to optimal strategies but also recovering from suboptimal decisions and capitalizing on opponents' errors. Herein lies the significance of transitioning from NE to Bayesian Perfect Equilibrium (BPE), which accounts for every possible condition, including the irrationality of opponents. To bridge this gap, we propose Belief Update Fictitious Play (BUFP), which innovatively blends fictitious play with belief to target BPE, a more comprehensive equilibrium concept than NE. Specifically, through adjusting iteration stepsizes, BUFP allows for strategic convergence to both NE and BPE. For instance, in our experiments, BUFP(EF) leverages the stepsize of Extensive Form Fictitious Play (EFFP) to achieve BPE, outperforming traditional CFR by securing a 48.53\% increase in benefits in scenarios characterized by dominated strategies.",,http://arxiv.org/abs/2409.02706v1,arxiv,,,,,,,,,pdf_cache/pdfs/1e386ef32629f958.pdf,cached,2409.027061,,http://arxiv.org/pdf/2409.02706v1,cs.GT,bbc55c521f479d52d110b38981ee7f47379c42042d3c9b0de08075ee626e5568,,,,,,en,conference,digital,no,yes,none,none,Exploitability and total exploitability metrics to assess algorithmic convergence.,,6,none,no,success,1.0,,,,,
SCREEN_0176,Active Causal Structure Learning with Latent Variables: Towards Learning to Detour in Autonomous Robots,Pablo de los Riscos; Fernando Corbacho,2024,,"Artificial General Intelligence (AGI) Agents and Robots must be able to cope with everchanging environments and tasks. They must be able to actively construct new internal causal models of their interactions with the environment when new structural changes take place in the environment. Thus, we claim that active causal structure learning with latent variables (ACSLWL) is a necessary component to build AGI agents and robots. This paper describes how a complex planning and expectation-based detour behavior can be learned by ACSLWL when, unexpectedly, and for the first time, the simulated robot encounters a sort of transparent barrier in its pathway towards its target. ACSWL consists of acting in the environment, discovering new causal relations, constructing new causal models, exploiting the causal models to maximize its expected utility, detecting possible latent variables when unexpected observations occur, and constructing new structures-internal causal models and optimal estimation of the associated parameters, to be able to cope efficiently with the new encountered situations. That is, the agent must be able to construct new causal internal models that transform a previously unexpected and inefficient (sub-optimal) situation, into a predictable situation with an optimal operating plan.",,http://arxiv.org/abs/2410.20894v1,arxiv,,,,,,,,,pdf_cache/pdfs/8f7a0257ca7e785d.pdf,cached,2410.208941,,http://arxiv.org/pdf/2410.20894v1,cs.AI; cs.LG; I.2.6; I.2.8,b99030eccd7e198c3f8e62fca80964133e931a3d86f86aa9fb8dd587809a07cd,,,,,,en,tech-report,digital,no,yes,none,none,"Maximum Expected Utility (MEU), transition probabilities, and utility functions",,6,none,yes,success,1.0,,,,,
SCREEN_0177,An Auditing Test To Detect Behavioral Shift in Language Models,Leo Richter; Xuanli He; Pasquale Minervini; Matt J. Kusner,2024,International Conference on Learning Representations (ICLR) 2025,"As language models (LMs) approach human-level performance, a comprehensive understanding of their behavior becomes crucial. This includes evaluating capabilities, biases, task performance, and alignment with societal values. Extensive initial evaluations, including red teaming and diverse benchmarking, can establish a model's behavioral profile. However, subsequent fine-tuning or deployment modifications may alter these behaviors in unintended ways. We present a method for continual Behavioral Shift Auditing (BSA) in LMs. Building on recent work in hypothesis testing, our auditing test detects behavioral shifts solely through model generations. Our test compares model generations from a baseline model to those of the model under scrutiny and provides theoretical guarantees for change detection while controlling false positives. The test features a configurable tolerance parameter that adjusts sensitivity to behavioral changes for different use cases. We evaluate our approach using two case studies: monitoring changes in (a) toxicity and (b) translation performance. We find that the test is able to detect meaningful changes in behavior distributions using just hundreds of examples.",,http://arxiv.org/abs/2410.19406v2,arxiv,,,,,,,,,pdf_cache/pdfs/bb9386f3df798963.pdf,cached,2410.194062,,http://arxiv.org/pdf/2410.19406v2,cs.LG,787749ae8feca30cf4b363ff01658b4bea3e88b188e7b4f469d5d045ad45532a,,,,,,en,conference,none,no,yes,Llama3-70B-Instruct,analyst,"Detection of behavioral shifts using a sequential hypothesis test with a tolerance parameter, false positive rate control, and sample efficiency.",,6,https://github.com/richterleo/lm-auditing-test,no,success,1.0,,,,,
SCREEN_0178,Seeing Seeds Beyond Weeds: Green Teaming Generative AI for Beneficial Uses,Logan Stapleton; Jordan Taylor; Sarah Fox; Tongshuang Wu; Haiyi Zhu,2023,,"Large generative AI models (GMs) like GPT and DALL-E are trained to generate content for general, wide-ranging purposes. GM content filters are generalized to filter out content which has a risk of harm in many cases, e.g., hate speech. However, prohibited content is not always harmful -- there are instances where generating prohibited content can be beneficial. So, when GMs filter out content, they preclude beneficial use cases along with harmful ones. Which use cases are precluded reflects the values embedded in GM content filtering. Recent work on red teaming proposes methods to bypass GM content filters to generate harmful content. We coin the term green teaming to describe methods of bypassing GM content filters to design for beneficial use cases. We showcase green teaming by: 1) Using ChatGPT as a virtual patient to simulate a person experiencing suicidal ideation, for suicide support training; 2) Using Codex to intentionally generate buggy solutions to train students on debugging; and 3) Examining an Instagram page using Midjourney to generate images of anti-LGBTQ+ politicians in drag. Finally, we discuss how our use cases demonstrate green teaming as both a practical design method and a mode of critique, which problematizes and subverts current understandings of harms and values in generative AI.",,http://arxiv.org/abs/2306.03097v1,arxiv,,,,,,,,,pdf_cache/pdfs/d8d0aea7823d6ab9.pdf,cached,2306.030971,,http://arxiv.org/pdf/2306.03097v1,cs.HC; cs.AI,fa04094630783d08c1d3e6b311beb66fd8d51f1d743ac1733e003f69f528e42f,,,,,,en,tech-report,digital,yes,no,GPT-3.5 Turbo,player,The paper does not specify formal evaluation metrics but discusses the effectiveness of bypassing content filters for beneficial use cases.,,3,none,yes,success,1.0,,,,,
SCREEN_0179,Can Language Models be Instructed to Protect Personal Information?,Yang Chen; Ethan Mendes; Sauvik Das; Wei Xu; Alan Ritter,2023,,"Large multimodal language models have proven transformative in numerous applications. However, these models have been shown to memorize and leak pre-training data, raising serious user privacy and information security concerns. While data leaks should be prevented, it is also crucial to examine the trade-off between the privacy protection and model utility of proposed approaches. In this paper, we introduce PrivQA -- a multimodal benchmark to assess this privacy/utility trade-off when a model is instructed to protect specific categories of personal information in a simulated scenario. We also propose a technique to iteratively self-moderate responses, which significantly improves privacy. However, through a series of red-teaming experiments, we find that adversaries can also easily circumvent these protections with simple jailbreaking methods through textual and/or image inputs. We believe PrivQA has the potential to support the development of new models with improved privacy protections, as well as the adversarial robustness of these protections. We release the entire PrivQA dataset at https://llm-access-control.github.io/.",,http://arxiv.org/abs/2310.02224v1,arxiv,,,,,,,,,pdf_cache/pdfs/18c3e01ba739792f.pdf,cached,2310.022241,,http://arxiv.org/pdf/2310.02224v1,cs.CL,0a4e825f0e7953cf9a8df0a377aa107b03300cc68796c529bde98dd6ee28df57,,,,,,en,tech-report,digital,no,yes,GPT-4,player,"Protective precision, Response F1, sensitivity, specificity",bias|data_leakage|jailbreak|jailbreaking,6,https://llm-access-control.github.io/,yes,success,1.0,data_leakage|jailbreak,,,,
SCREEN_0180,Clustered Federated Learning Architecture for Network Anomaly Detection in Large Scale Heterogeneous IoT Networks,Xabier Sáez-de-Cámara; Jose Luis Flores; Cristóbal Arellano; Aitor Urbieta; Urko Zurutuza,2023,,"There is a growing trend of cyberattacks against Internet of Things (IoT) devices; moreover, the sophistication and motivation of those attacks is increasing. The vast scale of IoT, diverse hardware and software, and being typically placed in uncontrolled environments make traditional IT security mechanisms such as signature-based intrusion detection and prevention systems challenging to integrate. They also struggle to cope with the rapidly evolving IoT threat landscape due to long delays between the analysis and publication of the detection rules. Machine learning methods have shown faster response to emerging threats; however, model training architectures like cloud or edge computing face multiple drawbacks in IoT settings, including network overhead and data isolation arising from the large scale and heterogeneity that characterizes these networks. This work presents an architecture for training unsupervised models for network intrusion detection in large, distributed IoT and Industrial IoT (IIoT) deployments. We leverage Federated Learning (FL) to collaboratively train between peers and reduce isolation and network overhead problems. We build upon it to include an unsupervised device clustering algorithm fully integrated into the FL pipeline to address the heterogeneity issues that arise in FL settings. The architecture is implemented and evaluated using a testbed that includes various emulated IoT/IIoT devices and attackers interacting in a complex network topology comprising 100 emulated devices, 30 switches and 10 routers. The anomaly detection models are evaluated on real attacks performed by the testbed's threat actors, including the entire Mirai malware lifecycle, an additional botnet based on the Merlin command and control server and other red-teaming tools performing scanning activities and multiple attacks targeting the emulated devices.",10.1016/j.cose.2023.103299,http://arxiv.org/abs/2303.15986v2,arxiv,,,,,,,,,pdf_cache/pdfs/10_1016_j_cose_2023_103299.pdf,cached,2303.159862,,http://arxiv.org/pdf/2303.15986v2,cs.CR,6529f82de911f89cf5424c41be44580a2da690ec499ca7466e2eea9339c8b682,,,,,,en,journal,none,no,no,none,none,none,,6,none,no,success,1.0,,,,,
SCREEN_0181,Cyber Key Terrain Identification Using Adjusted PageRank Centrality,Lukáš Sadlek; Pavel Čeleda,2023,,"The cyber terrain contains devices, network services, cyber personas, and other network entities involved in network operations. Designing a method that automatically identifies key network entities to network operations is challenging. However, such a method is essential for determining which cyber assets should the cyber defense focus on. In this paper, we propose an approach for the classification of IP addresses belonging to cyber key terrain according to their network position using the PageRank centrality computation adjusted by machine learning. We used hill climbing and random walk algorithms to distinguish PageRank's damping factors based on source and destination ports captured in IP flows. The one-time learning phase on a static data sample allows near-real-time stream-based classification of key hosts from IP flow data in operational conditions without maintaining a complete network graph. We evaluated the approach on a dataset from a cyber defense exercise and on data from the campus network. The results show that cyber key terrain identification using the adjusted computation of centrality is more precise than its original version.",10.1007/978-3-031-56326-3_21,http://arxiv.org/abs/2306.11018v2,arxiv,,,,,,,,,pdf_cache/pdfs/10_1007_978-3-031-56326-3_21.pdf,cached,2306.110182,,http://arxiv.org/pdf/2306.11018v2,cs.CR,94e738df5cff04640e3dd0153d54d7639778ef683c5672a6cc7c2ec3c913098b,,,,,,en,conference,digital,no,yes,none,none,"F1 score, precision, recall",,6,https://doi.org/10.5281/zenodo.7884228,no,success,1.0,,,,f1_score,
SCREEN_0182,Investigation of Multi-stage Attack and Defense Simulation for Data Synthesis,Ömer Sen; Bozhidar Ivanov; Martin Henze; Andreas Ulbig,2023,"Proceedings of the 2023 International Conference on Smart Energy
  Systems and Technologies (SEST)","The power grid is a critical infrastructure that plays a vital role in modern society. Its availability is of utmost importance, as a loss can endanger human lives. However, with the increasing digitalization of the power grid, it also becomes vulnerable to new cyberattacks that can compromise its availability. To counter these threats, intrusion detection systems are developed and deployed to detect cyberattacks targeting the power grid. Among intrusion detection systems, anomaly detection models based on machine learning have shown potential in detecting unknown attack vectors. However, the scarcity of data for training these models remains a challenge due to confidentiality concerns. To overcome this challenge, this study proposes a model for generating synthetic data of multi-stage cyber attacks in the power grid, using attack trees to model the attacker's sequence of steps and a game-theoretic approach to incorporate the defender's actions. This model aims to create diverse attack data on which machine learning algorithms can be trained.",10.1109/SEST57387.2023.10257329,http://arxiv.org/abs/2312.13697v1,arxiv,,,,,,,,,pdf_cache/pdfs/10_1109_SEST57387_2023_10257329.pdf,cached,2312.136971,,http://arxiv.org/pdf/2312.13697v1,cs.CR; cs.SY; eess.SY,d8e391db18d51876dc65f9b94aac9da62ae1f14406c3d4f03b5c1b3cd5757eba,,,,,,en,conference,digital,no,yes,none,none,"Accuracy, Recall, Precision, F1-Score, AUC, MCC",,6,none,no,success,1.0,,,,accuracy|f1_score,
SCREEN_0183,GNN-based Passenger Request Prediction,Aqsa Ashraf Makhdomi; Iqra Altaf Gillani,2023,,"Passenger request prediction is essential for operations planning, control, and management in ride-sharing platforms. While the demand prediction problem has been studied extensively, the Origin-Destination (OD) flow prediction of passengers has received less attention from the research community. This paper develops a Graph Neural Network framework along with the Attention Mechanism to predict the OD flow of passengers. The proposed framework exploits various linear and non-linear dependencies that arise among requests originating from different locations and captures the repetition pattern and the contextual data of that place. Moreover, the optimal size of the grid cell that covers the road network and preserves the complexity and accuracy of the model is determined. Extensive simulations are conducted to examine the characteristics of our proposed approach and its various components. The results show the superior performance of our proposed model compared to the existing baselines.",,http://arxiv.org/abs/2301.02515v2,arxiv,,,,,,,,,pdf_cache/pdfs/bebab4431cb987b0.pdf,cached,2301.025152,,http://arxiv.org/pdf/2301.02515v2,cs.LG; cs.AI,aa196df04c468390bf7a04527bcbf6ff25719621797130a2b774cb27269e17ac,,,,,,en,tech-report,,no,no,,,,,6,none,yes,success,0.7699999999999999,,,,accuracy,
SCREEN_0184,Learning Environment for the Air Domain (LEAD),Andreas Strand; Patrick Gorton; Martin Asprusten; Karsten Brathen,2023,,"A substantial part of fighter pilot training is simulation-based and involves computer-generated forces controlled by predefined behavior models. The behavior models are typically manually created by eliciting knowledge from experienced pilots, which is a time-consuming process. Despite the work put in, the behavior models are often unsatisfactory due to their predictable nature and lack of adaptivity, forcing instructors to spend time manually monitoring and controlling them. Reinforcement and imitation learning pose as alternatives to handcrafted models. This paper presents the Learning Environment for the Air Domain (LEAD), a system for creating and integrating intelligent air combat behavior in military simulations. By incorporating the popular programming library and interface Gymnasium, LEAD allows users to apply readily available machine learning algorithms. Additionally, LEAD can communicate with third-party simulation software through distributed simulation protocols, which allows behavior models to be learned and employed using simulation systems of different fidelities.",,http://arxiv.org/abs/2304.14423v1,arxiv,,,,,,,,,pdf_cache/pdfs/958f24809657f6bc.pdf,cached,2304.144231,,http://arxiv.org/pdf/2304.14423v1,cs.LG,719679319f3acb6d809f70129d5107d9d9aa08ac522bd87d044e6d92269c59b3,,,,,,en,conference,digital,no,yes,none,none,"Performance was evaluated based on the agent's ability to achieve formation within a specified time limit, with success in 7597 out of 9000 episodes.",,6,none,no,success,1.0,,,,,
SCREEN_0185,Query-Efficient Black-Box Red Teaming via Bayesian Optimization,Deokjae Lee; JunYeong Lee; Jung-Woo Ha; Jin-Hwa Kim; Sang-Woo Lee; Hwaran Lee; Hyun Oh Song,2023,,"The deployment of large-scale generative models is often restricted by their potential risk of causing harm to users in unpredictable ways. We focus on the problem of black-box red teaming, where a red team generates test cases and interacts with the victim model to discover a diverse set of failures with limited query access. Existing red teaming methods construct test cases based on human supervision or language model (LM) and query all test cases in a brute-force manner without incorporating any information from past evaluations, resulting in a prohibitively large number of queries. To this end, we propose Bayesian red teaming (BRT), novel query-efficient black-box red teaming methods based on Bayesian optimization, which iteratively identify diverse positive test cases leading to model failures by utilizing the pre-defined user input pool and the past evaluations. Experimental results on various user input pools demonstrate that our method consistently finds a significantly larger number of diverse positive test cases under the limited query budget than the baseline methods. The source code is available at https://github.com/snu-mllab/Bayesian-Red-Teaming.",,http://arxiv.org/abs/2305.17444v1,arxiv,,,,,,,,,pdf_cache/pdfs/210d6c479f94d084.pdf,cached,2305.174441,,http://arxiv.org/pdf/2305.17444v1,cs.AI; cs.CL; cs.CR; cs.LG,0335b149c8d6210616f74e2ff84793a72c8023017f515e5a5118d60eafeb4faf,,,,,,en,tech-report,digital,no,yes,BlenderBot-3B,player,"RSR (Red Team Success Rate), Self-BLEU(k) for diversity, human evaluation for precision",false positives|offensive content generation,6,https://github.com/snu-mllab/Bayesian-Red-Teaming,yes,success,1.0,,,,human_evaluation,github.com/snu-mllab/Bayesian-Red-Teaming
SCREEN_0186,Towards best practices in AGI safety and governance: A survey of expert opinion,Jonas Schuett; Noemi Dreksler; Markus Anderljung; David McCaffary; Lennart Heim; Emma Bluemke; Ben Garfinkel,2023,,"A number of leading AI companies, including OpenAI, Google DeepMind, and Anthropic, have the stated goal of building artificial general intelligence (AGI) - AI systems that achieve or exceed human performance across a wide range of cognitive tasks. In pursuing this goal, they may develop and deploy AI systems that pose particularly significant risks. While they have already taken some measures to mitigate these risks, best practices have not yet emerged. To support the identification of best practices, we sent a survey to 92 leading experts from AGI labs, academia, and civil society and received 51 responses. Participants were asked how much they agreed with 50 statements about what AGI labs should do. Our main finding is that participants, on average, agreed with all of them. Many statements received extremely high levels of agreement. For example, 98% of respondents somewhat or strongly agreed that AGI labs should conduct pre-deployment risk assessments, dangerous capabilities evaluations, third-party model audits, safety restrictions on model usage, and red teaming. Ultimately, our list of statements may serve as a helpful foundation for efforts to develop best practices, standards, and regulations for AGI labs.",,http://arxiv.org/abs/2305.07153v1,arxiv,,,,,,,,,pdf_cache/pdfs/74d74a06ee7e685d.pdf,cached,2305.071531,,http://arxiv.org/pdf/2305.07153v1,cs.CY,bb18071c3a08ee06af0b4bf8f764cafa6ba42a8f94fb5f42e8d7dd7c31edd5e9,,,,,,en,tech-report,none,no,no,none,none,none,,6,none,yes,success,1.0,,,,,
SCREEN_0187,The Promise and Peril of Artificial Intelligence -- Violet Teaming Offers a Balanced Path Forward,Alexander J. Titus; Adam H. Russell,2023,,"Artificial intelligence (AI) promises immense benefits across sectors, yet also poses risks from dual-use potentials, biases, and unintended behaviors. This paper reviews emerging issues with opaque and uncontrollable AI systems and proposes an integrative framework called violet teaming to develop reliable and responsible AI. Violet teaming combines adversarial vulnerability probing (red teaming) with solutions for safety and security (blue teaming) while prioritizing ethics and social benefit. It emerged from AI safety research to manage risks proactively by design. The paper traces the evolution of red, blue, and purple teaming toward violet teaming, and then discusses applying violet techniques to address biosecurity risks of AI in biotechnology. Additional sections review key perspectives across law, ethics, cybersecurity, macrostrategy, and industry best practices essential for operationalizing responsible AI through holistic technical and social considerations. Violet teaming provides both philosophy and method for steering AI trajectories toward societal good. With conscience and wisdom, the extraordinary capabilities of AI can enrich humanity. But without adequate precaution, the risks could prove catastrophic. Violet teaming aims to empower moral technology for the common welfare.",,http://arxiv.org/abs/2308.14253v1,arxiv,,,,,,,,,pdf_cache/pdfs/090b02894b39a35b.pdf,cached,2308.142531,,http://arxiv.org/pdf/2308.14253v1,cs.AI; cs.CR; cs.LG,1dc013d5251bc1c06844ad1550918420dfc040ef66b98a6bf82cf76319295d11,,,,,,en,tech-report,none,no,no,none,none,none,,4,none,yes,success,1.0,,,,,
SCREEN_0188,Where's the Liability in Harmful AI Speech?,Peter Henderson; Tatsunori Hashimoto; Mark Lemley,2023,,"Generative AI, in particular text-based ""foundation models"" (large models trained on a huge variety of information including the internet), can generate speech that could be problematic under a wide range of liability regimes. Machine learning practitioners regularly ""red team"" models to identify and mitigate such problematic speech: from ""hallucinations"" falsely accusing people of serious misconduct to recipes for constructing an atomic bomb. A key question is whether these red-teamed behaviors actually present any liability risk for model creators and deployers under U.S. law, incentivizing investments in safety mechanisms. We examine three liability regimes, tying them to common examples of red-teamed model behaviors: defamation, speech integral to criminal conduct, and wrongful death. We find that any Section 230 immunity analysis or downstream liability analysis is intimately wrapped up in the technical details of algorithm design. And there are many roadblocks to truly finding models (and their associated parties) liable for generated speech. We argue that AI should not be categorically immune from liability in these scenarios and that as courts grapple with the already fine-grained complexities of platform algorithms, the technical details of generative AI loom above with thornier questions. Courts and policymakers should think carefully about what technical design incentives they create as they evaluate these issues.",,http://arxiv.org/abs/2308.04635v2,arxiv,,,,,,,,,pdf_cache/pdfs/43ac5b0a7528c110.pdf,cached,2308.046352,,http://arxiv.org/pdf/2308.04635v2,cs.CY; cs.AI,52a413ed9294411e95fdf843137743b67d889e29820f776ef38ed02e0ce336e4,,,,,,en,journal,none,no,no,ChatGPT,none,none,hallucination,6,none,no,success,1.0,hallucination,,,,
SCREEN_0189,Price-Aware Deep Learning for Electricity Markets,Vladimir Dvorkin; Ferdinando Fioretto,2023,,"While deep learning gradually penetrates operational planning, its inherent prediction errors may significantly affect electricity prices. This letter examines how prediction errors propagate into electricity prices, revealing notable pricing errors and their spatial disparity in congested power systems. To improve fairness, we propose to embed electricity market-clearing optimization as a deep learning layer. Differentiating through this layer allows for balancing between prediction and pricing errors, as oppose to minimizing prediction errors alone. This layer implicitly optimizes fairness and controls the spatial distribution of price errors across the system. We showcase the price-aware deep learning in the nexus of wind power forecasting and short-term electricity market clearing.",,http://arxiv.org/abs/2308.01436v2,arxiv,,,,,,,,,pdf_cache/pdfs/82ef463377aa68e3.pdf,cached,2308.014362,,http://arxiv.org/pdf/2308.01436v2,cs.LG; cs.SY; eess.SY; math.OC,b16f6812f6cf913f91dbe594ec1f30b8f3c7852ab942ad78e592b5ad8c4070d4,,,,,,en,workshop,none,no,yes,none,none,"Root mean square error of wind power prediction, root mean square error of prices, root mean square error across 10% of the worst-case scenarios, and α−fairness bound.",,6,none,no,success,1.0,,,,,
SCREEN_0190,Confidence-Building Measures for Artificial Intelligence: Workshop Proceedings,Sarah Shoker; Andrew Reddie; Sarah Barrington; Ruby Booth; Miles Brundage; Husanjot Chahal; Michael Depp; Bill Drexel; Ritwik Gupta; Marina Favaro; Jake Hecla; Alan Hickey; Margarita Konaev; Kirthi Kumar; Nathan Lambert; Andrew Lohn; Cullen O'Keefe; Nazneen Rajani; Michael Sellitto; Robert Trager; Leah Walker; Alexa Wehsener; Jessica Young,2023,,"Foundation models could eventually introduce several pathways for undermining state security: accidents, inadvertent escalation, unintentional conflict, the proliferation of weapons, and the interference with human diplomacy are just a few on a long list. The Confidence-Building Measures for Artificial Intelligence workshop hosted by the Geopolitics Team at OpenAI and the Berkeley Risk and Security Lab at the University of California brought together a multistakeholder group to think through the tools and strategies to mitigate the potential risks introduced by foundation models to international security. Originating in the Cold War, confidence-building measures (CBMs) are actions that reduce hostility, prevent conflict escalation, and improve trust between parties. The flexibility of CBMs make them a key instrument for navigating the rapid changes in the foundation model landscape. Participants identified the following CBMs that directly apply to foundation models and which are further explained in this conference proceedings: 1. crisis hotlines 2. incident sharing 3. model, transparency, and system cards 4. content provenance and watermarks 5. collaborative red teaming and table-top exercises and 6. dataset and evaluation sharing. Because most foundation model developers are non-government entities, many CBMs will need to involve a wider stakeholder community. These measures can be implemented either by AI labs or by relevant government actors.",,http://arxiv.org/abs/2308.00862v2,arxiv,,,,,,,,,pdf_cache/pdfs/1816e3065b52ca5c.pdf,cached,2308.008622,,http://arxiv.org/pdf/2308.00862v2,cs.CY,7c4a14f0a6fd615660193b5cdee934b7b4e85bc30df0e04dd73a5e29a5bfb64e,,,,,,en,workshop,seminar,no,no,GPT-4,generator,"The workshop focused on generating practical confidence-building measures (CBMs) for foundation models, addressing risks associated with state-to-state interactions and international security.",escalation,5,none,yes,success,1.0,escalation,,,,
SCREEN_0191,"Red Teaming Generative AI/NLP, the BB84 quantum cryptography protocol and the NIST-approved Quantum-Resistant Cryptographic Algorithms",Petar Radanliev; David De Roure; Omar Santos,2023,,"In the contemporary digital age, Quantum Computing and Artificial Intelligence (AI) convergence is reshaping the cyber landscape, introducing unprecedented opportunities and potential vulnerabilities.This research, conducted over five years, delves into the cybersecurity implications of this convergence, with a particular focus on AI/Natural Language Processing (NLP) models and quantum cryptographic protocols, notably the BB84 method and specific NIST-approved algorithms. Utilising Python and C++ as primary computational tools, the study employs a ""red teaming"" approach, simulating potential cyber-attacks to assess the robustness of quantum security measures. Preliminary research over 12 months laid the groundwork, which this study seeks to expand upon, aiming to translate theoretical insights into actionable, real-world cybersecurity solutions. Located at the University of Oxford's technology precinct, the research benefits from state-of-the-art infrastructure and a rich collaborative environment. The study's overarching goal is to ensure that as the digital world transitions to quantum-enhanced operations, it remains resilient against AI-driven cyber threats. The research aims to foster a safer, quantum-ready digital future through iterative testing, feedback integration, and continuous improvement. The findings are intended for broad dissemination, ensuring that the knowledge benefits academia and the global community, emphasising the responsible and secure harnessing of quantum technology.",,http://arxiv.org/abs/2310.04425v1,arxiv,,,,,,,,,pdf_cache/pdfs/35a9d63a6dab11bf.pdf,cached,2310.044251,,http://arxiv.org/pdf/2310.04425v1,cs.CY; cs.CR; cs.ET; cs.LG,8791c734aacca60df897cdeca650f0b0636774e834c9b2e2b5f30de6ad7b6fd1,,,,,,en,tech-report,none,no,no,none,none,none,,6,none,yes,success,1.0,,,,,
SCREEN_0192,ASA-SimaaS: Advancing Digital Transformation through Simulation Services in the Brazilian Air Force,Joao P. A. Dantas; Diego Geraldo; Andre N. Costa; Marcos R. O. A. Maximo; Takashi Yoneyama,2023,,"This work explores the use of military simulations in predicting and evaluating the outcomes of potential scenarios. It highlights the evolution of military simulations and the increased capabilities that have arisen due to the advancement of artificial intelligence. Also, it discusses the various applications of military simulations, such as developing tactics and employment doctrines, training decision-makers, evaluating new acquisitions, and developing new technologies. The paper then focuses on the Brazilian Air Force's efforts to create its own simulation tool, the Aerospace Simulation Environment (Ambiente de Simula\c{c}\~ao Aeroespacial -- ASA in Portuguese), and how this cloud-based service called ASA Simulation as a Service (ASA-SimaaS) can provide greater autonomy and economy for the military force. The main contribution of this work is to present the ASA-SimaaS solution as a means of empowering digital transformation in defense scenarios, establishing a partnership network, and improving the military's simulation capabilities and competitiveness.",,http://arxiv.org/abs/2309.08680v1,arxiv,,,,,,,,,pdf_cache/pdfs/f619c5c7887b1dc8.pdf,cached,2309.086801,,http://arxiv.org/pdf/2309.08680v1,cs.CY; cs.ET,bf43739729900769c00208243a23b2574b807ea4a320643778c01b28c112d887,,,,,,en,tech-report,digital,no,yes,none,none,"Effectiveness and efficiency indicators, including the rate of meeting demanded scenarios and the cost of simulating a single scenario.",,6,none,yes,success,1.0,,,,,
SCREEN_0193,Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts,Zhi-Yi Chin; Chieh-Ming Jiang; Ching-Chun Huang; Pin-Yu Chen; Wei-Chen Chiu,2023,,"Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown remarkable ability in high-quality content generation, and become one of the representatives for the recent wave of transformative AI. Nevertheless, such advance comes with an intensifying concern about the misuse of this generative technology, especially for producing copyrighted or NSFW (i.e. not safe for work) images. Although efforts have been made to filter inappropriate images/prompts or remove undesirable concepts/styles via model fine-tuning, the reliability of these safety mechanisms against diversified problematic prompts remains largely unexplored. In this work, we propose Prompting4Debugging (P4D) as a debugging and red-teaming tool that automatically finds problematic prompts for diffusion models to test the reliability of a deployed safety mechanism. We demonstrate the efficacy of our P4D tool in uncovering new vulnerabilities of SD models with safety mechanisms. Particularly, our result shows that around half of prompts in existing safe prompting benchmarks which were originally considered ""safe"" can actually be manipulated to bypass many deployed safety mechanisms, including concept removal, negative prompt, and safety guidance. Our findings suggest that, without comprehensive testing, the evaluations on limited safe prompting benchmarks can lead to a false sense of safety for text-to-image models.",,http://arxiv.org/abs/2309.06135v2,arxiv,,,,,,,,,pdf_cache/pdfs/524c7061cbe52934.pdf,cached,2309.061352,,http://arxiv.org/pdf/2309.06135v2,cs.CL; cs.CV,62893b95fb421f43735530f7a05f33cf05524e589ac4b10a5b6b679b172cad0d,,,,,,en,conference,digital,no,yes,none,none,"Classifier accuracy, false negative rate, false positive rate, cosine similarity between images",deception,6,https://github.com/joycenerd/P4D,no,success,1.0,deception,,,accuracy,
SCREEN_0194,Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?,Yu-Lin Tsai; Chia-Yi Hsu; Chulin Xie; Chih-Hsun Lin; Jia-You Chen; Bo Li; Pin-Yu Chen; Chia-Mu Yu; Chun-Ying Huang,2023,,"Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential misuse, particularly in creating copyrighted, prohibited, and restricted content, or NSFW (not safe for work) images. While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored. In this work, we aim to investigate these safety mechanisms by proposing one novel concept retrieval algorithm for evaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming tool for T2I diffusion models, where the whole evaluation can be prepared in advance without prior knowledge of the target model. Specifically, Ring-A-Bell first performs concept extraction to obtain holistic representations for sensitive and inappropriate concepts. Subsequently, by leveraging the extracted concept, Ring-A-Bell automatically identifies problematic prompts for diffusion models with the corresponding generation of inappropriate content, allowing the user to assess the reliability of deployed safety mechanisms. Finally, we empirically validate our method by testing online services such as Midjourney and various methods of concept removal. Our results show that Ring-A-Bell, by manipulating safe prompting benchmarks, can transform prompts that were originally regarded as safe to evade existing safety mechanisms, thus revealing the defects of the so-called safety mechanisms which could practically lead to the generation of harmful contents. Our codes are available at https://github.com/chiayi-hsu/Ring-A-Bell.",,http://arxiv.org/abs/2310.10012v4,arxiv,,,,,,,,,pdf_cache/pdfs/76c03441951512eb.pdf,cached,2310.100124,,http://arxiv.org/pdf/2310.10012v4,cs.LG,31d1efb16708de2641f86a54933e31410399f186502dda1d215445759ec61693,,,,,,en,conference,digital,no,yes,none,none,Attack Success Rate (ASR) of Ring-A-Bell under different values of K,deception,6,https://github.com/chiayi-hsu/Ring-A-Bell,no,success,1.0,deception,,,,github.com/chiayi-hsu/Ring-A-Bell
SCREEN_0195,No Offense Taken: Eliciting Offensiveness from Language Models,Anugya Srivastava; Rahul Ahuja; Rohith Mukku,2023,,"This work was completed in May 2022. For safe and reliable deployment of language models in the real world, testing needs to be robust. This robustness can be characterized by the difficulty and diversity of the test cases we evaluate these models on. Limitations in human-in-the-loop test case generation has prompted an advent of automated test case generation approaches. In particular, we focus on Red Teaming Language Models with Language Models by Perez et al.(2022). Our contributions include developing a pipeline for automated test case generation via red teaming that leverages publicly available smaller language models (LMs), experimenting with different target LMs and red classifiers, and generating a corpus of test cases that can help in eliciting offensive responses from widely deployed LMs and identifying their failure modes.",,http://arxiv.org/abs/2310.00892v1,arxiv,,,,,,,,,pdf_cache/pdfs/4a4c57489f09790f.pdf,cached,2310.008921,,http://arxiv.org/pdf/2310.00892v1,cs.CL; cs.AI,f67ace366fb35a608c9e3027adb24f0b2abf28e832e3d25a77d2ff68856f6dc3,,,,,,en,tech-report,digital,yes,yes,GPT-2,generator,Self-BLEU score for diversity and percentage of generated test cases that led to harmful responses.,bias|toxicity,6,none,yes,success,1.0,,,,,
SCREEN_0196,InfoPattern: Unveiling Information Propagation Patterns in Social Media,Chi Han; Jialiang Xu; Manling Li; Hanning Zhang; Tarek Abdelzaher; Heng Ji,2023,,Social media play a significant role in shaping public opinion and influencing ideological communities through information propagation. Our demo InfoPattern centers on the interplay between language and human ideology. The demo (Code: https://github.com/blender-nlp/InfoPattern ) is capable of: (1) red teaming to simulate adversary responses from opposite ideology communities; (2) stance detection to identify the underlying political sentiments in each message; (3) information propagation graph discovery to reveal the evolution of claims across various communities over time. (Live Demo: https://incas.csl.illinois.edu/blender/About ),,http://arxiv.org/abs/2311.15642v1,arxiv,,,,,,,,,pdf_cache/pdfs/5b955364f98ba794.pdf,cached,2311.156421,,http://arxiv.org/pdf/2311.15642v1,cs.SI; cs.CL,89dd4f0a0325d8d490c5d39387135239256ba69d0ba3288d2c6f05f69c6ce57b,,,,,,en,tech-report,digital,yes,no,GPT-J-6B,generator,"Silhouette Score for clustering, likelihood for stance detection",,4,https://github.com/blender-nlp/InfoPattern,yes,success,1.0,,,,,github.com/blender-nlp/InfoPattern
SCREEN_0197,Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models,Zhaowei Zhu; Jialu Wang; Hao Cheng; Yang Liu,2023,,"Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we find and fix an average of 6.16% label errors in 11 datasets constructed from the above benchmarks. The data credibility and downstream learning performance can be remarkably improved by directly fixing label errors, indicating the significance of cleaning existing real-world datasets. We provide an open-source tool, Docta, for data cleaning at https://github.com/Docta-ai/docta.",,http://arxiv.org/abs/2311.11202v2,arxiv,,,,,,,,,pdf_cache/pdfs/d67873356abb3645.pdf,cached,2311.112022,,http://arxiv.org/pdf/2311.11202v2,cs.LG; cs.AI; cs.CL; cs.CY,96c21870ea8c36c062e2706fbe89c902e76fc2ca25879d4c97f3d0f364e83d31,,,,,,en,conference,none,no,no,none,none,Data credibility measured by the distance between the transition matrix and the identity matrix.,,6,https://github.com/Docta-ai/docta,no,success,1.0,,,,,github.com/Docta-ai/docta
SCREEN_0198,JAB: Joint Adversarial Prompting and Belief Augmentation,Ninareh Mehrabi; Palash Goyal; Anil Ramakrishna; Jwala Dhamala; Shalini Ghosh; Richard Zemel; Kai-Wei Chang; Aram Galstyan; Rahul Gupta,2023,,"With the recent surge of language models in different applications, attention to safety and robustness of these models has gained significant importance. Here we introduce a joint framework in which we simultaneously probe and improve the robustness of a black-box target model via adversarial prompting and belief augmentation using iterative feedback loops. This framework utilizes an automated red teaming approach to probe the target model, along with a belief augmenter to generate instructions for the target model to improve its robustness to those adversarial probes. Importantly, the adversarial model and the belief generator leverage the feedback from past interactions to improve the effectiveness of the adversarial prompts and beliefs, respectively. In our experiments, we demonstrate that such a framework can reduce toxic content generation both in dynamic cases where an adversary directly interacts with a target model and static cases where we use a static benchmark dataset to evaluate our model.",,http://arxiv.org/abs/2311.09473v1,arxiv,,,,,,,,,pdf_cache/pdfs/a2578c2d0e91cb0b.pdf,cached,2311.094731,,http://arxiv.org/pdf/2311.09473v1,cs.AI; cs.CL,33ee09ef9e16ee052a9af16408bedf7d1e7421a56ee914ade18f35e8b30116a5,,,,,,en,workshop,digital,yes,yes,GPT-4,generator,"Reduction in toxic content generation, percentage of toxic generations in static benchmarks",,5,none,yes,success,1.0,,,,,
SCREEN_0199,On the complexity of sabotage games for network security,Dhananjay Raju; Georgios Bakirtzis; Ufuk Topcu,2023,,"Securing dynamic networks against adversarial actions is challenging because of the need to anticipate and counter strategic disruptions by adversarial entities within complex network structures. Traditional game-theoretic models, while insightful, often fail to model the unpredictability and constraints of real-world threat assessment scenarios. We refine sabotage games to reflect the realistic limitations of the saboteur and the network operator. By transforming sabotage games into reachability problems, our approach allows applying existing computational solutions to model realistic restrictions on attackers and defenders within the game. Modifying sabotage games into dynamic network security problems successfully captures the nuanced interplay of strategy and uncertainty in dynamic network security. Theoretically, we extend sabotage games to model network security contexts and thoroughly explore if the additional restrictions raise their computational complexity, often the bottleneck of game theory in practical contexts. Practically, this research sets the stage for actionable insights for developing robust defense mechanisms by understanding what risks to mitigate in dynamically changing networks under threat.",,http://arxiv.org/abs/2312.13132v1,arxiv,,,,,,,,,pdf_cache/pdfs/b0cfc77b4e2e4715.pdf,cached,2312.131321,,http://arxiv.org/pdf/2312.13132v1,cs.CC; cs.CR,2eb0017a91e41527b4a6e693d8e4c58bcd3aa9af155ea32aed6a5dd3833900cb,,,,,,en,tech-report,digital,no,yes,none,none,Computational complexity classifications such as PSPACE-complete and EXPTIME-complete for different game scenarios.,,6,none,yes,success,1.0,,,,,
SCREEN_0200,Data-driven Estimation of Under Frequency Load Shedding after Outages in Small Power Systems,Mohammad Rajabdorri; Lukas Sigrist; Enrique Lobato; Matthias C. M. Troffaes; Behzad Kazemtabrizi,2023,,"This paper presents a data-driven methodology for estimating Under Frequency Load Shedding (UFLS) in small power systems. UFLS plays a vital role in maintaining system stability by shedding load when the frequency drops below a specified threshold following loss of generation. Using a dynamic System Frequency Response (SFR) model we generate different values of UFLS (i.e., labels) predicated on a set of carefully selected operating conditions (i.e., features). Machine Learning (ML) algorithms are then applied to learn the relationship between chosen features and the UFLS labels. A novel regression tree and the Tobit model are suggested for this purpose and we show how the resulting non-linear model can be directly incorporated into a Mixed Integer Linear Programming (MILP) problem. The trained model can be used to estimate UFLS in security-constrained operational planning problems, improving frequency response, optimizing reserve allocation, and reducing costs. The methodology is applied to the La Palma island power system, demonstrating its accuracy and effectiveness. The results confirm that the amount of UFLS can be estimated with the Mean Absolute Error (MAE) as small as 0.213 MW for the whole process, with a model that is representable as a MILP for use in scheduling problems such as unit commitment among others.",,http://arxiv.org/abs/2312.11389v2,arxiv,,,,,,,,,pdf_cache/pdfs/09c8a01e8cd41e5e.pdf,cached,2312.113892,,http://arxiv.org/pdf/2312.11389v2,eess.SY; cs.SY,21e78b1deab318dd7ef0c818285053ab02900b426d70352da7f3563198efdf09,,,,,,en,journal,none,no,yes,none,none,Mean Absolute Error (MAE),,6,none,no,success,1.0,,,,accuracy,
SCREEN_0201,Control Risk for Potential Misuse of Artificial Intelligence in Science,Jiyan He; Weitao Feng; Yaosen Min; Jingwei Yi; Kunsheng Tang; Shuai Li; Jie Zhang; Kejiang Chen; Wenbo Zhou; Xing Xie; Weiming Zhang; Nenghai Yu; Shuxin Zheng,2023,,"The expanding application of Artificial Intelligence (AI) in scientific fields presents unprecedented opportunities for discovery and innovation. However, this growth is not without risks. AI models in science, if misused, can amplify risks like creation of harmful substances, or circumvention of established regulations. In this study, we aim to raise awareness of the dangers of AI misuse in science, and call for responsible AI development and use in this domain. We first itemize the risks posed by AI in scientific contexts, then demonstrate the risks by highlighting real-world examples of misuse in chemical science. These instances underscore the need for effective risk management strategies. In response, we propose a system called SciGuard to control misuse risks for AI models in science. We also propose a red-teaming benchmark SciMT-Safety to assess the safety of different systems. Our proposed SciGuard shows the least harmful impact in the assessment without compromising performance in benign tests. Finally, we highlight the need for a multidisciplinary and collaborative effort to ensure the safe and ethical use of AI models in science. We hope that our study can spark productive discussions on using AI ethically in science among researchers, practitioners, policymakers, and the public, to maximize benefits and minimize the risks of misuse.",,http://arxiv.org/abs/2312.06632v1,arxiv,,,,,,,,,pdf_cache/pdfs/ef77513e422f95ff.pdf,cached,2312.066321,,http://arxiv.org/pdf/2312.06632v1,cs.AI,aa9e47d26b2cedd3c022121cb426ac01479a233e3296ad97ea7d8350770e0866,,,,,,en,tech-report,none,no,no,GPT-4,analyst,"Harmlessness and helpfulness scores, with criteria derived from prior work.",,6,https://github.com/SciMT/SciMT-benchmark,yes,success,1.0,,,,,
SCREEN_0202,Seamless: Multilingual Expressive and Streaming Speech Translation,Seamless Communication; Loïc Barrault; Yu-An Chung; Mariano Coria Meglioli; David Dale; Ning Dong; Mark Duppenthaler; Paul-Ambroise Duquenne; Brian Ellis; Hady Elsahar; Justin Haaheim; John Hoffman; Min-Jae Hwang; Hirofumi Inaguma; Christopher Klaiber; Ilia Kulikov; Pengwei Li; Daniel Licht; Jean Maillard; Ruslan Mavlyutov; Alice Rakotoarison; Kaushik Ram Sadagopan; Abinesh Ramakrishnan; Tuan Tran; Guillaume Wenzek; Yilin Yang; Ethan Ye; Ivan Evtimov; Pierre Fernandez; Cynthia Gao; Prangthip Hansanti; Elahe Kalbassi; Amanda Kallet; Artyom Kozhevnikov; Gabriel Mejia Gonzalez; Robin San Roman; Christophe Touret; Corinne Wong; Carleigh Wood; Bokai Yu; Pierre Andrews; Can Balioglu; Peng-Jen Chen; Marta R. Costa-jussà; Maha Elbayad; Hongyu Gong; Francisco Guzmán; Kevin Heffernan; Somya Jain; Justine Kao; Ann Lee; Xutai Ma; Alex Mourachko; Benjamin Peloquin; Juan Pino; Sravya Popuri; Christophe Ropers; Safiyyah Saleem; Holger Schwenk; Anna Sun; Paden Tomasello; Changhan Wang; Jeff Wang; Skyler Wang; Mary Williamson,2023,,"Large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model-SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. SeamlessM4T v2 provides the foundation on which our next two models are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one's voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. The contributions to this work are publicly released and accessible at https://github.com/facebookresearch/seamless_communication",,http://arxiv.org/abs/2312.05187v1,arxiv,,,,,,,,,pdf_cache/pdfs/527d3b66d297cdfc.pdf,cached,2312.051871,,http://arxiv.org/pdf/2312.05187v1,cs.CL; cs.SD; eess.AS,0cdd32197ea09ea3625d45c6573c54d181c532d67d6a5c1075a2b7cc92001474,,,,,,en,tech-report,digital,no,yes,none,none,"BLEU, Average Lagging (AL), Length-Adaptive Average Lagging (LAAL), Word Error Rate (WER), Mean Opinion Score (MOS), Prosodic Consistency Protocol (PCP), Ending Offset",bias,6,https://github.com/facebookresearch/seamless_communication,yes,success,1.0,bias,,,,github.com/facebookresearch/seamless_communication
SCREEN_0203,A Red Teaming Framework for Securing AI in Maritime Autonomous Systems,Mathew J. Walter; Aaron Barrett; Kimberly Tam,2023,,"Artificial intelligence (AI) is being ubiquitously adopted to automate processes in science and industry. However, due to its often intricate and opaque nature, AI has been shown to possess inherent vulnerabilities which can be maliciously exploited with adversarial AI, potentially putting AI users and developers at both cyber and physical risk. In addition, there is insufficient comprehension of the real-world effects of adversarial AI and an inadequacy of AI security examinations; therefore, the growing threat landscape is unknown for many AI solutions. To mitigate this issue, we propose one of the first red team frameworks for evaluating the AI security of maritime autonomous systems. The framework provides operators with a proactive (secure by design) and reactive (post-deployment evaluation) response to securing AI technology today and in the future. This framework is a multi-part checklist, which can be tailored to different systems and requirements. We demonstrate this framework to be highly effective for a red team to use to uncover numerous vulnerabilities within a real-world maritime autonomous systems AI, ranging from poisoning to adversarial patch attacks. The lessons learned from systematic AI red teaming can help prevent MAS-related catastrophic events in a world with increasing uptake and reliance on mission-critical AI.",,http://arxiv.org/abs/2312.11500v1,arxiv,,,,,,,,,pdf_cache/pdfs/0de8326db383c862.pdf,cached,2312.115001,,http://arxiv.org/pdf/2312.11500v1,cs.CR; cs.AI,69b0bb850807da63fbf3c89444a714e82d639819fbd6fc79556d7cce33b04e3b,,,,,,en,tech-report,none,no,no,none,none,The paper does not specify evaluation metrics related to wargames.,,6,none,yes,success,1.0,,,,,
SCREEN_0204,Metis: Multi-Agent Based Crisis Simulation System,George Sidiropoulos; Chairi Kiourt; Lefteris Moussiades,2020,,"With the advent of the computational technologies (Graphics Processing Units - GPUs) and Machine Learning, the research domain of crowd simulation for crisis management has flourished. Along with the new techniques and methodologies that have been proposed all those years, aiming to increase the realism of crowd simulation, several crisis simulation systems/tools have been developed, but most of them focus on special cases without providing users the ability to adapt them based on their needs. Towards these directions, in this paper, we introduce a novel multi-agent-based crisis simulation system for indoor cases. The main advantage of the system is its ease of use feature, focusing on non-expert users (users with little to no programming skills) that can exploit its capabilities a, adapt the entire environment based on their needs (Case studies) and set up building evacuation planning experiments with some of the most popular Reinforcement Learning algorithms. Simply put, the system's features focus on dynamic environment design and crisis management, interconnection with popular Reinforcement Learning libraries, agents with different characteristics (behaviors), fire propagation parameterization, realistic physics based on popular game engine, GPU-accelerated agents training and simulation end conditions. A case study exploiting a popular reinforcement learning algorithm, for training of the agents, presents the dynamics and the capabilities of the proposed systems and the paper is concluded with the highlights of the system and some future directions.",,http://arxiv.org/abs/2009.03934v1,arxiv,,,,,,,,,pdf_cache/pdfs/d6e92f0c8f2ecd90.pdf,cached,2009.039341,,http://arxiv.org/pdf/2009.03934v1,cs.MA,692bb6d9969bafb2dde8818a43e48d6de82d98e0df18c41720669c51d80f3549,,,,,,en,workshop,digital,no,yes,none,none,Accuracy in training and evacuation success rate,,6,none,no,success,1.0,,,,accuracy,
SCREEN_0205,Gotham Testbed: a Reproducible IoT Testbed for Security Experiments and Dataset Generation,Xabier Sáez-de-Cámara; Jose Luis Flores; Cristóbal Arellano; Aitor Urbieta; Urko Zurutuza,2022,,"The growing adoption of the Internet of Things (IoT) has brought a significant increase in attacks targeting those devices. Machine learning (ML) methods have shown promising results for intrusion detection; however, the scarcity of IoT datasets remains a limiting factor in developing ML-based security systems for IoT scenarios. Static datasets get outdated due to evolving IoT architectures and threat landscape; meanwhile, the testbeds used to generate them are rarely published. This paper presents the Gotham testbed, a reproducible and flexible security testbed extendable to accommodate new emulated devices, services or attackers. Gotham is used to build an IoT scenario composed of 100 emulated devices communicating via MQTT, CoAP and RTSP protocols, among others, in a topology composed of 30 switches and 10 routers. The scenario presents three threat actors, including the entire Mirai botnet lifecycle and additional red-teaming tools performing DoS, scanning, and attacks targeting IoT protocols. The testbed has many purposes, including a cyber range, testing security solutions, and capturing network and application data to generate datasets. We hope that researchers can leverage and adapt Gotham to include other devices, state-of-the-art attacks and topologies to share scenarios and datasets that reflect the current IoT settings and threat landscape.",10.1109/TDSC.2023.3247166,http://arxiv.org/abs/2207.13981v3,arxiv,,,,,,,,,pdf_cache/pdfs/10_1109_TDSC_2023_3247166.pdf,cached,2207.139813,,http://arxiv.org/pdf/2207.13981v3,cs.CR,12a3339d58b37a2bb0864d612de5a4bf3f622877368b25bd3302b8672b2d5738,,,,,,en,journal,none,no,no,none,none,none,,6,https://github.com/xsaga/gotham-iot-testbed,no,success,1.0,,,,,
SCREEN_0206,No-Regret Learning in Games is Turing Complete,Gabriel P. Andrade; Rafael Frongillo; Georgios Piliouras,2022,,"Games are natural models for multi-agent machine learning settings, such as generative adversarial networks (GANs). The desirable outcomes from algorithmic interactions in these games are encoded as game theoretic equilibrium concepts, e.g. Nash and coarse correlated equilibria. As directly computing an equilibrium is typically impractical, one often aims to design learning algorithms that iteratively converge to equilibria. A growing body of negative results casts doubt on this goal, from non-convergence to chaotic and even arbitrary behaviour. In this paper we add a strong negative result to this list: learning in games is Turing complete. Specifically, we prove Turing completeness of the replicator dynamic on matrix games, one of the simplest possible settings. Our results imply the undecicability of reachability problems for learning algorithms in games, a special case of which is determining equilibrium convergence.",,http://arxiv.org/abs/2202.11871v1,arxiv,,,,,,,,,pdf_cache/pdfs/daba3b58ea18b70d.pdf,cached,2202.118711,,http://arxiv.org/pdf/2202.11871v1,cs.GT; cs.LG; math.DS,4aaa6758b6ce8d6ccc89d68ca64ff7353e57f1a417f1d99db8dc3c0ca832e9ac,,,,,,en,journal,matrix,no,yes,none,none,The paper does not specify evaluation metrics as it is theoretical in nature.,,6,none,no,success,1.0,,,,,
SCREEN_0207,Cooperative Artificial Intelligence,Tobias Baumann,2022,,"In the future, artificial learning agents are likely to become increasingly widespread in our society. They will interact with both other learning agents and humans in a variety of complex settings including social dilemmas. We argue that there is a need for research on the intersection between game theory and artificial intelligence, with the goal of achieving cooperative artificial intelligence that can navigate social dilemmas well. We consider the problem of how an external agent can promote cooperation between artificial learners by distributing additional rewards and punishments based on observing the actions of the learners. We propose a rule for automatically learning how to create the right incentives by considering the anticipated parameter updates of each agent. Using this learning rule leads to cooperation with high social welfare in matrix games in which the agents would otherwise learn to defect with high probability. We show that the resulting cooperative outcome is stable in certain games even if the planning agent is turned off after a given number of episodes, while other games require ongoing intervention to maintain mutual cooperation. Finally, we reflect on what the goals of multi-agent reinforcement learning should be in the first place, and discuss the necessary building blocks towards the goal of building cooperative AI.",,http://arxiv.org/abs/2202.09859v1,arxiv,,,,,,,,,pdf_cache/pdfs/df9a2dd53b25ca32.pdf,cached,2202.098591,,http://arxiv.org/pdf/2202.09859v1,cs.AI; cs.GT; cs.MA,4c302cd5a036e69e7cfd871767ec317979b3ff2daa77b23e350f10424797a820,,,,,,en,tech-report,matrix,no,yes,none,none,"Levels of cooperation, mean total reward, mean reward per player, fraction of picked up coins of the right color",,6,none,yes,success,1.0,,,,,
SCREEN_0208,Red Teaming Language Models with Language Models,Ethan Perez; Saffron Huang; Francis Song; Trevor Cai; Roman Ring; John Aslanides; Amelia Glaese; Nat McAleese; Geoffrey Irving,2022,,"Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (""red teaming"") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.",,http://arxiv.org/abs/2202.03286v1,arxiv,,,,,,,,,pdf_cache/pdfs/90c7357a5b0467b2.pdf,cached,2202.032861,,http://arxiv.org/pdf/2202.03286v1,cs.CL; cs.AI; cs.CR; cs.LG,9d55d0ac9691ff8b9aee8e472dcab867d6650d36b0ae02f94865e3cea3068d51,,,,,,en,tech-report,digital,yes,no,Gopher,generator,Classifier trained to detect offensive content,data leakage|distributional bias|inappropriate content generation|offensive language|prompt_sensitivity,4,none,yes,success,1.0,prompt_sensitivity,,,,
SCREEN_0209,"Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",Deep Ganguli; Liane Lovitt; Jackson Kernion; Amanda Askell; Yuntao Bai; Saurav Kadavath; Ben Mann; Ethan Perez; Nicholas Schiefer; Kamal Ndousse; Andy Jones; Sam Bowman; Anna Chen; Tom Conerly; Nova DasSarma; Dawn Drain; Nelson Elhage; Sheer El-Showk; Stanislav Fort; Zac Hatfield-Dodds; Tom Henighan; Danny Hernandez; Tristan Hume; Josh Jacobson; Scott Johnston; Shauna Kravec; Catherine Olsson; Sam Ringer; Eli Tran-Johnson; Dario Amodei; Tom Brown; Nicholas Joseph; Sam McCandlish; Chris Olah; Jared Kaplan; Jack Clark,2022,,"We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.",,http://arxiv.org/abs/2209.07858v2,arxiv,,,,,,,,,pdf_cache/pdfs/d493f487f7930fca.pdf,cached,2209.078582,,http://arxiv.org/pdf/2209.07858v2,cs.CL; cs.AI; cs.CY,4396213de970d6fcdf9f6659056ad0d8cde5ee88cc522713ae9918105496b794,,,,,,en,tech-report,digital,yes,no,"Anthropic models (2.7B, 13B, 52B parameters)",player,"Attack success measured by average red team member self-report, average minimum harmlessness score, and distribution of minimum harmlessness score.",aiding in disinformation campaigns|generating extremist texts|leaking personally identifiable information|offensive language|reinforcing social biases|spreading falsehoods,5,https://github.com/anthropics/hh-rlhf,yes,success,1.0,,,,,
SCREEN_0210,CTI4AI: Threat Intelligence Generation and Sharing after Red Teaming AI Models,Chuyen Nguyen; Caleb Morgan; Sudip Mittal,2022,,"As the practicality of Artificial Intelligence (AI) and Machine Learning (ML) based techniques grow, there is an ever increasing threat of adversarial attacks. There is a need to red team this ecosystem to identify system vulnerabilities, potential threats, characterize properties that will enhance system robustness, and encourage the creation of effective defenses. A secondary need is to share this AI security threat intelligence between different stakeholders like, model developers, users, and AI/ML security professionals. In this paper, we create and describe a prototype system CTI4AI, to overcome the need to methodically identify and share AI/ML specific vulnerabilities and threat intelligence.",,http://arxiv.org/abs/2208.07476v1,arxiv,,,,,,,,,pdf_cache/pdfs/18ff3d39f3049ba5.pdf,cached,2208.074761,,http://arxiv.org/pdf/2208.07476v1,cs.CR; cs.AI; cs.LG,e696c4876e2fce64730f61d726a8e60c29223aca4e4f2dcc44cd5c925d97c38f,,,,,,en,tech-report,digital,no,yes,none,none,Accuracy difference imposed by FGM on ResNet-50 model,,6,none,yes,success,1.0,,,,accuracy,
SCREEN_0211,Red Teaming with Mind Reading: White-Box Adversarial Policies Against RL Agents,Stephen Casper; Taylor Killian; Gabriel Kreiman; Dylan Hadfield-Menell,2022,,"Adversarial examples can be useful for identifying vulnerabilities in AI systems before they are deployed. In reinforcement learning (RL), adversarial policies can be developed by training an adversarial agent to minimize a target agent's rewards. Prior work has studied black-box versions of these attacks where the adversary only observes the world state and treats the target agent as any other part of the environment. However, this does not take into account additional structure in the problem. In this work, we study white-box adversarial policies and show that having access to a target agent's internal state can be useful for identifying its vulnerabilities. We make two contributions. (1) We introduce white-box adversarial policies where an attacker observes both a target's internal state and the world state at each timestep. We formulate ways of using these policies to attack agents in 2-player games and text-generating language models. (2) We demonstrate that these policies can achieve higher initial and asymptotic performance against a target agent than black-box controls. Code is available at https://github.com/thestephencasper/lm_white_box_attacks",,http://arxiv.org/abs/2209.02167v3,arxiv,,,,,,,,,pdf_cache/pdfs/9999ff0a774685b2.pdf,cached,2209.021673,,http://arxiv.org/pdf/2209.02167v3,cs.AI; cs.CR; cs.LG,b47fcf8d93593e67860561605b20c6b093c9c2bbbbb22b43aa00754f3daafb44,,,,,,en,tech-report,digital,no,yes,GPT-2,player,Performance comparison of white-box adversarial attacks with conventional attacks in two-player games and language model red teaming.,,6,https://github.com/thestephencasper/lm_white_box_attacks,yes,success,1.0,,,,,github.com/thestephencasper/lm_white_box_attacks
SCREEN_0212,Learning Explicit Credit Assignment for Cooperative Multi-Agent Reinforcement Learning via Polarization Policy Gradient,Wubing Chen; Wenbin Li; Xiao Liu; Shangdong Yang; Yang Gao,2022,,"Cooperative multi-agent policy gradient (MAPG) algorithms have recently attracted wide attention and are regarded as a general scheme for the multi-agent system. Credit assignment plays an important role in MAPG and can induce cooperation among multiple agents. However, most MAPG algorithms cannot achieve good credit assignment because of the game-theoretic pathology known as \textit{centralized-decentralized mismatch}. To address this issue, this paper presents a novel method, \textit{\underline{M}ulti-\underline{A}gent \underline{P}olarization \underline{P}olicy \underline{G}radient} (MAPPG). MAPPG takes a simple but efficient polarization function to transform the optimal consistency of joint and individual actions into easily realized constraints, thus enabling efficient credit assignment in MAPG. Theoretically, we prove that individual policies of MAPPG can converge to the global optimum. Empirically, we evaluate MAPPG on the well-known matrix game and differential game, and verify that MAPPG can converge to the global optimum for both discrete and continuous action spaces. We also evaluate MAPPG on a set of StarCraft II micromanagement tasks and demonstrate that MAPPG outperforms the state-of-the-art MAPG algorithms.",,http://arxiv.org/abs/2210.05367v2,arxiv,,,,,,,,,pdf_cache/pdfs/18d58507a0cc8d48.pdf,cached,2210.053672,,http://arxiv.org/pdf/2210.05367v2,cs.LG,0a778d57cdaccdb7d61b7a008da87d966502217de694126898cf132d922a6ab7,,,,,,en,journal,matrix,no,yes,none,none,"Convergence to global optimum, performance on StarCraft II micromanagement tasks, ablation studies",,6,none,no,success,1.0,,,matrix,,
SCREEN_0213,AiCEF: An AI-assisted Cyber Exercise Content Generation Framework Using Named Entity Recognition,Alexandros Zacharis; Constantinos Patsakis,2022,,"Content generation that is both relevant and up to date with the current threats of the target audience is a critical element in the success of any Cyber Security Exercise (CSE). Through this work, we explore the results of applying machine learning techniques to unstructured information sources to generate structured CSE content. The corpus of our work is a large dataset of publicly available cyber security articles that have been used to predict future threats and to form the skeleton for new exercise scenarios. Machine learning techniques, like named entity recognition (NER) and topic extraction, have been utilised to structure the information based on a novel ontology we developed, named Cyber Exercise Scenario Ontology (CESO). Moreover, we used clustering with outliers to classify the generated extracted data into objects of our ontology. Graph comparison methodologies were used to match generated scenario fragments to known threat actors' tactics and help enrich the proposed scenario accordingly with the help of synthetic text generators. CESO has also been chosen as the prominent way to express both fragments and the final proposed scenario content by our AI-assisted Cyber Exercise Framework (AiCEF). Our methodology was put to test by providing a set of generated scenarios for evaluation to a group of experts to be used as part of a real-world awareness tabletop exercise.",,http://arxiv.org/abs/2211.10806v1,arxiv,,,,,,,,,pdf_cache/pdfs/a6036ca9588bdf69.pdf,cached,2211.108061,,http://arxiv.org/pdf/2211.10806v1,cs.CR,8f89d6699dfddeaa732257130bf2e5055699d0b374b4d134726cf0f9b7c15cba,,,,,,en,tech-report,digital,no,yes,none,generator,"Improvement in speed and quality of CSE generation, relevance to current threat landscape, evaluation by an Ad-hoc Cyber Awareness Expert Group",,6,none,yes,success,1.0,,,,,
SCREEN_0214,Anticipatory Fictitious Play,Alex Cloud; Albert Wang; Wesley Kerr,2022,,"Fictitious play is an algorithm for computing Nash equilibria of matrix games. Recently, machine learning variants of fictitious play have been successfully applied to complicated real-world games. This paper presents a simple modification of fictitious play which is a strict improvement over the original: it has the same theoretical worst-case convergence rate, is equally applicable in a machine learning context, and enjoys superior empirical performance. We conduct an extensive comparison of our algorithm with fictitious play, proving an optimal convergence rate for certain classes of games, demonstrating superior performance numerically across a variety of games, and concluding with experiments that extend these algorithms to the setting of deep multiagent reinforcement learning.",,http://arxiv.org/abs/2212.09941v1,arxiv,,,,,,,,,pdf_cache/pdfs/64f5f55f8903f6b3.pdf,cached,2212.099411,,http://arxiv.org/pdf/2212.09941v1,cs.GT; cs.MA,f9951142af79dc289e298905fd1e41fa9f7f9e65833a2bcd7657437b76acb302,,,,,,en,tech-report,matrix,no,yes,none,none,"Convergence rate to Nash equilibrium, exploitability reduction",,6,none,yes,success,1.0,,,,,
SCREEN_0215,Automating Privilege Escalation with Deep Reinforcement Learning,Kalle Kujanpää; Willie Victor; Alexander Ilin,2021,,"AI-based defensive solutions are necessary to defend networks and information assets against intelligent automated attacks. Gathering enough realistic data for training machine learning-based defenses is a significant practical challenge. An intelligent red teaming agent capable of performing realistic attacks can alleviate this problem. However, there is little scientific evidence demonstrating the feasibility of fully automated attacks using machine learning. In this work, we exemplify the potential threat of malicious actors using deep reinforcement learning to train automated agents. We present an agent that uses a state-of-the-art reinforcement learning algorithm to perform local privilege escalation. Our results show that the autonomous agent can escalate privileges in a Windows 7 environment using a wide variety of different techniques depending on the environment configuration it encounters. Hence, our agent is usable for generating realistic attack sensor data for training and evaluating intrusion detection systems.",10.1145/3474369.3486877,http://arxiv.org/abs/2110.01362v1,arxiv,,,,,,,,,pdf_cache/pdfs/10_1145_3474369_3486877.pdf,cached,2110.013621,,http://arxiv.org/pdf/2110.01362v1,cs.CR; cs.LG,f108d7a269d56f3a26bd452e681b476a385531fe33b61fb9ef2fb1bbe0798a97,,,,,,en,workshop,digital,no,yes,none,none,The paper does not specify evaluation metrics related to LLMs or wargames.,escalation,6,none,no,success,1.0,escalation,,,,
SCREEN_0216,Needle in a Haystack: Detecting Subtle Malicious Edits to Additive Manufacturing G-code Files,Caleb Beckwith; Harsh Sankar Naicker; Svara Mehta; Viba R. Udupa; Nghia Tri Nim; Varun Gadre; Hammond Pearce; Gary Mac; Nikhil Gupta,2021,,"Increasing usage of Digital Manufacturing (DM) in safety-critical domains is increasing attention on the cybersecurity of the manufacturing process, as malicious third parties might aim to introduce defects in digital designs. In general, the DM process involves creating a digital object (as CAD files) before using a slicer program to convert the models into printing instructions (e.g. g-code) suitable for the target printer. As the g-code is an intermediate machine format, malicious edits may be difficult to detect, especially when the golden (original) models are not available to the manufacturer. In this work we aim to quantify this hypothesis through a red-team/blue-team case study, whereby the red-team aims to introduce subtle defects that would impact the properties (strengths) of the 3D printed parts, and the blue-team aims to detect these modifications in the absence of the golden models. The case study had two sets of models, the first with 180 designs (with 2 compromised using 2 methods) and the second with 4320 designs (with 60 compromised using 6 methods). Using statistical modelling and machine learning (ML), the blue-team was able to detect all the compromises in the first set of data, and 50 of the compromises in the second.",10.1109/LES.2021.3129108,http://arxiv.org/abs/2111.12746v1,arxiv,,,,,,,,,pdf_cache/pdfs/10_1109_LES_2021_3129108.pdf,cached,2111.127461,,http://arxiv.org/pdf/2111.12746v1,cs.CR,c36fef9b303a304e344ec522c1ce30c105e3e35607c3ec993b9371deaceaeb29,,,,,,en,tech-report,none,no,yes,none,none,"True Positive, False Positive, True Negative, False Negative for defect detection",,6,none,yes,success,1.0,,,,,
SCREEN_0217,Shifted Brownian Fluctuation Game,Song-Kyoo Kim,2021,"Mathematics 2022, 10(10), 1735","This article analyzes the behavior of a Brownian fluctuation process under a mixed strategic game setup. A variant of a compound Brownian motion has been newly proposed, which is called the Shifted Brownian Fluctuation Process to predict the turning points of a stochastic process. This compound process evolves until it reaches one step prior to the turning point. The Shifted Brownian Fluctuation Game has been constructed based on this new process to find the optimal moment of actions. Analytically tractable results are obtained by using the fluctuation theory and the mixed strategy game theory. The joint functional of the Shifted Brownian Fluctuation Process is targeted for transformation of the first passage time and its index. These results enable us to predict the moment of a turning point and the moment of actions to obtain the optimal payoffs of a game. This research adapts the theoretical framework to implement an autonomous trader for value assets including stocks and cybercurrencies.",10.3390/math10101735,http://arxiv.org/abs/2111.09311v2,arxiv,,,,,,,,,pdf_cache/pdfs/10_3390_math10101735.pdf,cached,2111.093112,,http://arxiv.org/pdf/2111.09311v2,math.PR; cs.GT,f2cb83955b8a0106a74ea05482f4d4f70724bab6729c955237f923ba1e79f28e,,,,,,en,journal,digital,no,yes,none,none,Analytically tractable results using fluctuation theory and mixed strategy game theory to predict optimal payoffs.,,6,none,no,success,1.0,,,,,
SCREEN_0218,Adaptive Synthetic Characters for Military Training,Volkan Ustun; Rajay Kumar; Adam Reilly; Seyed Sajjadi; Andrew Miller,2021,"2020 Interservice/Industry Training, Simulation, and Education
  Conference (I/ITSEC)","Behaviors of the synthetic characters in current military simulations are limited since they are generally generated by rule-based and reactive computational models with minimal intelligence. Such computational models cannot adapt to reflect the experience of the characters, resulting in brittle intelligence for even the most effective behavior models devised via costly and labor-intensive processes. Observation-based behavior model adaptation that leverages machine learning and the experience of synthetic entities in combination with appropriate prior knowledge can address the issues in the existing computational behavior models to create a better training experience in military training simulations. In this paper, we introduce a framework that aims to create autonomous synthetic characters that can perform coherent sequences of believable behavior while being aware of human trainees and their needs within a training simulation. This framework brings together three mutually complementary components. The first component is a Unity-based simulation environment - Rapid Integration and Development Environment (RIDE) - supporting One World Terrain (OWT) models and capable of running and supporting machine learning experiments. The second is Shiva, a novel multi-agent reinforcement and imitation learning framework that can interface with a variety of simulation environments, and that can additionally utilize a variety of learning algorithms. The final component is the Sigma Cognitive Architecture that will augment the behavior models with symbolic and probabilistic reasoning capabilities. We have successfully created proof-of-concept behavior models leveraging this framework on realistic terrain as an essential step towards bringing machine learning into military simulations.",,http://arxiv.org/abs/2101.02185v1,arxiv,,,,,,,,,pdf_cache/pdfs/42b943c76bbbe52b.pdf,cached,2101.021851,,http://arxiv.org/pdf/2101.02185v1,cs.AI; cs.LG,551692dd736d9bfa12e19b4146c41830316f1f54c9f805e75349acea71a3b04f,,,,,,en,conference,digital,no,yes,none,none,"Performance of agents in training simulations, competitive and collaborative dynamics, strategic control decisions, and effectiveness in combat scenarios.",,6,none,no,success,1.0,,,,,
SCREEN_0219,Automating Defense Against Adversarial Attacks: Discovery of Vulnerabilities and Application of Multi-INT Imagery to Protect Deployed Models,Josh Kalin; David Noever; Matthew Ciolino; Dominick Hambrick; Gerry Dozier,2021,,"Image classification is a common step in image recognition for machine learning in overhead applications. When applying popular model architectures like MobileNetV2, known vulnerabilities expose the model to counter-attacks, either mislabeling a known class or altering box location. This work proposes an automated approach to defend these models. We evaluate the use of multi-spectral image arrays and ensemble learners to combat adversarial attacks. The original contribution demonstrates the attack, proposes a remedy, and automates some key outcomes for protecting the model's predictions against adversaries. In rough analogy to defending cyber-networks, we combine techniques from both offensive (""red team"") and defensive (""blue team"") approaches, thus generating a hybrid protective outcome (""green team""). For machine learning, we demonstrate these methods with 3-color channels plus infrared for vehicles. The outcome uncovers vulnerabilities and corrects them with supplemental data inputs commonly found in overhead cases particularly.",,http://arxiv.org/abs/2103.15897v1,arxiv,,,,,,,,,pdf_cache/pdfs/12daa729874c9011.pdf,cached,2103.158971,,http://arxiv.org/pdf/2103.15897v1,cs.CR; cs.CV,ce1397e9cb5510e37f435e57eb84e3fe8d1508b784e8ff95d15f2fb6efaf6162,,,,,,en,tech-report,none,no,yes,none,none,"Adversarial surface evaluation, delta accuracy between original and poisoned inputs, model robustness against adversarial attacks",,6,none,yes,success,1.0,,,,accuracy,
SCREEN_0220,A VAE-Bayesian Deep Learning Scheme for Solar Generation Forecasting based on Dimensionality Reduction,Devinder Kaur; Shama Naz Islam; Md. Apel Mahmud; Md. Enamul Haque; Adnan Anwar,2021,,"The advancement of distributed generation technologies in modern power systems has led to a widespread integration of renewable power generation at customer side. However, the intermittent nature of renewable energy poses new challenges to the network operational planning with underlying uncertainties. This paper proposes a novel Bayesian probabilistic technique for forecasting renewable solar generation by addressing data and model uncertainties by integrating bidirectional long short-term memory (BiLSTM) neural networks while compressing the weight parameters using variational autoencoder (VAE). Existing Bayesian deep learning methods suffer from high computational complexities as they require to draw a large number of samples from weight parameters expressed in the form of probability distributions. The proposed method can deal with uncertainty present in model and data in a more computationally efficient manner by reducing the dimensionality of model parameters. The proposed method is evaluated using quantile loss, reconstruction error, and deterministic forecasting evaluation metrics such as root-mean square error. It is inferred from the numerical results that VAE-Bayesian BiLSTM outperforms other probabilistic and deterministic deep learning methods for solar power forecasting in terms of accuracy and computational efficiency for different sizes of the dataset.",,http://arxiv.org/abs/2103.12969v2,arxiv,,,,,,,,,pdf_cache/pdfs/6d3e6438473cd3e2.pdf,cached,2103.129692,,http://arxiv.org/pdf/2103.12969v2,cs.LG; eess.SP,b29ecbec295dbf55054ce8ed310874f186a95c039e69dc492e62fb8ef04b9f31,,,,,,en,journal,none,no,yes,none,none,"quantile loss, reconstruction error, root mean square error (RMSE), pinball losses, prediction intervals",,6,none,no,success,1.0,,,,accuracy,
SCREEN_0221,Online Double Oracle,Le Cong Dinh; Yaodong Yang; Stephen McAleer; Zheng Tian; Nicolas Perez Nieves; Oliver Slumbers; David Henry Mguni; Haitham Bou Ammar; Jun Wang,2021,Transactions on Machine Learning Research 2022,"Solving strategic games with huge action space is a critical yet under-explored topic in economics, operations research and artificial intelligence. This paper proposes new learning algorithms for solving two-player zero-sum normal-form games where the number of pure strategies is prohibitively large. Specifically, we combine no-regret analysis from online learning with Double Oracle (DO) methods from game theory. Our method -- \emph{Online Double Oracle (ODO)} -- is provably convergent to a Nash equilibrium (NE). Most importantly, unlike normal DO methods, ODO is \emph{rationale} in the sense that each agent in ODO can exploit strategic adversary with a regret bound of $\mathcal{O}(\sqrt{T k \log(k)})$ where $k$ is not the total number of pure strategies, but rather the size of \emph{effective strategy set} that is linearly dependent on the support size of the NE. On tens of different real-world games, ODO outperforms DO, PSRO methods, and no-regret algorithms such as Multiplicative Weight Update by a significant margin, both in terms of convergence rate to a NE and average payoff against strategic adversaries.",,http://arxiv.org/abs/2103.07780v5,arxiv,,,,,,,,,pdf_cache/pdfs/7b11c85ec0f22d67.pdf,cached,2103.077805,,http://arxiv.org/pdf/2103.07780v5,cs.AI; cs.GT,8081977206506b9a0dcd8f7edf714c016b3238f94a272b817ab93669139946f6,,,,,,en,journal,digital,no,yes,none,none,"convergence rate to a Nash equilibrium, average payoff against strategic adversaries",,6,none,no,success,1.0,,,,,
SCREEN_0222,Learning in Matrix Games can be Arbitrarily Complex,Gabriel P. Andrade; Rafael Frongillo; Georgios Piliouras,2021,,"A growing number of machine learning architectures, such as Generative Adversarial Networks, rely on the design of games which implement a desired functionality via a Nash equilibrium. In practice these games have an implicit complexity (e.g. from underlying datasets and the deep networks used) that makes directly computing a Nash equilibrium impractical or impossible. For this reason, numerous learning algorithms have been developed with the goal of iteratively converging to a Nash equilibrium. Unfortunately, the dynamics generated by the learning process can be very intricate and instances of training failure hard to interpret. In this paper we show that, in a strong sense, this dynamic complexity is inherent to games. Specifically, we prove that replicator dynamics, the continuous-time analogue of Multiplicative Weights Update, even when applied in a very restricted class of games -- known as finite matrix games -- is rich enough to be able to approximate arbitrary dynamical systems. Our results are positive in the sense that they show the nearly boundless dynamic modelling capabilities of current machine learning practices, but also negative in implying that these capabilities may come at the cost of interpretability. As a concrete example, we show how replicator dynamics can effectively reproduce the well-known strange attractor of Lonrenz dynamics (the ""butterfly effect"") while achieving no regret.",,http://arxiv.org/abs/2103.03405v1,arxiv,,,,,,,,,pdf_cache/pdfs/8f9e1e5a07289398.pdf,cached,2103.034051,,http://arxiv.org/pdf/2103.03405v1,cs.LG; cs.GT; math.DS; nlin.CD,a74eb3ce7a04d19c9b71b7c3c22a204dd8243ee9afe496d32169e3709b2fe261,,,,,,en,tech-report,matrix,no,yes,none,none,none,chaos|cycling|divergence,6,none,yes,success,1.0,,,,,
SCREEN_0223,Predicting Adversary Lateral Movement Patterns with Deep Learning,Nathan Danneman; James Hyde,2021,,"This paper develops a predictive model for which host, in an enterprise network, an adversary is likely to compromise next in the course of a campaign. Such a model might support dynamic monitoring or defenses. We generate data for this model using simulated networks, with hosts, users, and adversaries as first-class entities. We demonstrate the predictive accuracy of the model on out-of-sample simulated data, and validate the findings against data captured from a Red Team event on a live enterprise network",,http://arxiv.org/abs/2104.13195v1,arxiv,,,,,,,,,pdf_cache/pdfs/a72d86e862ea5996.pdf,cached,2104.131951,,http://arxiv.org/pdf/2104.13195v1,cs.CR; cs.LG,db2177ed11bcbea3b7971670cdd8be7f3e869c802bae0320d8938ebd1db9fe4f,,,,,,en,tech-report,digital,no,yes,none,none,Predictive accuracy on out-of-sample simulated data and validation against Red Team event data,,6,none,yes,success,1.0,,,,accuracy,
SCREEN_0224,CybORG: A Gym for the Development of Autonomous Cyber Agents,Maxwell Standen; Martin Lucas; David Bowman; Toby J. Richer; Junae Kim; Damian Marriott,2021,,"Autonomous Cyber Operations (ACO) involves the development of blue team (defender) and red team (attacker) decision-making agents in adversarial scenarios. To support the application of machine learning algorithms to solve this problem, and to encourage researchers in this field to attend to problems in the ACO setting, we introduce CybORG, a work-in-progress gym for ACO research. CybORG features a simulation and emulation environment with a common interface to facilitate the rapid training of autonomous agents that can then be tested on real-world systems. Initial testing demonstrates the feasibility of this approach.",,http://arxiv.org/abs/2108.09118v1,arxiv,,,,,,,,,pdf_cache/pdfs/b0c4a6a833853310.pdf,cached,2108.091181,,http://arxiv.org/pdf/2108.09118v1,cs.CR,69fc40a8ca959adb3333a2716fa4bc6f52bf1e3bebf76129fcf42fe9891ed783,,,,,,en,workshop,digital,no,yes,none,none,Success rate of RL agents in emulator; number of steps to achieve goals,overfitting|simulation-reality gap,6,none,no,success,1.0,,,,,
SCREEN_0225,GalaxAI: Machine learning toolbox for interpretable analysis of spacecraft telemetry data,Ana Kostovska; Matej Petković; Tomaž Stepišnik; Luke Lucas; Timothy Finn; José Martínez-Heras; Panče Panov; Sašo Džeroski; Alessandro Donati; Nikola Simidjievski; Dragi Kocev,2021,"8th IEEE International Conference on Space Mission Challenges for
  Information Technology (SMC-IT 2021)","We present GalaxAI - a versatile machine learning toolbox for efficient and interpretable end-to-end analysis of spacecraft telemetry data. GalaxAI employs various machine learning algorithms for multivariate time series analyses, classification, regression and structured output prediction, capable of handling high-throughput heterogeneous data. These methods allow for the construction of robust and accurate predictive models, that are in turn applied to different tasks of spacecraft monitoring and operations planning. More importantly, besides the accurate building of models, GalaxAI implements a visualisation layer, providing mission specialists and operators with a full, detailed and interpretable view of the data analysis process. We show the utility and versatility of GalaxAI on two use-cases concerning two different spacecraft: i) analysis and planning of Mars Express thermal power consumption and ii) predicting of INTEGRAL's crossings through Van Allen belts.",,http://arxiv.org/abs/2108.01407v2,arxiv,,,,,,,,,pdf_cache/pdfs/191068854ce43383.pdf,cached,2108.014072,,http://arxiv.org/pdf/2108.01407v2,cs.LG,444c57e1037671d7a576852e73ca691737e0872ed47064d42a86967cbc2f5ac2,,,,,,en,conference,,no,no,,,,,6,none,no,success,0.7699999999999999,,,,,
SCREEN_0226,Accelerated nonlinear primal-dual hybrid gradient methods with applications to supervised machine learning,Jérôme Darbon; Gabriel P. Langlois,2021,,"The linear primal-dual hybrid gradient (PDHG) method is a first-order method that splits convex optimization problems with saddle-point structure into smaller subproblems. Unlike those obtained in most splitting methods, these subproblems can generally be solved efficiently because they involve simple operations such as matrix-vector multiplications or proximal mappings that are fast to evaluate numerically. This advantage comes at the price that the linear PDHG method requires precise stepsize parameters for the problem at hand to achieve an optimal convergence rate. Unfortunately, these stepsize parameters are often prohibitively expensive to compute for large-scale optimization problems, such as those in machine learning. This issue makes the otherwise simple linear PDHG method unsuitable for such problems, and it is also shared by most first-order optimization methods as well. To address this issue, we introduce accelerated nonlinear PDHG methods that achieve an optimal convergence rate with stepsize parameters that are simple and efficient to compute. We prove rigorous convergence results, including results for strongly convex or smooth problems posed on infinite-dimensional reflexive Banach spaces. We illustrate the efficiency of our methods on $\ell_{1}$-constrained logistic regression and entropy-regularized matrix games. Our numerical experiments show that the nonlinear PDHG methods are considerably faster than competing methods.",,http://arxiv.org/abs/2109.12222v2,arxiv,,,,,,,,,pdf_cache/pdfs/1bde24bcecad3523.pdf,cached,2109.122222,,http://arxiv.org/pdf/2109.12222v2,"math.OC; cs.LG; math.ST; stat.TH; 65K10 (Primary), 49M29 (Secondary), 62J99 (Secondary); G.1.6; I.2.6",4a811225ef6beb7d3e5147efd16f2874302943fb232656b7b4ad61ed6afb6113,,,,,,en,journal,digital,no,yes,none,none,"Convergence rate, computational efficiency",,6,none,no,success,1.0,,,,,
SCREEN_0227,Quantifying the Risk of Wildfire Ignition by Power Lines under Extreme Weather Conditions,Reza Bayani; Muhammad Waseem; Saeed D. Manshadi; Hassan Davani,2021,,"Utilities in California conduct Public Safety Power Shut-offs (PSPSs) to eliminate the elevated chances of wildfire ignitions caused by power lines during extreme weather conditions. We propose Wildfire Risk Aware operation planning Problem (WRAP), which enables system operators to pinpoint the segments of the network that should be de-energized. Sustained wind and wind gust can lead to conductor clashing, which could ignite surrounding vegetation. The 3D non-linear vibration equations of power lines are employed to generate a dataset that considers physical, structural, and meteorological parameters. With the help of machine learning techniques, a surrogate model is obtained which quantifies the risk of wildfire ignition by individual power lines under extreme weather conditions. The cases illustrate the superior performance of WRAP under extreme weather conditions in mitigating wildfire risk and serving customers compared to the naive PSPS approach and another method in the literature. Cases are also designated to sensitivity analysis of WRAP to critical load-serving control parameters in different weather conditions. Finally, a discussion is provided to explore our wildfire risk monetization approach and its implications for WRAP decisions.",,http://arxiv.org/abs/2110.05551v2,arxiv,,,,,,,,,pdf_cache/pdfs/5d65179aa5297b9d.pdf,cached,2110.055512,,http://arxiv.org/pdf/2110.05551v2,stat.AP; cs.SY; eess.SY,4dd8a52b92bb7694273af3cf8d7a139ba246d3b8087e5863dcd9099b02b24719,,,,,,en,tech-report,none,no,yes,none,none,Comparison of different learning algorithms for predicting wildfire risk; quantification of risk scores; balance between fire hazard risk and load shedding; monetization of wildfire risks.,,6,none,yes,success,1.0,,,,,
SCREEN_0228,Catch Me If You GAN: Using Artificial Intelligence for Fake Log Generation,Christian Toemmel,2021,,"With artificial intelligence (AI) becoming relevant in various parts of everyday life, other technologies are already widely influenced by the new way of handling large amounts of data. Although widespread already, AI has had only punctual influences on the cybersecurity field specifically. Many techniques and technologies used by cybersecurity experts function through manual labor and barely draw on automation, e.g., logs are often reviewed manually by system admins for potentially malicious keywords. This work evaluates the use of a special type of AI called generative adversarial networks (GANs) for log generation. More precisely, three different generative adversarial networks, SeqGAN, MaliGAN, and CoT, are reviewed in this research regarding their performance, focusing on generating new logs as a means of deceiving system admins for red teams. Although static generators for fake logs have been around for a while, their produces are usually easy to reveal as such. Using AI as an approach to this problem has not been widely researched. Identified challenges consist of formatting, dates and times, and overall consistency. Summing up the results, GANs seem not to be a good fit for generating fake logs. Their capability to detect fake logs, however, might be of use in practical scenarios.",,http://arxiv.org/abs/2112.12006v1,arxiv,,,,,,,,,pdf_cache/pdfs/e12b39b133efe7cf.pdf,cached,2112.120061,,http://arxiv.org/pdf/2112.12006v1,cs.CR; cs.LG,e0543748709012605142de9f004359a1f60dae15e9acd2bfc2c641f81335ba5d,,,,,,en,tech-report,digital,no,no,none,none,"Accuracy, generator and discriminator losses, negative log-likelihoods",bias|deception,6,https://github.com/williamSYSU/TextGAN-PyTorch/,yes,success,1.0,bias|deception,,,accuracy,
SCREEN_0229,Learning to Resolve Alliance Dilemmas in Many-Player Zero-Sum Games,Edward Hughes; Thomas W. Anthony; Tom Eccles; Joel Z. Leibo; David Balduzzi; Yoram Bachrach,2020,,"Zero-sum games have long guided artificial intelligence research, since they possess both a rich strategy space of best-responses and a clear evaluation metric. What's more, competition is a vital mechanism in many real-world multi-agent systems capable of generating intelligent innovations: Darwinian evolution, the market economy and the AlphaZero algorithm, to name a few. In two-player zero-sum games, the challenge is usually viewed as finding Nash equilibrium strategies, safeguarding against exploitation regardless of the opponent. While this captures the intricacies of chess or Go, it avoids the notion of cooperation with co-players, a hallmark of the major transitions leading from unicellular organisms to human civilization. Beyond two players, alliance formation often confers an advantage; however this requires trust, namely the promise of mutual cooperation in the face of incentives to defect. Successful play therefore requires adaptation to co-players rather than the pursuit of non-exploitability. Here we argue that a systematic study of many-player zero-sum games is a crucial element of artificial intelligence research. Using symmetric zero-sum matrix games, we demonstrate formally that alliance formation may be seen as a social dilemma, and empirically that na\""ive multi-agent reinforcement learning therefore fails to form alliances. We introduce a toy model of economic competition, and show how reinforcement learning may be augmented with a peer-to-peer contract mechanism to discover and enforce alliances. Finally, we generalize our agent model to incorporate temporally-extended contracts, presenting opportunities for further work.",,http://arxiv.org/abs/2003.00799v1,arxiv,,,,,,,,,pdf_cache/pdfs/df4688367a3da4af.pdf,cached,2003.007991,,http://arxiv.org/pdf/2003.00799v1,cs.GT; cs.LG; cs.MA; stat.ML,d051532e4cce9dd9749b7184951914284003e19fcd4fcf68418fb14ecb547ff4,,,,,,en,conference,matrix,no,yes,none,none,"Performance against a human expert, size of the game tree, and empirical results showing alliance formation as a social dilemma.",,6,none,no,success,1.0,,,,,
SCREEN_0230,Monte Carlo Neural Fictitious Self-Play: Approach to Approximate Nash equilibrium of Imperfect-Information Games,Li Zhang; Wei Wang; Shijian Li; Gang Pan,2019,,"Researchers on artificial intelligence have achieved human-level intelligence in large-scale perfect-information games, but it is still a challenge to achieve (nearly) optimal results (in other words, an approximate Nash Equilibrium) in large-scale imperfect-information games (i.e. war games, football coach or business strategies). Neural Fictitious Self Play (NFSP) is an effective algorithm for learning approximate Nash equilibrium of imperfect-information games from self-play without prior domain knowledge. However, it relies on Deep Q-Network, which is off-line and is hard to converge in online games with changing opponent strategy, so it can't approach approximate Nash equilibrium in games with large search scale and deep search depth. In this paper, we propose Monte Carlo Neural Fictitious Self Play (MC-NFSP), an algorithm combines Monte Carlo tree search with NFSP, which greatly improves the performance on large-scale zero-sum imperfect-information games. Experimentally, we demonstrate that the proposed Monte Carlo Neural Fictitious Self Play can converge to approximate Nash equilibrium in games with large-scale search depth while the Neural Fictitious Self Play can't. Furthermore, we develop Asynchronous Neural Fictitious Self Play (ANFSP). It use asynchronous and parallel architecture to collect game experience. In experiments, we show that parallel actor-learners have a further accelerated and stabilizing effect on training.",,http://arxiv.org/abs/1903.09569v2,arxiv,,,,,,,,,pdf_cache/pdfs/7a8983a7a83be829.pdf,cached,1903.095692,,http://arxiv.org/pdf/1903.09569v2,cs.AI,4f21fd276371a4b6f7a4737622e8dbae85db2de46c2bc2204265d10ef51b06c7,,,,,,en,tech-report,digital,no,yes,none,none,"Exploitability, convergence to Nash equilibrium, training time, win rate against human players",,6,none,yes,success,1.0,,,,win_rate,
SCREEN_0231,CybORG: An Autonomous Cyber Operations Research Gym,Callum Baillie; Maxwell Standen; Jonathon Schwartz; Michael Docking; David Bowman; Junae Kim,2020,,"Autonomous Cyber Operations (ACO) involves the consideration of blue team (defender) and red team (attacker) decision-making models in adversarial scenarios. To support the application of machine learning algorithms to solve this problem, and to encourage such practitioners to attend to problems in the ACO setting, a suitable gym (toolkit for experiments) is necessary. We introduce CybORG, a work-in-progress gym for ACO research. Driven by the need to efficiently support reinforcement learning to train adversarial decision-making models through simulation and emulation, our design differs from prior related work. Our early evaluation provides some evidence that CybORG is appropriate for our purpose and may provide a basis for advancing ACO research towards practical applications.",,http://arxiv.org/abs/2002.10667v2,arxiv,,,,,,,,,pdf_cache/pdfs/1f8f839c9a72f8bc.pdf,cached,2002.106672,,http://arxiv.org/pdf/2002.10667v2,cs.CR,6a651f8b2e4a2d990e8d9f659e006885ceaaa1801e58673d7dc4f8670cfc0ee2,,,,,,en,tech-report,digital,no,yes,none,none,Total reward and number of steps to determine the performance of the agent.,,6,none,yes,success,1.0,,,,,
SCREEN_0232,Min-Max Q-Learning for Multi-Player Pursuit-Evasion Games,Jhanani Selvakumar; Efstathios Bakolas,2020,,"In this paper, we address a pursuit-evasion game involving multiple players by utilizing tools and techniques from reinforcement learning and matrix game theory. In particular, we consider the problem of steering an evader to a goal destination while avoiding capture by multiple pursuers, which is a high-dimensional and computationally intractable problem in general. In our proposed approach, we first formulate the multi-agent pursuit-evasion game as a sequence of discrete matrix games. Next, in order to simplify the solution process, we transform the high-dimensional state space into a low-dimensional manifold and the continuous action space into a feature-based space, which is a discrete abstraction of the original space. Based on these transformed state and action spaces, we subsequently employ min-max Q-learning, to generate the entries of the payoff matrix of the game, and subsequently obtain the optimal action for the evader at each stage. Finally, we present extensive numerical simulations to evaluate the performance of the proposed learning-based evading strategy in terms of the evader's ability to reach the desired target location without being captured, as well as computational efficiency.",,http://arxiv.org/abs/2003.03727v1,arxiv,,,,,,,,,pdf_cache/pdfs/3a99544aedf8e2b6.pdf,cached,2003.037271,,http://arxiv.org/pdf/2003.03727v1,eess.SY; cs.SY; math.OC,575aa142a0e9c78bcad8800049b0e8cc6b0c49cdef3758f00198dd83eaa1e12f,,,,,,en,tech-report,matrix,no,yes,none,none,"Evader's ability to reach the target without being captured, computational efficiency, success rate of reaching the target, and avoiding capture.",,6,none,yes,success,1.0,,,matrix,,
SCREEN_0233,Predicting Plans and Actions in Two-Player Repeated Games,Najma Mathema; Michael A. Goodrich; Jacob W. Crandall,2020,,"Artificial intelligence (AI) agents will need to interact with both other AI agents and humans. Creating models of associates help to predict the modeled agents' actions, plans, and intentions. This work introduces algorithms that predict actions, plans and intentions in repeated play games, with providing an exploration of algorithms. We form a generative Bayesian approach to model S#. S# is designed as a robust algorithm that learns to cooperate with its associate in 2 by 2 matrix games. The actions, plans and intentions associated with each S# expert are identified from the literature, grouping the S# experts accordingly, and thus predicting actions, plans, and intentions based on their state probabilities. Two prediction methods are explored for Prisoners Dilemma: the Maximum A Posteriori (MAP) and an Aggregation approach. MAP (~89% accuracy) performed the best for action prediction. Both methods predicted plans of S# with ~88% accuracy. Paired T-test shows that MAP performs significantly better than Aggregation for predicting S#'s actions without cheap talk. Intention is explored based on the goals of the S# experts; results show that goals are predicted precisely when modeling S#. The obtained results show that the proposed Bayesian approach is well suited for modeling agents in two-player repeated games.",,http://arxiv.org/abs/2004.12480v1,arxiv,,,,,,,,,pdf_cache/pdfs/657dbee057fbd4c3.pdf,cached,2004.124801,,http://arxiv.org/pdf/2004.12480v1,cs.AI; cs.GT; cs.HC; cs.MA,b971ed7127263fbc0f43a04611d3435cba5480f23cb8ccd72d36fbe7f083e8bd,,,,,,en,tech-report,matrix,no,yes,none,none,"Accuracy of action and plan predictions, Paired T-test for statistical significance",,6,none,yes,success,1.0,,,,accuracy,
SCREEN_0234,Analytic Deep Learning-based Surrogate Model for Operational Planning with Dynamic TTC Constraints,Gao Qiu; Youbo Liu; Junyong Liu; Junbo Zhao; Lingfeng Wang; Tingjian Liu; Hongjun Gao,2020,,"The increased penetration of wind power introduces more operational changes of critical corridors and the traditional time-consuming transient stability constrained total transfer capability (TTC) operational planning is unable to meet the real-time monitoring need. This paper develops a more computationally efficient approach to address that challenge via the analytical deep learning-based surrogate model. The key idea is to resort to the deep learning for developing a computationally cheap surrogate model to replace the original time-consuming differential-algebraic constraints related to TTC. However, the deep learning-based surrogate model introduces implicit rules that are difficult to handle in the optimization process. To this end, we derive the Jacobian and Hessian matrices of the implicit surrogate models and finally transfer them into an analytical formulation that can be easily solved by the interior point method. Surrogate modeling and problem reformulation allow us to achieve significantly improved computational efficiency and the yielded solutions can be used for operational planning. Numerical results carried out on the modified IEEE 39-bus system demonstrate the effectiveness of the proposed method in dealing with com-plicated TTC constraints while balancing the computational efficiency and accuracy.",,http://arxiv.org/abs/2006.16186v1,arxiv,,,,,,,,,pdf_cache/pdfs/b422b3a198aa8c06.pdf,cached,2006.161861,,http://arxiv.org/pdf/2006.16186v1,eess.SY; cs.SY,c3b0c1018509f362ca51f0657e10cfe1b4eb32dff092dc2d558712e0313f7d4f,,,,,,en,tech-report,none,no,yes,none,none,"Computational efficiency, accuracy, mean squared error (MSE)",,6,none,yes,success,1.0,,,,accuracy,
SCREEN_0235,"Machine Learning-Assisted UAV Operations with UTM: Requirements, Challenges, and Solutions",Aly Sabri Abdalla; Vuk Marojevic,2020,,"Unmanned aerial vehicles (UAVs) are emerging in commercial spaces and will support many applications and services, such as smart agriculture, dynamic network deployment, and network coverage extension, surveillance and security. The unmanned aircraft system (UAS) traffic management (UTM) provides a framework for safe UAV operation integrating UAV controllers and central data bases via a communications network. This paper discusses the challenges and opportunities for machine learning (ML) for effectively providing critical UTM services. We introduce the four pillars of UTM---operation planning, situational awareness, status and advisors and security---and discuss the main services, specific opportunities for ML and the ongoing research. We conclude that the multi-faceted operating environment and operational parameters will benefit from collected data and data-driven algorithms, as well as online learning to face new UAV operation situations.",,http://arxiv.org/abs/2006.14544v1,arxiv,,,,,,,,,pdf_cache/pdfs/f3fdf1ec9a3602a4.pdf,cached,2006.145441,,http://arxiv.org/pdf/2006.14544v1,eess.SP; cs.SY; eess.SY,71152368797f6eda1745bf8647664c7978fddd931b5e32522e166542fa8a047f,,,,,,en,tech-report,none,no,no,none,none,none,,6,none,yes,success,1.0,,,,,
SCREEN_0236,The Effect of Strategic Noise in Linear Regression,Safwan Hossain; Nisarg Shah,2020,,"We build on an emerging line of work which studies strategic manipulations in training data provided to machine learning algorithms. Specifically, we focus on the ubiquitous task of linear regression. Prior work focused on the design of strategyproof algorithms, which aim to prevent such manipulations altogether by aligning the incentives of data sources. However, algorithms used in practice are often not strategyproof, which induces a strategic game among the agents. We focus on a broad class of non-strategyproof algorithms for linear regression, namely $\ell_p$ norm minimization ($p > 1$) with convex regularization. We show that when manipulations are bounded, every algorithm in this class admits a unique pure Nash equilibrium outcome. We also shed light on the structure of this equilibrium by uncovering a surprising connection between strategyproof algorithms and pure Nash equilibria of non-strategyproof algorithms in a broader setting, which may be of independent interest. Finally, we analyze the quality of equilibria under these algorithms in terms of the price of anarchy.",,http://arxiv.org/abs/2007.07316v1,arxiv,,,,,,,,,pdf_cache/pdfs/103e0512d1b651b5.pdf,cached,2007.073161,,http://arxiv.org/pdf/2007.07316v1,cs.GT,6e752f2fae4267141a6c8a08cebb8dadb0527226fdc2e7198246653218e4ca6a,,,,,,en,tech-report,digital,no,yes,none,none,"Pure price of anarchy (PPoA) measured as the ratio between the maximum social cost under any PNE and the optimal social cost under honest reporting, using mean squared error (MSE) as the measure of fit.",deception,6,none,yes,success,1.0,deception,,,,
SCREEN_0237,Investigation on Research Ethics and Building a Benchmark,Shun Inagaki; Robert Ramirez; Masaki Shimaoka; Kenichi Magata,2020,,"When dealing with leading edge cyber security research, especially when operating from the perspective of an attacker or a red team, it becomes necessary for one to at times consider how ethics comes into play. There are currently no cyber security-specific ethics standards, which in particular is one reason more adversarial cyber security research lags behind in Japan. In this research, using machine learning and manual methods we extracted best practices for research ethics from past top conference papers. Using this knowledge we constructed an ethics knowledge base for cyber security research. Such a knowledge base can be used to properly distinguish grey-area research so that it is not wrongly forbidden. Using a decision tree-style user interface that we created for our knowledge base, researchers may be able to efficiently identify which aspects of their research require ethical consideration. In this work, as a preliminary step we focused on only a portion of the areas of research covered by cyber security conferences, but our results are applicable to any area of research.",,http://arxiv.org/abs/2011.13925v1,arxiv,,,,,,,,,pdf_cache/pdfs/49d0974c0a6c04b6.pdf,cached,2011.139251,,http://arxiv.org/pdf/2011.13925v1,"cs.CR; 68T50, 68M25; I.7.5; K.4.1; H.3.1; H.3.7",65344e62214a362dad24a4bbed1fb5b596043c1c329a9e6d4546d71c46a9abcc,,,,,,en,conference,none,no,no,none,none,none,,6,none,no,success,1.0,,,,,
SCREEN_0238,A Data-Driven Machine Learning Approach for Consumer Modeling with Load Disaggregation,A. Khaled Zarabie; Sanjoy Das; Hongyu Wu,2020,,"While non-parametric models, such as neural networks, are sufficient in the load forecasting, separate estimates of fixed and shiftable loads are beneficial to a wide range of applications such as distribution system operational planning, load scheduling, energy trading, and utility demand response programs. A semi-parametric estimation model is usually required, where cost sensitivities of demands must be known. Existing research work consistently uses somewhat arbitrary parameters that seem to work best. In this paper, we propose a generic class of data-driven semiparametric models derived from consumption data of residential consumers. A two-stage machine learning approach is developed. In the first stage, disaggregation of the load into fixed and shiftable components is accomplished by means of a hybrid algorithm consisting of non-negative matrix factorization (NMF) and Gaussian mixture models (GMM), with the latter trained by an expectation-maximization (EM) algorithm. The fixed and shiftable loads are subject to analytic treatment with economic considerations. In the second stage, the model parameters are estimated using an L2-norm, epsilon-insensitive regression approach. Actual energy usage data of two residential customers show the validity of the proposed method.",,http://arxiv.org/abs/2011.03519v1,arxiv,,,,,,,,,pdf_cache/pdfs/b8b85b048ec8167f.pdf,cached,2011.035191,,http://arxiv.org/pdf/2011.03519v1,eess.SP; cs.CE; cs.LG; cs.SY; eess.SY,bb0e60b4d5cfa959c42b0a7ac442a5c42c75bd629550ece4158ce775c683c401,,,,,,en,tech-report,none,no,yes,none,none,"The paper evaluates the proposed method by comparing the median values of parameters obtained from disaggregated estimates with those obtained from real data, and by calculating correlation coefficients.",,6,none,yes,success,1.0,,,,,
SCREEN_0239,Sublinear classical and quantum algorithms for general matrix games,Tongyang Li; Chunhao Wang; Shouvanik Chakrabarti; Xiaodi Wu,2020,,"We investigate sublinear classical and quantum algorithms for matrix games, a fundamental problem in optimization and machine learning, with provable guarantees. Given a matrix $A\in\mathbb{R}^{n\times d}$, sublinear algorithms for the matrix game $\min_{x\in\mathcal{X}}\max_{y\in\mathcal{Y}} y^{\top} Ax$ were previously known only for two special cases: (1) $\mathcal{Y}$ being the $\ell_{1}$-norm unit ball, and (2) $\mathcal{X}$ being either the $\ell_{1}$- or the $\ell_{2}$-norm unit ball. We give a sublinear classical algorithm that can interpolate smoothly between these two cases: for any fixed $q\in (1,2]$, we solve the matrix game where $\mathcal{X}$ is a $\ell_{q}$-norm unit ball within additive error $\epsilon$ in time $\tilde{O}((n+d)/{\epsilon^{2}})$. We also provide a corresponding sublinear quantum algorithm that solves the same task in time $\tilde{O}((\sqrt{n}+\sqrt{d})\textrm{poly}(1/\epsilon))$ with a quadratic improvement in both $n$ and $d$. Both our classical and quantum algorithms are optimal in the dimension parameters $n$ and $d$ up to poly-logarithmic factors. Finally, we propose sublinear classical and quantum algorithms for the approximate Carath\'eodory problem and the $\ell_{q}$-margin support vector machines as applications.",,http://arxiv.org/abs/2012.06519v1,arxiv,,,,,,,,,pdf_cache/pdfs/b61899a25f9a9808.pdf,cached,2012.065191,,http://arxiv.org/pdf/2012.06519v1,quant-ph; cs.DS; cs.LG; math.OC,c45a0af4cf75f6bb16c8860b263323f3e6c30947ed4ab21a61edea71715b4272,,,,,,en,conference,matrix,no,yes,none,none,Time complexity and error bounds for classical and quantum algorithms,,6,none,no,success,1.0,,,matrix,,
SCREEN_0240,Predicting Tactical Solutions to Operational Planning Problems under Imperfect Information,Eric Larsen; Sébastien Lachapelle; Yoshua Bengio; Emma Frejinger; Simon Lacoste-Julien; Andrea Lodi,2019,"INFORMS Journal on Computing 34(1):227-242, 2021","This paper offers a methodological contribution at the intersection of machine learning and operations research. Namely, we propose a methodology to quickly predict tactical solutions to a given operational problem. In this context, the tactical solution is less detailed than the operational one but it has to be computed in very short time and under imperfect information. The problem is of importance in various applications where tactical and operational planning problems are interrelated and information about the operational problem is revealed over time. This is for instance the case in certain capacity planning and demand management systems. We formulate the problem as a two-stage optimal prediction stochastic program whose solution we predict with a supervised machine learning algorithm. The training data set consists of a large number of deterministic (second stage) problems generated by controlled probabilistic sampling. The labels are computed based on solutions to the deterministic problems (solved independently and offline) employing appropriate aggregation and subselection methods to address uncertainty. Results on our motivating application in load planning for rail transportation show that deep learning algorithms produce highly accurate predictions in very short computing time (milliseconds or less). The prediction accuracy is comparable to solutions computed by sample average approximation of the stochastic program.",10.1287/ijoc.2021.1091,http://arxiv.org/abs/1901.07935v4,arxiv,,,,,,,,,pdf_cache/pdfs/10_1287_ijoc_2021_1091.pdf,cached,1901.079354,,http://arxiv.org/pdf/1901.07935v4,cs.LG; math.OC; stat.ML,68fdc00bd3c897e0cd163cb0087cf6c71308280c019237277e75c22d129c2f71,,,,,,en,journal,none,no,yes,none,none,"Prediction accuracy, absolute error, computation time",,6,none,no,success,1.0,,,,accuracy,
SCREEN_0241,Flexible Production Systems: Automated Generation of Operations Plans Based on ISA-95 and PDDL,Bernhard Wally; Jirí Vyskočil; Petr Novák; Christian Huemer; Radek Šindelář; Petr Kadera; Alexandra Mazak; Manuel Wimmer,2019,,"Model-driven engineering (MDE) provides tools and methods for the manipulation of formal models. In this letter, we leverage MDE for the transformation of production system models into flat files that are understood by general purpose planning tools and that enable the computation of plans, i.e., sequences of production steps that are required to reach certain production goals. These plans are then merged back into the production system model, thus enriching the formalized production system knowledge.",10.1109/LRA.2019.2929991,http://arxiv.org/abs/1911.05481v1,arxiv,,,,,,,,,pdf_cache/pdfs/10_1109_LRA_2019_2929991.pdf,cached,1911.054811,,http://arxiv.org/pdf/1911.05481v1,eess.SY; cs.SY,77633078e41ebb306a959d874c206865cd8b39b59231eef460e22453f873645e,,,,,,en,journal,none,no,yes,none,none,"Performance data of the PDDL solver, cost of the solutions, solving time, and plan execution length.",deception,6,none,no,success,1.0,deception,,,,
SCREEN_0242,Learning for DC-OPF: Classifying active sets using neural nets,Deepjyoti Deka; Sidhant Misra,2019,,"The optimal power flow is an optimization problem used in power systems operational planning to maximize economic efficiency while satisfying demand and maintaining safety margins. Due to uncertainty and variability in renewable energy generation and demand, the optimal solution needs to be updated in response to observed uncertainty realizations or near real-time forecast updates. To address the challenge of computing such frequent real-time updates to the optimal solution, recent literature has proposed the use of machine learning to learn the mapping between the uncertainty realization and the optimal solution. Further, learning the active set of constraints at optimality, as opposed to directly learning the optimal solution, has been shown to significantly simplify the machine learning task, and the learnt model can be used to predict optimal solutions in real-time. In this paper, we propose the use of classification algorithms to learn the mapping between the uncertainty realization and the active set of constraints at optimality, thus further enhancing the computational efficiency of the real-time prediction. We employ neural net classifiers for this task and demonstrate the excellent performance of this approach on a number of systems in the IEEE PES PGLib-OPF benchmark library.",,http://arxiv.org/abs/1902.05607v1,arxiv,,,,,,,,,pdf_cache/pdfs/66e51b5d81bb9c09.pdf,cached,1902.056071,,http://arxiv.org/pdf/1902.05607v1,cs.SY,16111ac550c2b3b576fa01a5dab58e9a35ed350cbd1e0c077c7d9c6543876804,,,,,,en,tech-report,none,no,yes,none,none,"Prediction accuracy, computational efficiency, number of training samples required",,6,https://github.com/power-grid-lib/pglib-opf,yes,success,1.0,,,,accuracy,
SCREEN_0243,Increasing Iterate Averaging for Solving Saddle-Point Problems,Yuan Gao; Christian Kroer; Donald Goldfarb,2019,,"Many problems in machine learning and game theory can be formulated as saddle-point problems, for which various first-order methods have been developed and proven efficient in practice. Under the general convex-concave assumption, most first-order methods only guarantee an ergodic convergence rate, that is, the uniform averages of the iterates converge at a $O(1/T)$ rate in terms of the saddle-point residual. However, numerically, the iterates themselves can often converge much faster than the uniform averages. This observation motivates increasing averaging schemes that put more weight on later iterates, in contrast to the usual uniform averaging. We show that such increasing averaging schemes, applied to various first-order methods, are able to preserve the $O(1/T)$ convergence rate with no additional assumptions or computational overhead. Extensive numerical experiments on zero-sum game solving, market equilibrium computation and image denoising demonstrate the effectiveness of the proposed schemes. In particular, the increasing averages consistently outperform the uniform averages in all test problems by orders of magnitude. When solving matrix and extensive-form games, increasing averages consistently outperform the last iterates as well. For matrix games, a first-order method equipped with increasing averaging outperforms the highly competitive CFR$^+$ algorithm.",,http://arxiv.org/abs/1903.10646v3,arxiv,,,,,,,,,pdf_cache/pdfs/f96813ad1a019722.pdf,cached,1903.106463,,http://arxiv.org/pdf/1903.10646v3,cs.LG; cs.GT; math.OC; stat.ML,8759cf9f9d4f2a5a31acdee1e87aa03b3b82ae03f4d5efce2c7c8c6f128a6013,,,,,,en,journal,matrix,no,yes,none,none,Convergence rate of increasing averaging schemes compared to uniform averages in terms of saddle-point residual.,,6,none,no,success,1.0,,,,,
SCREEN_0244,HARK Side of Deep Learning -- From Grad Student Descent to Automated Machine Learning,Oguzhan Gencoglu; Mark van Gils; Esin Guldogan; Chamin Morikawa; Mehmet Süzen; Mathias Gruber; Jussi Leinonen; Heikki Huttunen,2019,,"Recent advancements in machine learning research, i.e., deep learning, introduced methods that excel conventional algorithms as well as humans in several complex tasks, ranging from detection of objects in images and speech recognition to playing difficult strategic games. However, the current methodology of machine learning research and consequently, implementations of the real-world applications of such algorithms, seems to have a recurring HARKing (Hypothesizing After the Results are Known) issue. In this work, we elaborate on the algorithmic, economic and social reasons and consequences of this phenomenon. We present examples from current common practices of conducting machine learning research (e.g. avoidance of reporting negative results) and failure of generalization ability of the proposed algorithms and datasets in actual real-life usage. Furthermore, a potential future trajectory of machine learning research and development from the perspective of accountable, unbiased, ethical and privacy-aware algorithmic decision making is discussed. We would like to emphasize that with this discussion we neither claim to provide an exhaustive argumentation nor blame any specific institution or individual on the raised issues. This is simply a discussion put forth by us, insiders of the machine learning field, reflecting on us.",,http://arxiv.org/abs/1904.07633v1,arxiv,,,,,,,,,pdf_cache/pdfs/9d3083f2481921ae.pdf,cached,1904.076331,,http://arxiv.org/pdf/1904.07633v1,cs.LG,67f8757c0faa1ddab4d95a43c6fe1e6dcdb5c69ebfdfebececaf5e81c74241ea,,,,,,en,tech-report,none,no,no,none,none,none,,6,none,yes,success,1.0,,,,,
SCREEN_0245,Learning Causality: Synthesis of Large-Scale Causal Networks from High-Dimensional Time Series Data,Mark-Oliver Stehr; Peter Avar; Andrew R. Korte; Lida Parvin; Ziad J. Sahab; Deborah I. Bunin; Merrill Knapp; Denise Nishita; Andrew Poggio; Carolyn L. Talcott; Brian M. Davis; Christine A. Morton; Christopher J. Sevinsky; Maria I. Zavodszky; Akos Vertes,2019,,"There is an abundance of complex dynamic systems that are critical to our daily lives and our society but that are hardly understood, and even with today's possibilities to sense and collect large amounts of experimental data, they are so complex and continuously evolving that it is unlikely that their dynamics will ever be understood in full detail. Nevertheless, through computational tools we can try to make the best possible use of the current technologies and available data. We believe that the most useful models will have to take into account the imbalance between system complexity and available data in the context of limited knowledge or multiple hypotheses. The complex system of biological cells is a prime example of such a system that is studied in systems biology and has motivated the methods presented in this paper. They were developed as part of the DARPA Rapid Threat Assessment (RTA) program, which is concerned with understanding of the mechanism of action (MoA) of toxins or drugs affecting human cells. Using a combination of Gaussian processes and abstract network modeling, we present three fundamentally different machine-learning-based approaches to learn causal relations and synthesize causal networks from high-dimensional time series data. While other types of data are available and have been analyzed and integrated in our RTA work, we focus on transcriptomics (that is gene expression) data obtained from high-throughput microarray experiments in this paper to illustrate capabilities and limitations of our algorithms. Our algorithms make different but overall relatively few biological assumptions, so that they are applicable to other types of biological data and potentially even to other complex systems that exhibit high dimensionality but are not of biological nature.",,http://arxiv.org/abs/1905.02291v1,arxiv,,,,,,,,,pdf_cache/pdfs/1ddd8e91598bb2a8.pdf,cached,1905.022911,,http://arxiv.org/pdf/1905.02291v1,cs.LG; q-bio.CB; stat.ML,1762738558f890c622519e9a8a437845a1c56ee318b63c362c0b8fc319dc6a9a,,,,,,en,tech-report,none,no,no,none,none,none,,6,none,yes,success,1.0,,,,,
SCREEN_0246,Relative Hausdorff Distance for Network Analysis,Sinan G. Aksoy; Kathleen E. Nowak; Emilie Purvine; Stephen J. Young,2019,,"Similarity measures are used extensively in machine learning and data science algorithms. The newly proposed graph Relative Hausdorff (RH) distance is a lightweight yet nuanced similarity measure for quantifying the closeness of two graphs. In this work we study the effectiveness of RH distance as a tool for detecting anomalies in time-evolving graph sequences. We apply RH to cyber data with given red team events, as well to synthetically generated sequences of graphs with planted attacks. In our experiments, the performance of RH distance is at times comparable, and sometimes superior, to graph edit distance in detecting anomalous phenomena. Our results suggest that in appropriate contexts, RH distance has advantages over more computationally intensive similarity measures.",,http://arxiv.org/abs/1906.04936v1,arxiv,,,,,,,,,pdf_cache/pdfs/ae9fe789c3893c4a.pdf,cached,1906.049361,,http://arxiv.org/pdf/1906.04936v1,cs.DM; cs.LG; cs.SI,7efb0fc13605c475aed431200893732b105ac7e09207779ada44b36407258ee3,,,,,,en,journal,,no,no,,,,,6,none,no,success,0.7699999999999999,,,,,
SCREEN_0247,Artificial Intelligence in Surgery,Xiao-Yun Zhou; Yao Guo; Mali Shen; Guang-Zhong Yang,2019,,"Artificial Intelligence (AI) is gradually changing the practice of surgery with the advanced technological development of imaging, navigation and robotic intervention. In this article, the recent successful and influential applications of AI in surgery are reviewed from pre-operative planning and intra-operative guidance to the integration of surgical robots. We end with summarizing the current state, emerging trends and major challenges in the future development of AI in surgery.",,http://arxiv.org/abs/2001.00627v1,arxiv,,,,,,,,,pdf_cache/pdfs/6af450c9b7273d2d.pdf,cached,2001.006271,,http://arxiv.org/pdf/2001.00627v1,physics.med-ph; cs.AI; eess.IV,8bbe280ca86aafb864d058b2aac989156cd417d5297a2c20d494045e10391131,,,,,,en,journal,none,no,no,none,none,none,,6,none,no,success,1.0,,,,,
SCREEN_0248,"Proceedings of the 2018 Workshop on Compositional Approaches in Physics, NLP, and Social Sciences",Martha Lewis; Bob Coecke; Jules Hedges; Dimitri Kartsaklis; Dan Marsden,2018,"EPTCS 283, 2018","The ability to compose parts to form a more complex whole, and to analyze a whole as a combination of elements, is desirable across disciplines. This workshop bring together researchers applying compositional approaches to physics, NLP, cognitive science, and game theory. Within NLP, a long-standing aim is to represent how words can combine to form phrases and sentences. Within the framework of distributional semantics, words are represented as vectors in vector spaces. The categorical model of Coecke et al. [2010], inspired by quantum protocols, has provided a convincing account of compositionality in vector space models of NLP. There is furthermore a history of vector space models in cognitive science. Theories of categorization such as those developed by Nosofsky [1986] and Smith et al. [1988] utilise notions of distance between feature vectors. More recently G\""ardenfors [2004, 2014] has developed a model of concepts in which conceptual spaces provide geometric structures, and information is represented by points, vectors and regions in vector spaces. The same compositional approach has been applied to this formalism, giving conceptual spaces theory a richer model of compositionality than previously [Bolt et al., 2018]. Compositional approaches have also been applied in the study of strategic games and Nash equilibria. In contrast to classical game theory, where games are studied monolithically as one global object, compositional game theory works bottom-up by building large and complex games from smaller components. Such an approach is inherently difficult since the interaction between games has to be considered. Research into categorical compositional methods for this field have recently begun [Ghani et al., 2018]. Moreover, the interaction between the three disciplines of cognitive science, linguistics and game theory is a fertile ground for research. Game theory in cognitive science is a well-established area [Camerer, 2011]. Similarly game theoretic approaches have been applied in linguistics [J\""ager, 2008]. Lastly, the study of linguistics and cognitive science is intimately intertwined [Smolensky and Legendre, 2006, Jackendoff, 2007]. Physics supplies compositional approaches via vector spaces and categorical quantum theory, allowing the interplay between the three disciplines to be examined.",10.4204/EPTCS.283,http://arxiv.org/abs/1811.02701v1,arxiv,,,,,,,,,,not_found,1811.027011,,http://arxiv.org/pdf/1811.02701v1,cs.CL; cs.AI; cs.GT,,,,,,,en,,,,,,,,,,,,,,,,,,
SCREEN_0249,Publish-and-Flourish: decentralized co-creation and curation of scholarly content,Emilija Stojmenova Duh; Andrej Duh; Uroš Droftina; Tim Kos; Urban Duh; Tanja Simonič Korošak; Dean Korošak,2018,,Scholarly communication is today immersed in publish or perish culture that propels noncooperative behaviour in the sense of strategic games played by researchers. Here we introduce and describe a blockchain based platform for decentralized scholarly communication. The design of the platform rests on community driven publishing reviewing processes and implements incentives that promote cooperative user behaviour. Key to achieve cooperation in blockchain based scholarly communication is to transform a static research paper into a modifiable research paper under continuous peer review process. We describe and discuss the implementation of a modifiable research paper as a smart contract on the blockchain.,,http://arxiv.org/abs/1810.10263v1,arxiv,,,,,,,,,pdf_cache/pdfs/4c4e60b9fe282ba9.pdf,cached,1810.102631,,http://arxiv.org/pdf/1810.10263v1,cs.DC; cs.DL,4d14214b0dcd98f7575196df2b189a8f0d672b697ccbd2fa96a3ea4e2801fe5b,,,,,,en,,,,,,,,,,,,insufficient_text,,,,,,
SCREEN_0250,VMAV-C: A Deep Attention-based Reinforcement Learning Algorithm for Model-based Control,Xingxing Liang; Qi Wang; Yanghe Feng; Zhong Liu; Jincai Huang,2018,,"Recent breakthroughs in Go play and strategic games have witnessed the great potential of reinforcement learning in intelligently scheduling in uncertain environment, but some bottlenecks are also encountered when we generalize this paradigm to universal complex tasks. Among them, the low efficiency of data utilization in model-free reinforcement algorithms is of great concern. In contrast, the model-based reinforcement learning algorithms can reveal underlying dynamics in learning environments and seldom suffer the data utilization problem. To address the problem, a model-based reinforcement learning algorithm with attention mechanism embedded is proposed as an extension of World Models in this paper. We learn the environment model through Mixture Density Network Recurrent Network(MDN-RNN) for agents to interact, with combinations of variational auto-encoder(VAE) and attention incorporated in state value estimates during the process of learning policy. In this way, agent can learn optimal policies through less interactions with actual environment, and final experiments demonstrate the effectiveness of our model in control problem.",,http://arxiv.org/abs/1812.09968v1,arxiv,,,,,,,,,pdf_cache/pdfs/48d8a82ae3c9b000.pdf,cached,1812.099681,,http://arxiv.org/pdf/1812.09968v1,cs.LG; cs.AI; cs.NE,be820521e73dfb6de5108b4517008061493c161b924b5f4ef63ea7edfd978d3b,,,,,,en,tech-report,digital,no,yes,none,none,"Cumulative rewards, value network losses, prediction accuracy for ending state",,6,none,yes,success,1.0,,,,accuracy,
