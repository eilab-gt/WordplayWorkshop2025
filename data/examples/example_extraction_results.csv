screening_id,title,authors,year,venue,abstract,doi,url,source_db,venue_type,game_type,open_ended,quantitative,llm_family,llm_role,eval_metrics,failure_modes,awscale,code_release,grey_lit_flag,extraction_status,extraction_confidence,failure_modes_regex,llm_detected,game_type_detected,metrics_detected,code_detected
SCREEN_0001,"Escalation Risks in LLM-Powered Wargaming: A First Assessment","Reddie, Andrew; Goldblum, Bethany; Laderman, Sarah; Trinkunas, Harold",2023,"arXiv preprint","This paper presents findings from a wargaming exercise exploring nuclear escalation risks when large language models participate in crisis simulations. We find that LLMs tend to escalate conflicts more readily than human players.","","https://arxiv.org/abs/2310.03857","arxiv","tech-report","seminar","yes","no","GPT-4","player","Escalation frequency; Expert evaluation; Decision analysis","escalation|bias",4,"github.com/cset-wargaming/llm-escalation","yes","success",0.92,"escalation|bias","gpt4","seminar","escalation_frequency|expert_evaluation","github.com/cset-wargaming/llm-escalation"
SCREEN_0002,"GPT-4 as Strategic Decision Maker in Military Simulations","Johnson, Michael; Smith, Karen",2024,"Journal of Strategic Security","We evaluate GPT-4's performance as a strategic decision maker in complex military simulations, comparing its choices against expert human players and identifying systematic biases.","10.1234/jsec.2024.001","https://example.com/paper1","google_scholar","journal","digital","no","yes","GPT-4","player","Win rate; Decision optimality; Strategic coherence score","bias|prompt_sensitivity",2,"none","no","success",0.88,"bias","gpt4","digital","win_rate|decision_quality","none"
SCREEN_0003,"Adversarial Red Teaming with Language Models","Chen, Lisa; Park, James",2023,"IEEE Security & Privacy Workshops","This study examines the use of large language models as red team adversaries in cybersecurity wargaming exercises, demonstrating both novel attack strategies and inherent limitations.","10.1109/SPW.2023.00123","https://semanticscholar.org/paper/123456","semantic_scholar","workshop","digital","yes","yes","GPT-3.5","generator","Attack success rate; Novel strategy count; Detection evasion rate","hallucination|deception",3,"github.com/redteam-llm/adversarial","no","success",0.85,"hallucination|deception","gpt3","digital","success_rate|detection_rate","github.com/redteam-llm/adversarial"
SCREEN_0005,"Hallucination and Deception in AI Wargame Agents","Martinez, Carlos; Anderson, Jennifer",2023,"arXiv preprint","Analysis of hallucination patterns and deceptive behaviors exhibited by LLM agents in competitive wargaming scenarios, with implications for trust and verification.","","https://arxiv.org/abs/2308.12345","arxiv","tech-report","matrix","yes","no","Multiple","analyst","Hallucination rate; Deception detection accuracy; Trust scores","hallucination|deception|data_leakage",4,"none","yes","success",0.90,"hallucination|deception|data_leakage","multiple","matrix","hallucination_rate|trust_score","none"
SCREEN_0007,"Open-Ended Naval Strategy Games with Transformer Models","Kim, Steve; Lee, Anna",2023,"Naval War College Review","Implementation of open-ended naval strategy wargames using transformer-based language models, allowing for emergent tactics and natural language command interfaces.","","https://semanticscholar.org/paper/789012","semantic_scholar","journal","hybrid","yes","no","Custom Transformer","player","Tactical novelty score; Command interpretation accuracy; Engagement outcomes","",5,"github.com/naval-ai/open-wargame","no","success",0.82,"","transformer","hybrid","novelty_score|accuracy","github.com/naval-ai/open-wargame"