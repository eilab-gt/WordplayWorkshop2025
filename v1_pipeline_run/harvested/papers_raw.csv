title,authors,year,abstract,source_db,url,doi,arxiv_id,venue,citations,pdf_url,keywords
ALYMPICS: LLM Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents,Shaoguang Mao; Yuzhe Cai; Yan Xia; Wenshan Wu; Xun Wang; Fengyi Wang; Tao Ge; Furu Wei,2023,"This paper introduces Alympics (Olympics for Agents), a systematic simulation framework utilizing Large Language Model (LLM) agents for game theory research. Alympics creates a versatile platform for studying complex game theory problems, bridging the gap between theoretical game theory and empirical investigations by providing a controlled environment for simulating human-like strategic interactions with LLM agents. In our pilot case study, the ""Water Allocation Challenge,"" we explore Alympics through a challenging strategic game focused on the multi-round auction on scarce survival resources. This study demonstrates the framework's ability to qualitatively and quantitatively analyze game determinants, strategies, and outcomes. Additionally, we conduct a comprehensive human assessment and an in-depth evaluation of LLM agents in strategic decision-making scenarios. Our findings not only expand the understanding of LLM agents' proficiency in emulating human strategic behavior but also highlight their potential in advancing game theory knowledge, thereby enriching our understanding of both game theory and empowering further research into strategic decision-making domains with LLM agents. Codes, prompts, and all related resources are available at https://github.com/microsoft/Alympics.",arxiv,http://arxiv.org/abs/2311.03220v4,,2311.032204,,,http://arxiv.org/pdf/2311.03220v4,cs.CL; cs.AI; cs.GT
Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making,Kehan Zheng; Jinfeng Zhou; Hongning Wang,2025,"Large language models are increasingly used in strategic decision-making settings, yet evidence shows that, like humans, they often deviate from full rationality. In this study, we compare LLMs and humans using experimental paradigms directly adapted from behavioral game-theory research. We focus on two well-studied strategic games, Rock-Paper-Scissors and the Prisoner's Dilemma, which are well known for revealing systematic departures from rational play in human subjects. By placing LLMs in identical experimental conditions, we evaluate whether their behaviors exhibit the bounded rationality characteristic of humans. Our findings show that LLMs reproduce familiar human heuristics, such as outcome-based strategy switching and increased cooperation when future interaction is possible, but they apply these rules more rigidly and demonstrate weaker sensitivity to the dynamic changes in the game environment. Model-level analyses reveal distinctive architectural signatures in strategic behavior, and even reasoning models sometimes struggle to find effective strategies in adaptive situations. These results indicate that current LLMs capture only a partial form of human-like bounded rationality and highlight the need for training methods that encourage flexible opponent modeling and stronger context awareness.",arxiv,http://arxiv.org/abs/2506.09390v1,,2506.093901,,,http://arxiv.org/pdf/2506.09390v1,cs.AI; cs.GT
CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents,Siyuan Qi; Shuo Chen; Yexin Li; Xiangyu Kong; Junqi Wang; Bangcheng Yang; Pring Wong; Yifan Zhong; Xiaoyuan Zhang; Zhaowei Zhang; Nian Liu; Wei Wang; Yaodong Yang; Song-Chun Zhu,2024,"The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we introduce CivRealm, an environment inspired by the Civilization game. Civilization's profound alignment with human history and society necessitates sophisticated learning, while its ever-changing situations demand strong reasoning to generalize. Particularly, CivRealm sets up an imperfect-information general-sum game with a changing number of players; it presents a plethora of complex features, challenging the agent to deal with open-ended stochastic environments that require diplomacy and negotiation skills. Within CivRealm, we provide interfaces for two typical agent types: tensor-based agents that focus on learning, and language-based agents that emphasize reasoning. To catalyze further research, we present initial results for both paradigms. The canonical RL-based agents exhibit reasonable performance in mini-games, whereas both RL- and LLM-based agents struggle to make substantial progress in the full game. Overall, CivRealm stands as a unique learning and reasoning challenge for decision-making agents. The code is available at https://github.com/bigai-ai/civrealm.",arxiv,http://arxiv.org/abs/2401.10568v2,,2401.105682,,,http://arxiv.org/pdf/2401.10568v2,cs.AI
DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy,Kaixuan Xu; Jiajun Chai; Sicheng Li; Yuqian Fu; Yuanheng Zhu; Dongbin Zhao,2025,"Diplomacy is a complex multiplayer game that requires both cooperation and competition, posing significant challenges for AI systems. Traditional methods rely on equilibrium search to generate extensive game data for training, which demands substantial computational resources. Large Language Models (LLMs) offer a promising alternative, leveraging pre-trained knowledge to achieve strong performance with relatively small-scale fine-tuning. However, applying LLMs to Diplomacy remains challenging due to the exponential growth of possible action combinations and the intricate strategic interactions among players. To address this challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns equilibrium policies for Diplomacy. DipLLM employs an autoregressive factorization framework to simplify the complex task of multi-unit action assignment into a sequence of unit-level decisions. By defining an equilibrium policy within this framework as the learning objective, we fine-tune the model using only 1.5% of the data required by the state-of-the-art Cicero model, surpassing its performance. Our results demonstrate the potential of fine-tuned LLMs for tackling complex strategic decision-making in multiplayer games.",arxiv,http://arxiv.org/abs/2506.09655v2,,2506.096552,,,http://arxiv.org/pdf/2506.09655v2,cs.AI; cs.LG
DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments,Wenjie Tang; Yuan Zhou; Erqiang Xu; Keyan Cheng; Minne Li; Liquan Xiao,2025,"Large Language Model~(LLM) based agents have been increasingly popular in solving complex and dynamic tasks, which requires proper evaluation systems to assess their capabilities. Nevertheless, existing benchmarks usually either focus on single-objective tasks or use overly broad assessing metrics, failing to provide a comprehensive inspection of the actual capabilities of LLM-based agents in complicated decision-making tasks. To address these issues, we introduce DSGBench, a more rigorous evaluation platform for strategic decision-making. Firstly, it incorporates six complex strategic games which serve as ideal testbeds due to their long-term and multi-dimensional decision-making demands and flexibility in customizing tasks of various difficulty levels or multiple targets. Secondly, DSGBench employs a fine-grained evaluation scoring system which examines the decision-making capabilities by looking into the performance in five specific dimensions and offering a comprehensive assessment in a well-designed way. Furthermore, DSGBench also incorporates an automated decision-tracking mechanism which enables in-depth analysis of agent behaviour patterns and the changes in their strategies. We demonstrate the advances of DSGBench by applying it to multiple popular LLM-based agents and our results suggest that DSGBench provides valuable insights in choosing LLM-based agents as well as improving their future development. DSGBench is available at https://github.com/DeciBrain-Group/DSGBench.",arxiv,http://arxiv.org/abs/2503.06047v1,,2503.060471,,,http://arxiv.org/pdf/2503.06047v1,cs.AI; cs.CL
Dynamic Coalition Structure Detection in Natural Language-based Interactions,Abhishek N. Kulkarni; Andy Liu; Jean-Raphael Gaglione; Daniel Fried; Ufuk Topcu,2025,"In strategic multi-agent sequential interactions, detecting dynamic coalition structures is crucial for understanding how self-interested agents coordinate to influence outcomes. However, natural-language-based interactions introduce unique challenges to coalition detection due to ambiguity over intents and difficulty in modeling players' subjective perspectives. We propose a new method that leverages recent advancements in large language models and game theory to predict dynamic multilateral coalition formation in Diplomacy, a strategic multi-agent game where agents negotiate coalitions using natural language. The method consists of two stages. The first stage extracts the set of agreements discussed by two agents in their private dialogue, by combining a parsing-based filtering function with a fine-tuned language model trained to predict player intents. In the second stage, we define a new metric using the concept of subjective rationalizability from hypergame theory to evaluate the expected value of an agreement for each player. We then compute this metric for each agreement identified in the first stage by assessing the strategic value of the agreement for both players and taking into account the subjective belief of one player that the second player would honor the agreement. We demonstrate that our method effectively detects potential coalition structures in online Diplomacy gameplay by assigning high values to agreements likely to be honored and low values to those likely to be violated. The proposed method provides foundational insights into coalition formation in multi-agent environments with language-based negotiation and offers key directions for future research on the analysis of complex natural language-based interactions between agents.",arxiv,http://arxiv.org/abs/2502.16339v1,,2502.163391,,,http://arxiv.org/pdf/2502.16339v1,cs.MA; cs.CL; cs.GT
Escalation Risks from Language Models in Military and Diplomatic Decision-Making,Juan-Pablo Rivera; Gabriel Mukobi; Anka Reuel; Max Lamparth; Chandler Smith; Jacquelyn Schneider,2024,"Governments are increasingly considering integrating autonomous AI agents in high-stakes military and foreign-policy decision-making, especially with the emergence of advanced generative AI models like GPT-4. Our work aims to scrutinize the behavior of multiple AI agents in simulated wargames, specifically focusing on their predilection to take escalatory actions that may exacerbate multilateral conflicts. Drawing on political science and international relations literature about escalation dynamics, we design a novel wargame simulation and scoring framework to assess the escalation risks of actions taken by these agents in different scenarios. Contrary to prior studies, our research provides both qualitative and quantitative insights and focuses on large language models (LLMs). We find that all five studied off-the-shelf LLMs show forms of escalation and difficult-to-predict escalation patterns. We observe that models tend to develop arms-race dynamics, leading to greater conflict, and in rare cases, even to the deployment of nuclear weapons. Qualitatively, we also collect the models' reported reasonings for chosen actions and observe worrying justifications based on deterrence and first-strike tactics. Given the high stakes of military and foreign-policy contexts, we recommend further examination and cautious consideration before deploying autonomous language model agents for strategic military or diplomatic decision-making.",arxiv,http://arxiv.org/abs/2401.03408v1,10.1145/3630106.3658942,2401.034081,"The 2024 ACM Conference on Fairness, Accountability, and
  Transparency (FAccT 24), June 3-6, 2024, Rio de Janeiro, Brazil",,http://arxiv.org/pdf/2401.03408v1,cs.AI; cs.CL; cs.CY; cs.MA
Human vs. Machine: Behavioral Differences Between Expert Humans and Language Models in Wargame Simulations,Max Lamparth; Anthony Corso; Jacob Ganz; Oriana Skylar Mastro; Jacquelyn Schneider; Harold Trinkunas,2024,"To some, the advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness while reducing the influence of human error and emotions. However, there is still debate about how AI systems, especially large language models (LLMs) that can be applied to many tasks, behave compared to humans in high-stakes military decision-making scenarios with the potential for increased risks towards escalation. To test this potential and scrutinize the use of LLMs for such purposes, we use a new wargame experiment with 214 national security experts designed to examine crisis escalation in a fictional U.S.-China scenario and compare the behavior of human player teams to LLM-simulated team responses in separate simulations. Here, we find that the LLM-simulated responses can be more aggressive and significantly affected by changes in the scenario. We show a considerable high-level agreement in the LLM and human responses and significant quantitative and qualitative differences in individual actions and strategic tendencies. These differences depend on intrinsic biases in LLMs regarding the appropriate level of violence following strategic instructions, the choice of LLM, and whether the LLMs are tasked to decide for a team of players directly or first to simulate dialog between a team of players. When simulating the dialog, the discussions lack quality and maintain a farcical harmony. The LLM simulations cannot account for human player characteristics, showing no significant difference even for extreme traits, such as ""pacifist"" or ""aggressive sociopath."" When probing behavioral consistency across individual moves of the simulation, the tested LLMs deviated from each other but generally showed somewhat consistent behavior. Our results motivate policymakers to be cautious before granting autonomy or following AI-based strategy recommendations.",arxiv,http://arxiv.org/abs/2403.03407v4,,2403.034074,,,http://arxiv.org/pdf/2403.03407v4,cs.CY; cs.AI; cs.CL
Le formateur et son public dans le cadre de l’élaboration d’une simulation de type <i>wargame</i> sur plateau,Stéphane Goria; Philippe Hardy,2019,"Abstract Le wargame est un outil de réflexion qui emploie des techniques créatives pour stimuler l’imagination des historiens, des décideurs et des analystes dans un but d’identification d’opportunités et de menaces. Son objet est de permettre à ses joueurs de recréer des situations spécifiques et, plus important encore, d’être capable d’explorer ce qui aurait pu (approche historique) ou pourrait (approche prospective) se passer si le joueur décidait de faire les choses différemment. Issu d’une tradition assez longue de tentatives d’adaptation du jeu d’échecs à la simulation de bataille, le wargame s’applique depuis plus de deux siècles à la modélisation d’affrontements militaires à des fins de formation et d’élaboration de nouvelles stratégies ou tactiques. Par la suite, son champ d’application s’est étendu, il y a un peu plus d’un siècle, à la réflexion historico-militaire et depuis une soixantaine d’années à l’aide à la décision stratégique concernant des affrontements concurrentiels et économiques. Selon les moyens disponibles et les contextes de leurs mises en œuvre, les wargames prennent désormais quatre formes différentes : jeux de rôles de type grandeur nature, simulations informatiques, modélisation à base de figurines (ou sandbox) et modélisation sur des cartes ou plateaux. Nous nous intéresserons, dans le cadre de ce travail, spécifiquement aux cas des wargames sur cartes et plateaux (board wargames). Nous les aborderons d’abord en tant que moyen de compréhension d’une situation historique et militaire, puis en tant que transposition à des fins d’analyse stratégique concurrentielle. Nous commencerons par en décrire les contours historiques et fondamentaux pratiques. Puis, nous montrerons comment le formateur peut adapter ou concevoir le “jeu” selon la situation qu’il doit modéliser ainsi qu’au public auquel il est destiné. Nous discuterons ainsi de la conception d’un wargame en fonction des objectifs visés, du développement du prototype et de la phase de tests en tant que première confrontation au public ciblé par le formateur. Nous présenterons aussi comment, la modélisation d’un wargame est transposable à la modélisation d’un marché. Au-delà de l’aspect ludique et des objectifs du jeu, nous montrerons comment les participants peuvent prendre part à sa conception. De cette manière, nous aborderons la réflexion autour de la conception du jeu qui, bien menée, permet d’appréhender dans son entier un environnement concurrentiel et ses acteurs et d’en comprendre tous ses mécanismes et ses enjeux.",crossref,https://doi.org/10.2478/bgs-2019-0003,10.2478/bgs-2019-0003,,Board Game Studies Journal,3.0,,journal-article
LLMs are Superior Feedback Providers: Bootstrapping Reasoning for Lie Detection with Self-Generated Feedback,Tanushree Banerjee; Richard Zhu; Runzhe Yang; Karthik Narasimhan,2024,"Large Language Models (LLMs) excel at generating human-like dialogues and comprehending text. However, understanding the subtleties of complex exchanges in language remains a challenge. We propose a bootstrapping framework that leverages self-generated feedback to enhance LLM reasoning capabilities for lie detection. The framework consists of three stages: suggestion, feedback collection, and modification. In the suggestion stage, a cost-effective language model generates initial predictions based on game state and dialogue. The feedback-collection stage involves a language model providing feedback on these predictions. In the modification stage, a more advanced language model refines the initial predictions using the auto-generated feedback. We investigate the application of the proposed framework for detecting betrayal and deception in Diplomacy games, and compare it with feedback from professional human players. The LLM-generated feedback exhibits superior quality and significantly enhances the performance of the model. Our approach achieves a 39% improvement over the zero-shot baseline in lying-F1 without the need for any training data, rivaling state-of-the-art supervised learning results.",arxiv,http://arxiv.org/abs/2408.13915v1,,2408.139151,,,http://arxiv.org/pdf/2408.13915v1,cs.CL; cs.AI
Measuring Free-Form Decision-Making Inconsistency of Language Models in Military Crisis Simulations,Aryan Shrivastava; Jessica Hullman; Max Lamparth,2024,"There is an increasing interest in using language models (LMs) for automated decision-making, with multiple countries actively testing LMs to aid in military crisis decision-making. To scrutinize relying on LM decision-making in high-stakes settings, we examine the inconsistency of responses in a crisis simulation (""wargame""), similar to reported tests conducted by the US military. Prior work illustrated escalatory tendencies and varying levels of aggression among LMs but were constrained to simulations with pre-defined actions. This was due to the challenges associated with quantitatively measuring semantic differences and evaluating natural language decision-making without relying on pre-defined actions. In this work, we query LMs for free form responses and use a metric based on BERTScore to measure response inconsistency quantitatively. Leveraging the benefits of BERTScore, we show that the inconsistency metric is robust to linguistic variations that preserve semantic meaning in a question-answering setting across text lengths. We show that all five tested LMs exhibit levels of inconsistency that indicate semantic differences, even when adjusting the wargame setting, anonymizing involved conflict countries, or adjusting the sampling temperature parameter $T$. Further qualitative evaluation shows that models recommend courses of action that share few to no similarities. We also study the impact of different prompt sensitivity variations on inconsistency at temperature $T = 0$. We find that inconsistency due to semantically equivalent prompt variations can exceed response inconsistency from temperature sampling for most studied models across different levels of ablations. Given the high-stakes nature of military deployment, we recommend further consideration be taken before using LMs to inform military decisions or other cases of high-stakes decision-making.",arxiv,http://arxiv.org/abs/2410.13204v1,,2410.132041,,,http://arxiv.org/pdf/2410.13204v1,cs.CL; cs.AI; cs.CY
Open-Ended Wargames with Large Language Models,Daniel P. Hogan; Andrea Brennen,2024,"Wargames are a powerful tool for understanding and rehearsing real-world decision making. Automated play of wargames using artificial intelligence (AI) enables possibilities beyond those of human-conducted games, such as playing the game many times over to see a range of possible outcomes. There are two categories of wargames: quantitative games, with discrete types of moves, and qualitative games, which revolve around open-ended responses. Historically, automation efforts have focused on quantitative games, but large language models (LLMs) make it possible to automate qualitative wargames. We introduce ""Snow Globe,"" an LLM-powered multi-agent system for playing qualitative wargames. With Snow Globe, every stage of a text-based qualitative wargame from scenario preparation to post-game analysis can be optionally carried out by AI, humans, or a combination thereof. We describe its software architecture conceptually and release an open-source implementation alongside this publication. As case studies, we simulate a tabletop exercise about an AI incident response and a political wargame about a geopolitical crisis. We discuss potential applications of the approach and how it fits into the broader wargaming ecosystem.",arxiv,http://arxiv.org/abs/2404.11446v1,,2404.114461,,,http://arxiv.org/pdf/2404.11446v1,cs.CL; cs.AI; cs.CY
PokerBench: Training Large Language Models to become Professional Poker Players,Richard Zhuang; Akshat Gupta; Richard Yang; Aniket Rahane; Zhengyu Li; Gopala Anumanchipalli,2025,"We introduce PokerBench - a benchmark for evaluating the poker-playing abilities of large language models (LLMs). As LLMs excel in traditional NLP tasks, their application to complex, strategic games like poker poses a new challenge. Poker, an incomplete information game, demands a multitude of skills such as mathematics, reasoning, planning, strategy, and a deep understanding of game theory and human psychology. This makes Poker the ideal next frontier for large language models. PokerBench consists of a comprehensive compilation of 11,000 most important scenarios, split between pre-flop and post-flop play, developed in collaboration with trained poker players. We evaluate prominent models including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models, finding that all state-of-the-art LLMs underperform in playing optimal poker. However, after fine-tuning, these models show marked improvements. We validate PokerBench by having models with different scores compete with each other, demonstrating that higher scores on PokerBench lead to higher win rates in actual poker games. Through gameplay between our fine-tuned model and GPT-4, we also identify limitations of simple supervised fine-tuning for learning optimal playing strategy, suggesting the need for more advanced methodologies for effectively training language models to excel in games. PokerBench thus presents a unique benchmark for a quick and reliable evaluation of the poker-playing ability of LLMs as well as a comprehensive benchmark to study the progress of LLMs in complex game-playing scenarios.",arxiv,http://arxiv.org/abs/2501.08328v2,,2501.083282,,,http://arxiv.org/pdf/2501.08328v2,cs.CL; cs.AI; cs.GT
Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning,Marc Lanctot; John Schultz; Neil Burch; Max Olan Smith; Daniel Hennes; Thomas Anthony; Julien Perolat,2023,"Progress in fields of machine learning and adversarial planning has benefited significantly from benchmark domains, from checkers and the classic UCI data sets to Go and Diplomacy. In sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors along with a population of forty-three tournament entries, some of which are intentionally sub-optimal. We describe metrics to measure the quality of agents based both on average returns and exploitability. We then show that several RL, online learning, and language model approaches can learn good counter-strategies and generalize well, but ultimately lose to the top-performing bots, creating an opportunity for research in multiagent learning.",arxiv,http://arxiv.org/abs/2303.03196v2,,2303.031962,,,http://arxiv.org/pdf/2303.03196v2,cs.GT; cs.AI; cs.LG; cs.MA
PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning,Bhavinkumar Vinodbhai Kuwar; Bikrant Bikram Pratap Maurya; Priyanshu Gupta; Nitin Choudhury,2025,"Detecting deception in strategic dialogues is a complex and high-stakes task due to the subtlety of language and extreme class imbalance between deceptive and truthful communications. In this work, we revisit deception detection in the Diplomacy dataset, where less than 5% of messages are labeled deceptive. We introduce a lightweight yet effective model combining frozen BERT embeddings, interpretable linguistic and game-specific features, and a Positive-Unlabeled (PU) learning objective. Unlike traditional binary classifiers, PU-Lie is tailored for situations where only a small portion of deceptive messages are labeled, and the majority are unlabeled. Our model achieves a new best macro F1 of 0.60 while reducing trainable parameters by over 650x. Through comprehensive evaluations and ablation studies across seven models, we demonstrate the value of PU learning, linguistic interpretability, and speaker-aware representations. Notably, we emphasize that in this problem setting, accurately detecting deception is more critical than identifying truthful messages. This priority guides our choice of PU learning, which explicitly models the rare but vital deceptive class.",arxiv,http://arxiv.org/abs/2507.09157v1,,2507.091571,,,http://arxiv.org/pdf/2507.09157v1,cs.CL
Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy,Zhenyu Guan; Xiangyu Kong; Fangwei Zhong; Yizhou Wang,2024,"Diplomacy is one of the most sophisticated activities in human society, involving complex interactions among multiple parties that require skills in social reasoning, negotiation, and long-term strategic planning. Previous AI agents have demonstrated their ability to handle multi-step games and large action spaces in multi-agent tasks. However, diplomacy involves a staggering magnitude of decision spaces, especially considering the negotiation stage required. While recent agents based on large language models (LLMs) have shown potential in various applications, they still struggle with extended planning periods in complex multi-agent settings. Leveraging recent technologies for LLM-based agents, we aim to explore AI's potential to create a human-like agent capable of executing comprehensive multi-agent missions by integrating three fundamental capabilities: 1) strategic planning with memory and reflection; 2) goal-oriented negotiation with social reasoning; and 3) augmenting memory through self-play games for self-evolution without human in the loop.",arxiv,http://arxiv.org/abs/2407.06813v4,,2407.068134,NuerIPS 2024,,http://arxiv.org/pdf/2407.06813v4,cs.AI; cs.MA; cs.SI
Self Generated Wargame AI: Double Layer Agent Task Planning Based on Large Language Model,Y. Sun; J. Zhao; C. Yu; W. Wang; X. Zhou,2023,"The large language models represented by ChatGPT have a disruptive impact on the field of artificial intelligence. But it mainly focuses on natural language processing, speech recognition, machine learning and natural language understanding. This paper innovatively applies the large language model to the field of intelligent decision-making, places the large language model in the decision-making center, and constructs an agent architecture with the large language model as the core. Based on this, it further proposes a two-layer agent task planning, issues and executes decision commands through the interaction of natural language, and carries out simulation verification through the wargame simulation environment. Through the game confrontation simulation experiment, it is found that the intelligent decision-making ability of the large language model is significantly stronger than the commonly used reinforcement learning AI and rule AI, and the intelligence, understandability and generalization are all better. And through experiments, it was found that the intelligence of the large language model is closely related to prompt. This work also extends the large language model from previous human-computer interaction to the field of intelligent decision-making, which has important reference value and significance for the development of intelligent decision-making.",arxiv,http://arxiv.org/abs/2312.01090v2,,2312.010902,,,http://arxiv.org/pdf/2312.01090v2,cs.AI; cs.CL
Strategic Reasoning with Language Models,Kanishk Gandhi; Dorsa Sadigh; Noah D. Goodman,2023,"Strategic reasoning enables agents to cooperate, communicate, and compete with other agents in diverse situations. Existing approaches to solving strategic games rely on extensive training, yielding strategies that do not generalize to new scenarios or games without retraining. Large Language Models (LLMs), with their ability to comprehend and generate complex, context-rich language, could prove powerful as tools for strategic gameplay. This paper introduces an approach that uses pretrained LLMs with few-shot chain-of-thought examples to enable strategic reasoning for AI agents. Our approach uses systematically generated demonstrations of reasoning about states, values, and beliefs to prompt the model. Using extensive variations of simple matrix games, we show that strategies that are derived based on systematically generated prompts generalize almost perfectly to new game structures, alternate objectives, and hidden information. Additionally, we demonstrate our approach can lead to human-like negotiation strategies in realistic scenarios without any extra training or fine-tuning. Our results highlight the ability of LLMs, guided by systematic reasoning demonstrations, to adapt and excel in diverse strategic scenarios.",arxiv,http://arxiv.org/abs/2305.19165v1,,2305.191651,,,http://arxiv.org/pdf/2305.19165v1,cs.AI; cs.CL; cs.GT; cs.HC
"Tracing LLM Reasoning Processes with Strategic Games: A Framework for Planning, Revision, and Resource-Constrained Decision Making",Xiaopeng Yuan; Xingjian Zhang; Ke Xu; Yifan Xu; Lijun Yu; Jindong Wang; Yushun Dong; Haohan Wang,2025,"Large language models (LLMs) are increasingly used for tasks that require complex reasoning. Most benchmarks focus on final outcomes but overlook the intermediate reasoning steps - such as planning, revision, and decision making under resource constraints. We argue that measuring these internal processes is essential for understanding model behavior and improving reliability. We propose using strategic games as a natural evaluation environment: closed, rule-based systems with clear states, limited resources, and automatic feedback. We introduce a framework that evaluates LLMs along three core dimensions: planning, revision, and resource-constrained decision making. To operationalize this, we define metrics beyond win rate, including overcorrection risk rate, correction success rate, improvement slope, and over-budget ratio. In 4320 adversarial rounds across 12 leading models, ChatGPT-o3-mini achieves the top composite score, with a win rate of 74.7 percent, a correction success rate of 78.6 percent, and an improvement slope of 0.041. By contrast, Qwen-Plus, despite an overcorrection risk rate of 81.6 percent, wins only 25.6 percent of its matches - primarily due to excessive resource use. We also observe a negative correlation between overcorrection risk rate and correction success rate (Pearson r = -0.51, p = 0.093), suggesting that more frequent edits do not always improve outcomes. Our findings highlight the value of assessing not only what LLMs decide but how they arrive at those decisions",arxiv,http://arxiv.org/abs/2506.12012v1,,2506.120121,,,http://arxiv.org/pdf/2506.12012v1,cs.AI
Welfare Diplomacy: Benchmarking Language Model Cooperation,Gabriel Mukobi; Hannah Erlebach; Niklas Lauffer; Lewis Hammond; Alan Chan; Jesse Clifton,2023,"The growing capabilities and increasingly widespread deployment of AI systems necessitate robust benchmarks for measuring their cooperative capabilities. Unfortunately, most multi-agent benchmarks are either zero-sum or purely cooperative, providing limited opportunities for such measurements. We introduce a general-sum variant of the zero-sum board game Diplomacy -- called Welfare Diplomacy -- in which players must balance investing in military conquest and domestic welfare. We argue that Welfare Diplomacy facilitates both a clearer assessment of and stronger training incentives for cooperative capabilities. Our contributions are: (1) proposing the Welfare Diplomacy rules and implementing them via an open-source Diplomacy engine; (2) constructing baseline agents using zero-shot prompted language models; and (3) conducting experiments where we find that baselines using state-of-the-art models attain high social welfare but are exploitable. Our work aims to promote societal safety by aiding researchers in developing and assessing multi-agent AI systems. Code to evaluate Welfare Diplomacy and reproduce our experiments is available at https://github.com/mukobi/welfare-diplomacy.",arxiv,http://arxiv.org/abs/2310.08901v1,,2310.089011,,,http://arxiv.org/pdf/2310.08901v1,cs.MA; cs.AI; cs.CL
WGSR-Bench: Wargame-based Game-theoretic Strategic Reasoning Benchmark for Large Language Models,Qiyue Yin; Pei Xu; Qiaozhe Li; Shengda Liu; Shengqi Shen; Tong Wang; Yihong Han; Xiaonan Zhao; Likun Yang; Shiyue Cao; Shiyu Qiu; Yuxuan Liu; Shizhao Yu; Lei Cui; Chengxin Yan; Jie Sun; Xiangquan Tang; Kaiqi Huang,2025,"Recent breakthroughs in Large Language Models (LLMs) have led to a qualitative leap in artificial intelligence' s performance on reasoning tasks, particularly demonstrating remarkable capabilities in mathematical, symbolic, and commonsense reasoning. However, as a critical component of advanced human cognition, strategic reasoning, i.e., the ability to assess multi-agent behaviors in dynamic environments, formulate action plans, and adapt strategies, has yet to be systematically evaluated or modeled. To address this gap, this paper introduces WGSR-Bench, the first strategy reasoning benchmark for LLMs using wargame as its evaluation environment. Wargame, a quintessential high-complexity strategic scenario, integrates environmental uncertainty, adversarial dynamics, and non-unique strategic choices, making it an effective testbed for assessing LLMs' capabilities in multi-agent decision-making, intent inference, and counterfactual reasoning. WGSR-Bench designs test samples around three core tasks, i.e., Environmental situation awareness, Opponent risk modeling and Policy generation, which serve as the core S-POE architecture, to systematically assess main abilities of strategic reasoning. Finally, an LLM-based wargame agent is designed to integrate these parts for a comprehensive strategy reasoning assessment. With WGSR-Bench, we hope to assess the strengths and limitations of state-of-the-art LLMs in game-theoretic strategic reasoning and to advance research in large model-driven strategic intelligence.",arxiv,http://arxiv.org/abs/2506.10264v1,,2506.102641,,,http://arxiv.org/pdf/2506.10264v1,cs.AI
