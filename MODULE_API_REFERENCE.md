# ðŸ”Œ Module API Reference

## ðŸŽ¯ Core APIs

### SearchHarvester
```python
class SearchHarvester:
    """Multi-source paper discovery orchestrator"""

    def search_all(sources: List[str], max_results: int, parallel: bool) â†’ DataFrame
    def normalize_results() â†’ DataFrame
    def deduplicate(threshold: float = 0.85) â†’ DataFrame
```

### ProductionHarvester
```python
class ProductionHarvester(SearchHarvester):
    """Enhanced production-ready harvesting"""

    def harvest_with_cache(query: str, cache_ttl: int) â†’ DataFrame
    def batch_harvest(queries: List[str]) â†’ DataFrame
    def validate_results() â†’ Dict[str, Any]
```

### LLMExtractor
```python
class LLMExtractor:
    """OpenAI GPT-based information extraction"""

    def extract(text: str, schema: Dict) â†’ Dict
    def batch_extract(texts: List[str]) â†’ List[Dict]
    def validate_extraction(result: Dict) â†’ bool
```

### EnhancedLLMExtractor
```python
class EnhancedLLMExtractor(LLMExtractor):
    """Production-grade extraction with retry logic"""

    def extract_with_retry(text: str, max_retries: int = 3) â†’ Dict
    def extract_with_fallback(text: str, fallback_model: str) â†’ Dict
    async def extract_async(texts: List[str]) â†’ List[Dict]
```

### PDFFetcher
```python
class PDFFetcher:
    """Multi-source PDF retrieval"""

    def fetch(url: str, use_scihub: bool = True) â†’ Optional[bytes]
    def fetch_batch(urls: List[str], parallel: bool = True) â†’ Dict[str, bytes]
    def get_pdf_text(pdf_bytes: bytes) â†’ str
```

### ContentCache
```python
class ContentCache:
    """Intelligent caching system"""

    def get(key: str) â†’ Optional[Any]
    def set(key: str, value: Any, ttl: int = 3600) â†’ None
    def invalidate_pattern(pattern: str) â†’ int
    def get_stats() â†’ Dict[str, int]
```

### Normalizer
```python
class Normalizer:
    """Data cleaning & standardization"""

    def normalize_authors(authors: Union[str, List]) â†’ List[str]
    def normalize_title(title: str) â†’ str
    def normalize_date(date: Any) â†’ Optional[int]
    def clean_abstract(abstract: str) â†’ str
```

### BatchProcessor
```python
class BatchProcessor:
    """Parallel processing orchestration"""

    async def process_batch(items: List, processor: Callable, max_workers: int) â†’ List
    def chunk_data(data: List, chunk_size: int) â†’ List[List]
    def merge_results(results: List[DataFrame]) â†’ DataFrame
```

### Visualizer
```python
class Visualizer:
    """Chart & analysis generation"""

    def plot_timeline(df: DataFrame) â†’ Figure
    def plot_venues(df: DataFrame) â†’ Figure
    def plot_failure_modes(df: DataFrame) â†’ Figure
    def generate_report(df: DataFrame, output_dir: Path) â†’ None
```

### LoggingDatabase
```python
class LoggingDatabase:
    """SQLite-based structured logging"""

    def log(level: str, message: str, metadata: Dict) â†’ None
    def query_logs(level: str = None, limit: int = 100) â†’ List[Dict]
    def get_summary() â†’ Dict[str, int]
    def export_logs(format: str = "csv") â†’ Path
```

## ðŸ“Š Data Models

### Paper
```python
@dataclass
class Paper:
    title: str
    authors: List[str]
    year: int
    abstract: str
    doi: Optional[str]
    url: str
    source: str
    pdf_url: Optional[str]
    metadata: Dict[str, Any]
```

### ExtractionResult
```python
@dataclass
class ExtractionResult:
    paper_id: str
    venue_type: str
    game_type: str
    llm_family: str
    llm_role: str
    evaluation_approach: str
    failure_modes: List[str]
    confidence: float
    raw_response: str
```

### HarvestResult
```python
@dataclass
class HarvestResult:
    query: str
    source: str
    papers: List[Paper]
    timestamp: datetime
    errors: List[str]
    stats: Dict[str, int]
```

## ðŸ”§ Configuration Objects

### HarvesterConfig
```python
class HarvesterConfig:
    sources: List[str]
    max_results_per_source: int
    rate_limits: Dict[str, float]
    parallel: bool
    timeout: int
```

### ExtractionConfig
```python
class ExtractionConfig:
    model: str  # gpt-4, gpt-3.5-turbo
    temperature: float
    max_tokens: int
    retry_attempts: int
    fallback_model: Optional[str]
```

### CacheConfig
```python
class CacheConfig:
    backend: str  # memory, redis, disk
    ttl: int
    max_size: int
    eviction_policy: str
```

## ðŸŽ¨ Utility Functions

### Query Building
```python
def expand_query(query: str, synonyms: Dict[str, List[str]]) â†’ List[str]
def build_boolean_query(terms: List[str], operators: List[str]) â†’ str
def validate_query_syntax(query: str) â†’ bool
```

### Data Validation
```python
def validate_paper(paper: Dict) â†’ bool
def validate_doi(doi: str) â†’ bool
def validate_extraction(result: Dict, schema: Dict) â†’ bool
```

### Text Processing
```python
def extract_keywords(text: str, n: int = 10) â†’ List[str]
def summarize_abstract(abstract: str, max_length: int = 200) â†’ str
def detect_language(text: str) â†’ str
```

## ðŸ”Œ Extension Points

### Custom Harvesters
```python
class CustomHarvester(BaseHarvester):
    """Extend for new data sources"""

    def search(self, query: str, max_results: int) â†’ List[Paper]:
        # Implement source-specific logic
        pass
```

### Custom Extractors
```python
class CustomExtractor(BaseExtractor):
    """Extend for different extraction approaches"""

    def extract(self, text: str) â†’ Dict:
        # Implement extraction logic
        pass
```

### Export Formats
```python
class CustomExporter(BaseExporter):
    """Extend for new export formats"""

    def export(self, data: DataFrame, output_path: Path) â†’ None:
        # Implement export logic
        pass
```

## ðŸš€ Async APIs

### Async Harvesting
```python
async def harvest_async(queries: List[str]) â†’ List[HarvestResult]
async def fetch_pdfs_async(urls: List[str]) â†’ Dict[str, bytes]
async def extract_batch_async(texts: List[str]) â†’ List[ExtractionResult]
```

### Stream Processing
```python
async def stream_papers(source: AsyncIterator[Paper]) â†’ None
async def process_stream(stream: AsyncIterator, processor: Callable) â†’ None
```

## ðŸ“¡ Event Hooks

### Pipeline Events
```python
on_harvest_start(query: str, sources: List[str])
on_harvest_complete(results: List[Paper])
on_extraction_start(paper: Paper)
on_extraction_complete(result: ExtractionResult)
on_error(error: Exception, context: Dict)
```

### Progress Callbacks
```python
def register_progress_callback(callback: Callable[[float, str], None])
def update_progress(percent: float, message: str)
```
