@misc{Yin2025WGSRBenchWargamebased,
  title = {WGSR-Bench: Wargame-based Game-theoretic Strategic Reasoning Benchmark for Large Language Models},
  author = {Qiyue Yin and  Pei Xu and  Qiaozhe Li and  Shengda Liu and  Shengqi Shen and  Tong Wang and  Yihong Han and  Xiaonan Zhao and  Likun Yang and  Shiyue Cao and  Shiyu Qiu and  Yuxuan Liu and  Shizhao Yu and  Lei Cui and  Chengxin Yan and  Jie Sun and  Xiangquan Tang and  Kaiqi Huang},
  year = {2025},
  doi = {nan},
  url = {http://arxiv.org/abs/2506.10264v1},
  abstract = {Recent breakthroughs in Large Language Models (LLMs) have led to a qualitative leap in artificial intelligence' s performance on reasoning tasks, particularly demonstrating remarkable capabilities in mathematical, symbolic, and commonsense reasoning. However, as a critical component of advanced human cognition, strategic reasoning, i.e., the ability to assess multi-agent behaviors in dynamic environments, formulate action plans, and adapt strategies, has yet to be systematically evaluated or modeled. To address this gap, this paper introduces WGSR-Bench, the first strategy reasoning benchmark for LLMs using wargame as its evaluation environment. Wargame, a quintessential high-complexity strategic scenario, integrates environmental uncertainty, adversarial dynamics, and non-unique strategic choices, making it an effective testbed for assessing LLMs' capabilities in multi-agent decision-making, intent inference, and counterfactual reasoning. WGSR-Bench designs test samples around three core tasks, i.e., Environmental situation awareness, Opponent risk modeling and Policy generation, which serve as the core S-POE architecture, to systematically assess main abilities of strategic reasoning. Finally, an LLM-based wargame agent is designed to integrate these parts for a comprehensive strategy reasoning assessment. With WGSR-Bench, we hope to assess the strengths and limitations of state-of-the-art LLMs in game-theoretic strategic reasoning and to advance research in large model-driven strategic intelligence.}
}

@misc{Lee2025ExploringPotential,
  title = {Exploring Potential Prompt Injection Attacks in Federated Military LLMs and Their Mitigation},
  author = {Youngjoon Lee and  Taehyun Park and  Yunho Lee and  Jinu Gong and  Joonhyuk Kang},
  year = {2025},
  doi = {nan},
  url = {http://arxiv.org/abs/2501.18416v1},
  abstract = {Federated Learning (FL) is increasingly being adopted in military collaborations to develop Large Language Models (LLMs) while preserving data sovereignty. However, prompt injection attacks-malicious manipulations of input prompts-pose new threats that may undermine operational security, disrupt decision-making, and erode trust among allies. This perspective paper highlights four potential vulnerabilities in federated military LLMs: secret data leakage, free-rider exploitation, system disruption, and misinformation spread. To address these potential risks, we propose a human-AI collaborative framework that introduces both technical and policy countermeasures. On the technical side, our framework uses red/blue team wargaming and quality assurance to detect and mitigate adversarial behaviors of shared LLM weights. On the policy side, it promotes joint AI-human policy development and verification of security protocols. Our findings will guide future research and emphasize proactive strategies for emerging military contexts.}
}

@misc{Chupilkin2025ThePrompt,
  title = {The Prompt War: How AI Decides on a Military Intervention},
  author = {Maxim Chupilkin},
  year = {2025},
  doi = {nan},
  url = {http://arxiv.org/abs/2507.06277v1},
  abstract = {Which factors determine AI propensity for military intervention? While the use of AI in war games and military planning is growing exponentially, the simple analysis of key drivers embedded in the models has not yet been done. This paper does a simple conjoint experiment proposing a model to decide on military intervention in 640 vignettes where each was run for 100 times allowing to explore AI decision on military intervention systematically. The analysis finds that largest predictors of AI decision to intervene are high domestic support and high probability of success. Costs such as international condemnation, military deaths, civilian deaths, and negative economic effect are statistically significant, but their effect is around half of domestic support and probability of victory. Closing window of opportunity only reaches statistical significance in interaction with other factors. The results are remarkably consistent across scenarios and across different models (OpenAI GPT, Anthropic Claude, Google Gemini) suggesting a pattern in AI decision-making.}
}
